[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bash for Bioinformatics",
    "section": "",
    "text": "Bash for Bio",
    "crumbs": [
      "Bash for Bio"
    ]
  },
  {
    "objectID": "index.html#who-this-course-is-for",
    "href": "index.html#who-this-course-is-for",
    "title": "Bash for Bioinformatics",
    "section": "Who this course is for",
    "text": "Who this course is for\n\nHave you needed to align a folder of FASTA files and not know how to do it?\nDo you want to automate an R or Python script you wrote to work on a bunch of files?\nDo you want to do all of this on a high performance cluster (Short for High Performance Computing. See the hpc basics chapter for more info.HPC)?\n\nIf so, this course is for you! We will learn enough bash scripting to do useful things on the Fred Hutch computing cluster (affectionately called “gizmo”) and automate the boring parts.",
    "crumbs": [
      "Bash for Bio"
    ]
  },
  {
    "objectID": "index.html#learning-objectives",
    "href": "index.html#learning-objectives",
    "title": "Bash for Bioinformatics",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nApply bash scripting to execute alignment, and Python/R scripts\nNavigate and process data on the different filesystems available at FH\nArticulate basic HPC architecture concepts and why they’re useful in your work\nLeverage bash scripting to execute jobs on a high performance cluster.\nUtilize workflow managers such as cromwell to process multiple files in a multi-step Short for Workflow Description Language. A standard for specifying a workflow, which includes describing inputs, saving intermediate outputs, and outputting processed files.WDL workflow.\nManage software dependencies reproducibly using container-based technologies such as Docker/Apptainer containers or EasyBuild modules\n\n\n\n\n\n\n\nWasn’t there another Bash for Bioinformatics book?\n\n\n\nI originally wrote a book that was called Bash for Bioinformatics, which was about learning enough bash to use the cloud-based DNANexus platform effectively.\nI have renamed that book Bash for DNANexus, and named this course Bash for Bioinformatics.\nThis book shares bones with Bash for DNANexus, but has more of a focus on running tasks on high performance computing systems (Short for High Performance Computing. See the hpc basics chapter for more info.HPC).",
    "crumbs": [
      "Bash for Bio"
    ]
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Bash for Bioinformatics",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou will need an account on rhino and know how to connect to it through VPN. If you have taken the Intro to Fred Hutch Cluster Computing workshop, then you will be ready.\nWe highly recommend reviewing Intro to Command Line and Intro to Fred Hutch Cluster Computing.\nBasic knowledge of the following commands:\n\nls\ncd and basic directory navigation\nmv/cp/mkdir/rm\n\n\nWe will assume that you will do all of your work in your home directory on Rhino. We will not be using that much space in your home directory.\n\n\n\n\n\n\nTerminology\n\n\n\nWe know that not all of us have the same vocabulary. We try to define terminology as much as possible. These are indicated by double underlines such as this:\nA task that we have assigned a computer to run. This computer can be a compute node in a cluster, or our own machine.Compute Job\nYou can click and hold on the term to define it.",
    "crumbs": [
      "Bash for Bio"
    ]
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Bash for Bioinformatics",
    "section": "Schedule",
    "text": "Schedule\nYou should have completed the readings before class, so we can hit the ground running.\n\n\n\nWeek\nTopics\nReading\n\n\n\n\nPreclass\nReview Intro to Command Line and Cluster 101\n\n\n\nWeek 1\nFilesystem Basics\nBite Size Bash\n\n\nWeek 2\nWriting and Running Bash Scripts\nBite Size Bash\n\n\nWeek 3\nBatch Processing and HPC Jobs\nHPC Basics\n\n\nWeek 4\nTesting Scripts/Workflow Managers\nContainer Basics\n\n\nOn your own time\nTesting Scripts\n\n\n\nOn your own time\nConfiguring your Bash Shell",
    "crumbs": [
      "Bash for Bio"
    ]
  },
  {
    "objectID": "index.html#reference-text",
    "href": "index.html#reference-text",
    "title": "Bash for Bioinformatics",
    "section": "Reference Text",
    "text": "Reference Text\n\nWe will be using Julia Evan’s Bite Size Bash as our reference text. Julia’s explanations are incredibly clear and it will be a valuable reference even beyond this course. You will receive the PDF as part of class.\nIf you want to know the true power of the command line, I recommend Data Science at the Command Line.",
    "crumbs": [
      "Bash for Bio"
    ]
  },
  {
    "objectID": "01_basics.html",
    "href": "01_basics.html",
    "title": "1  Navigating the Bash Filesystem",
    "section": "",
    "text": "1.1 Learning Objectives\nBy the end of this session, you should be able to:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Navigating the Bash Filesystem</span>"
    ]
  },
  {
    "objectID": "01_basics.html#learning-objectives",
    "href": "01_basics.html#learning-objectives",
    "title": "1  Navigating the Bash Filesystem",
    "section": "",
    "text": "Navigate and copy data to the different filesystems available at Fred Hutch.\nExplain the difference between absolute and relative file paths.\nSet Permissions on and execute a bash script\nFind help on the system",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Navigating the Bash Filesystem</span>"
    ]
  },
  {
    "objectID": "01_basics.html#map-of-the-material",
    "href": "01_basics.html#map-of-the-material",
    "title": "1  Navigating the Bash Filesystem",
    "section": "1.2 Map of the Material",
    "text": "1.2 Map of the Material\n\n\n\n\n\nflowchart TD\n    A[Signing In] --&gt; B[pwd: Find Current Directory]\n    B --&gt; D[Absolute vs Relative Paths]\n    D --&gt; F[chmod: set permissions]\n    F --&gt; G[execute a script]\n\n\n\n\n\n\n\n\n\n\n\n\nReminder about Terminology\n\n\n\nDefined words are double underlined. You can click and hold on them to see the definition. Try it below!\nInformation about a file or dataset, such as the filename, or date created.metadata",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Navigating the Bash Filesystem</span>"
    ]
  },
  {
    "objectID": "01_basics.html#navigating-the-bash-terminal",
    "href": "01_basics.html#navigating-the-bash-terminal",
    "title": "1  Navigating the Bash Filesystem",
    "section": "1.3 Navigating the Bash Terminal",
    "text": "1.3 Navigating the Bash Terminal\n\nWe recommend that you review the material for Intro to Command Line and know the following: Changing directories,\n\nBy default, when you log into a remote system such as rhino, you are in a bash A command processor that we interact with via the system prompt. In our case, the Shell we’re interacting with is on the server.shell.\nWhy is it a bash shell? Bash is the default shell for linux systems, especially for high performance clusters (HPCs), and there are some quirks about navigating around the command line you should be aware of.\n\n\n\n\n\n\nA helpful key: &lt;Up Arrow&gt;\n\n\n\nThe  key will let you cycle through your history, or previous executed commands. This can be super helpful if you have typed a long command with a syntax error. You can use &lt;Up Arrow&gt; to fix mistakes and run that command again.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Navigating the Bash Filesystem</span>"
    ]
  },
  {
    "objectID": "01_basics.html#setting-yourself-up-for-success",
    "href": "01_basics.html#setting-yourself-up-for-success",
    "title": "1  Navigating the Bash Filesystem",
    "section": "1.4 Setting Yourself Up for Success",
    "text": "1.4 Setting Yourself Up for Success\nSo we have logged into rhino. Now what?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Navigating the Bash Filesystem</span>"
    ]
  },
  {
    "objectID": "01_basics.html#navigating-the-filesystems",
    "href": "01_basics.html#navigating-the-filesystems",
    "title": "1  Navigating the Bash Filesystem",
    "section": "1.5 Navigating the Filesystems",
    "text": "1.5 Navigating the Filesystems\n\n1.5.1 pwd Where Am I?\nThe pwd command (short for present working directory) will let you know your current location in the filesystem. Knowing your current directory is critical when using relative file paths.\nIf I run pwd right after signing into rhino I get:\n/home/tladera2\nYou should have a similar path, except with your user name. This is your home directory - where you have a limited amount of space to store scripts and other files. Don’t worry, the majority of your data is stored elsewhere ()",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Navigating the Bash Filesystem</span>"
    ]
  },
  {
    "objectID": "01_basics.html#sec-home",
    "href": "01_basics.html#sec-home",
    "title": "1  Navigating the Bash Filesystem",
    "section": "1.6 Going /home: ~/",
    "text": "1.6 Going /home: ~/\nThere is one important file alias you should always remember: ~/ is shorthand for your own home directory.\nDepending on the linux distribution, this can be a different location. On the FH filesystem, when I use ~/, it maps to:\n/home/tladera2/\nThe home directory is also important because it is where your configuration files live, such as .bashrc (see Section 6.1).\nDepending on how you work, you may want to store your scripts and workflows in /home/. Some people prefer to keep their scripts, data, and results in a single folder. This is not really practical for most genomics projects, unless you are saving processed data. For more info, see Section 6.6.\n\n\n\n\n\n\nYour current working directory\n\n\n\nThere is an alias for your current directory: . (the period sign).\nThis becomes useful when you want to output files to your current location.\n\n\n\n1.6.1 du: How much space?\nOne of the things we can do is check for disk usage with the du command. If I run du by itself on the command line, it will give me the disk usage of all folders and files in our current directory, which is a lot of output.\nThere is an option called -d that lets us specify the depth. -d 1 will give us only the file sizes of the top level folders in our directory:\ndu -d 1 .\nHere are the first few lines of my du output.\n630440  ./Code\n32  ./Downloads\n32  ./Pictures\n2495144 ./miniconda3\n64  ./.launch-rstudio-server\n72  ./.ipynb_checkpoints\n64  ./.qt\n1616    ./.config\n32  ./Music\n32  ./Desktop\nIf we want to specify du to scan only a single folder, we can give the folder name.\ndu -d 1 Desktop\nI have nothing really stored in my Desktop/ folder, so I get the following:\n32  Desktop/\n\n\n\n\n\n\nTry it out\n\n\n\nTry checking the disk usage using du for the Desktop folder in your /home directory (mine is /home/tladera2).\ndu -d 1 --------/\nTry out using du -d 2 on your home directory:\ndu -d 2 ~/",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Navigating the Bash Filesystem</span>"
    ]
  },
  {
    "objectID": "01_basics.html#sec-filesystems",
    "href": "01_basics.html#sec-filesystems",
    "title": "1  Navigating the Bash Filesystem",
    "section": "1.7 FH users: the main filesystems",
    "text": "1.7 FH users: the main filesystems\nWhen working on the Fred Hutch HPC, there are four main filesystems you should consider:\n\n/home/ - The home filesystem. Your scripts can live here. Also where your configuration files (such as .bashrc) live. Can be accessed using ~/.\n/fh/fast/ (also known as fast) - Research storage. Raw files and processed results should live here.\n/hpc/temp/ (also known as temp) - The temporary filesystem. This filesystem is faster to access for gizmo nodes on the cluster, so files can be copied to for computation. The output files you generate should be moved back into an appropriate folder on /fh/fast/. Note that files on /fh/temp/ will be deleted after 30 days.\n/fh/regulated/ - A secure filesystem meant for NIH regulated data. If you are processing data that is regulated under the current NIH guidelines, you will process it here.\n\nSo, how do we utilize these filesystems? We will be running commands like this:\n1ml BWA\n2bwa mem -M -t 2\n3/fh/fast/reference_data/chr20\n4/fh/fast/laderas_t/raw_data/na12878_1.fq\n/fh/fast/laderas_t/raw_data/na12878_2.fq\n5&gt; /hpc/temp/laderas_t/aligned_data/na12878_1.sam\n\n1\n\nLoad bwa software\n\n2\n\nStart bwa mem (aligner)\n\n3\n\npath of genome index\n\n4\n\npath of paired end reads files\n\n5\n\npath of output\n\n\nTo understand the above, We first have to familiarize ourselves with absolute vs relative paths.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Navigating the Bash Filesystem</span>"
    ]
  },
  {
    "objectID": "01_basics.html#sec-paths",
    "href": "01_basics.html#sec-paths",
    "title": "1  Navigating the Bash Filesystem",
    "section": "1.8 Absolute versus relative paths",
    "text": "1.8 Absolute versus relative paths\nYou may have muddled with file paths, and maybe have used absolute paths to specify the location of a file. When you are processing files, it is important to understand the difference.\nAbsolute paths contain all the information needed to find a file in a file system from the root / directory. For example, this would be an absolute path:\n/fh/fast/laderast/immuno_project/raw_data/chr2.fa.gz\nAbsolute paths always start with /, because that is the root directory, where all the top folders and files live.\nIn terms of folder structure, this is what this looks like:\n1/\n2├── fh\n│   └──fast\n│       └──laderast\n|            └──immuno_project\n│                 └──raw_data\n│                    └──chr2.fa.gz\n\n1\n\nRoot directory\n\n2\n\nFolders in root directory\n\n\nRelative paths break up an absolute path into two pieces of information: 1) your current directory and 2) the path relative to that directory. Relative paths are really helpful because things don’t break when you move your folder or files.\nIf my current working directory is the directory /fh/fast/laderas_t/immuno_project/, then the relative path to that same file would be:\nraw_data/chr2.fa.gz\nWe can visualize the relative path like this, where our working directory is indicated by a star:\n1/\n2├── fh/fast/laderast/immuno_project/\n3|                                   └──raw_data\n│                                      └──chr2.fa.gz\n                                    \n\n1\n\nThe root directory\n\n2\n\nOur working directory\n\n3\n\nOur relative path\n\n\nNote that this relative path does not start with a /, because our current directory isn’t the root directory. Relative paths are incredibly useful when scripting in a reproducible manner, such as using project-based workflows to process files in a single folder.\n\n\n\n\n\n\n&lt;TAB&gt; is for autocompletion of paths\n\n\n\nNever underestimate the usefulness of the &lt;TAB&gt; key, which triggers autocompletion on the command line. It can help you complete paths to files and save you a lot of typing.\nFor example, say I have a path that I want to navigate to\n/home/tladera2/my_long_path\nI can type in the first part of the path and then hit &lt;TAB&gt;:\n/home/tladera2/my_&lt;TAB&gt;\nAnd if the prefix my_ is unique in my folder, it will autocomplete the path:\n/home/tladera2/my_long_path\nNote that we need to use enough of the folder name so that completing it is unambiguous. If there are multiple choices, then autocomplete will list all of them.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Navigating the Bash Filesystem</span>"
    ]
  },
  {
    "objectID": "01_basics.html#grabbing-stuff-from-github",
    "href": "01_basics.html#grabbing-stuff-from-github",
    "title": "1  Navigating the Bash Filesystem",
    "section": "1.9 Grabbing Stuff from GitHub",
    "text": "1.9 Grabbing Stuff from GitHub\nFor the rest of the exercises for today, we’ll be grabbing the scripts from github using git clone.\ngit clone https://github.com/fhdsl/bash_for_bio_scripts\nThis will create a folder called bash_for_bio_scripts/ in our current directory.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Navigating the Bash Filesystem</span>"
    ]
  },
  {
    "objectID": "01_basics.html#sec-permissions",
    "href": "01_basics.html#sec-permissions",
    "title": "1  Navigating the Bash Filesystem",
    "section": "1.10 File Permissions",
    "text": "1.10 File Permissions\nFile permissions are Information about a file or dataset, such as the filename, or date created.metadata that are attached to file objects. They are how the system prevents certain files from being modified or restricting access of these files to certain people or groups.\nAll files have the following level of access permissions:\n\n\n\nLevel\nDescription\n\n\n\n\nOwner-level\nThe owner of the file\n\n\nGroup-level\nThe group of the file\n\n\nEveryone\nThe rest of the world\n\n\n\nFor example, if I’m the owner of the file, I can restrict the type of access to only myself (owner-level), the group I’m in (Group-level), or make the file freely available to everyone on the system (Everyone).\nEach level has the following type of access:\n\n\n\n\n\n\n\n\n\nType\nDescription\nAbbreviation\nExample\n\n\n\n\nRead\nLevel can only read contents of file\nr\nA list of users in a text file\n\n\nWrite\nLevel can write to the file\nw\nAppending an entry to the end of a log\n\n\nExecute\nLevel can run the file as an executable\nx\nsamtools\n\n\n\nYou can see the permissions for a file using the ls -l &lt;FILENAME&gt;. For example:\nls -l scripts\nwill give me the following line:\n-rwxrwxrwx 1 tladera2  staff  16 Jul 11 11:05 tell_the_time.sh\nThe cardinal rule to remember is that:\n\nIf you want to run a file as an executable, you (or your group) needs to have executable level permission.\n\nFor example, if I want to run a script called run_samtools.sh in my directory like this:\n./run_samtools.sh my_bam_file.bam\nI will need to have execute privileges at the user, group, or others level.\nWe can change the permissions of our files using the chmod command.\n\n\n\n\n\n\nHelpful unix permissions situations\n\n\n\nI tend to just go by memory when setting file permissions. If I have collaborators who just want to set\n\n\n\nSituation\nCommand\n\n\n\n\nOnly I can execute/read/write a file\nchmod 700 &lt;filename&gt;\n\n\nOnly I and my group can read a file\nchmod 110 &lt;filename&gt;\n\n\nGrant my group read permissions\nchmod 710 &lt;filename&gt;\n\n\nMake executable/read/write by all\nchmod 777 &lt;filename&gt;\n\n\n\n\n\n\n\n\n\n\n\nEven if you don’t have execute permissions\n\n\n\nWith bash scripts, you can still run them if you have read permissions. You can still run bash scripts by using the bash command:\nbash run_samtools.sh my_bam_file.bam\n\n\n\n1.10.1 Try it out\nWhat are the permissions for the GitHub repo (bash_for_bio) that you just downloaded?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Navigating the Bash Filesystem</span>"
    ]
  },
  {
    "objectID": "01_basics.html#sec-moving",
    "href": "01_basics.html#sec-moving",
    "title": "1  Navigating the Bash Filesystem",
    "section": "1.11 Moving Things Around",
    "text": "1.11 Moving Things Around\nA lot of the time, we need to move files between shared filesystems. One filesystem might be good at storage and be backed up on a regular basis, while another filesystem might be better for temporary work on the cluster.\nYou might be familiar with mv, which lets you move files around in Unix. One thing to keep in mind when you’re mving things to a new folder that there is a difference between:\nmv log.txt my_folder   ## renames log.txt to my_folder\nand\nmv log.txt my_folder/  ## moves log.txt to be in my_folder\nThis is one thing that still trips me up all the time.\nThis is one situation where using a GUI such as Motuz (?sec-motuz) can be very helpful. You don’t have to worry about accidentally renaming files.\nOther tools for sync’ing between filesystems include rsync, which requires careful reading of documentation.\n\n\n\n\n\n\nThings I always forget: the difference between /home/mydir/ and home/mydir/\n\n\n\nSome things that trip me up all the time. The difference between\n/home/mydir/    #absolute path\nand\nhome/mydir/     #relative path\nThe first one is an absolute path, and the second is a relative path. Your clue is the leading / at the beginning of a path. If you’re getting file not found messages, check to make sure the path is the right format.\n\n\n\n1.11.1 Keep Everything in Folders\nWe need to talk about code and data organization. For the FH system, we have a /home/ directory, and if we have generated research data, a /fh/fast/ directory. If we want our scripts to live in /home/ and our data is in /fh/temp/, we’ll need to refer to each of these file locations.\nIdeally, we want to make the naming conventions of our code and our data as similar as possible.\n\n\n\n\n\n\nTry it Out\n\n\n\nCopy the script tell_the_time.sh in the scripts/ directory to your home directory.\nMake the script executable.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Navigating the Bash Filesystem</span>"
    ]
  },
  {
    "objectID": "01_basics.html#whats-in-the-script",
    "href": "01_basics.html#whats-in-the-script",
    "title": "1  Navigating the Bash Filesystem",
    "section": "1.12 What’s in the script",
    "text": "1.12 What’s in the script\nWe can see what’s in the script by using cat:\ncat tell_the_time.sh\nAnd you’ll get the following:\n#!/bin/bash\ndate",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Navigating the Bash Filesystem</span>"
    ]
  },
  {
    "objectID": "01_basics.html#running-a-bash-script",
    "href": "01_basics.html#running-a-bash-script",
    "title": "1  Navigating the Bash Filesystem",
    "section": "1.13 Running a Bash Script",
    "text": "1.13 Running a Bash Script\nOk, now we have a bash script tell_the_time.sh in our current directory, how do we run it?\nBecause the script is not on our $PATH (Section 6.5.2), we’ll need to use ./ to execute it. ./ is an alias for the current folder, and it is an indicator to bash that the command we want to execute is in our current folder.\ntladera2$ ./tell_the_time.sh\nIf we haven’t set the permissions (Section 1.10) correctly, we’ll get this message:\nbash: ./scripts/tell_the_time.sh: Permission denied\nBut if we have execute access, we’ll get something like this:\nFri Jul 11 13:27:47 PDT 2025\nWhich is the current date and time.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Navigating the Bash Filesystem</span>"
    ]
  },
  {
    "objectID": "01_basics.html#running-an-r-or-python-script-on-the-command-line",
    "href": "01_basics.html#running-an-r-or-python-script-on-the-command-line",
    "title": "1  Navigating the Bash Filesystem",
    "section": "1.14 Running an R or Python Script on the command line",
    "text": "1.14 Running an R or Python Script on the command line\n\n1.14.1 Loading the fhR or fhPython modules\nBefore we can run our software, we’ll need to load up either R or\nWe’ll talk more about software modules next week (Section 2.5).\n\n\n1.14.2 R Users\nYou might not be aware that there are multiple ways to run R:\n\nas an interactive console, which is what we usually use in an IDE such as RStudio\non the command line using the Rscript command.\n\nRscript my_r_script.R\n\n\n1.14.3 Python Users\nPython users are much more aware that you can run Python scripts on the command line:\npython3 my_python_script.py\nWithin a shell script, you can also use a shebang (Section 2.4) to make your script executable by providing the location of python3:\n#!/bin/python3\npython3 my_python_script.py",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Navigating the Bash Filesystem</span>"
    ]
  },
  {
    "objectID": "01_basics.html#recap",
    "href": "01_basics.html#recap",
    "title": "1  Navigating the Bash Filesystem",
    "section": "1.15 Recap",
    "text": "1.15 Recap\nWe learned the following this week:\n\nNavigate and copy data to the different filesystems available at Fred Hutch.\nExplain the difference between absolute and relative file paths.\nSet Permissions on and execute a bash script\nFind help on the system",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Navigating the Bash Filesystem</span>"
    ]
  },
  {
    "objectID": "01_basics.html#next-week",
    "href": "01_basics.html#next-week",
    "title": "1  Navigating the Bash Filesystem",
    "section": "1.16 Next Week",
    "text": "1.16 Next Week\nWe’ll focus on adding arguments to our scripts.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Navigating the Bash Filesystem</span>"
    ]
  },
  {
    "objectID": "02_scripting.html",
    "href": "02_scripting.html",
    "title": "2  Introduction to Scripting",
    "section": "",
    "text": "2.1 What we’re working towards\nBy the end of this session, you should be able to understand and run this shell script.\nIt seems a little intimidating, but we will take this apart line by line.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Scripting</span>"
    ]
  },
  {
    "objectID": "02_scripting.html#what-were-working-towards",
    "href": "02_scripting.html#what-were-working-towards",
    "title": "2  Introduction to Scripting",
    "section": "",
    "text": "#!/bin/bash\nmodule load SAMtools/1.19.2-GCC-13.2.0  #load the module\nsamtools view -c $1 &gt; $1.counts.txt     #run the script \nmodule purge                            #purge the module",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Scripting</span>"
    ]
  },
  {
    "objectID": "02_scripting.html#motivation",
    "href": "02_scripting.html#motivation",
    "title": "2  Introduction to Scripting",
    "section": "2.2 Motivation",
    "text": "2.2 Motivation\nThere is a rule in programming: if you do something more than 3 times, you should consider making it into a script or function.\nFor example, imagine that you use samtools view -c all the time with certain options and you want to save the output. You can put this command and options into a shell script that takes named files as an argument (such as samcount.sh. Instead of typing samtools stat over and over again, you can run\n./samcount.sh my_file.bam\nand it will output a file my_file.bam.counts.txt that has the counts we are interested in.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Scripting</span>"
    ]
  },
  {
    "objectID": "02_scripting.html#editing-on-a-linux-machine",
    "href": "02_scripting.html#editing-on-a-linux-machine",
    "title": "2  Introduction to Scripting",
    "section": "2.3 Editing on a Linux Machine",
    "text": "2.3 Editing on a Linux Machine\nOn the rhino machines, we have the option to use the nano editor. nano is the most like a word processor or code editors.\n\nOpen a file in nano: nano &lt;filename&gt;\nSave and quit: &lt;CTRL&gt; + x and then yes\nNavigate in file: using the arrow keys will work\nFind in file: &lt;CTRL&gt; + w\nCopy from outside the terminal (dependent on terminal program)\n\n\n2.3.1 Try it Out\nTry making your own file called my_file.txt:\nnano my_file.txt\nAdd some text to it.\nUse CTRL-X to exit, and make sure to select “Yes” to save.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Scripting</span>"
    ]
  },
  {
    "objectID": "02_scripting.html#sec-shebang",
    "href": "02_scripting.html#sec-shebang",
    "title": "2  Introduction to Scripting",
    "section": "2.4 The first line: the she-bang",
    "text": "2.4 The first line: the she-bang\nWhat’s this first line?\n#!/bin/bash\nthe #! is known as a she-bang - it’s a signal to Linux what language interpreter to use when running the script on the command line. In our case, we want to use bash. But we can also make python scripts executable by putting the path to python:\n#!/\nThe she-bang is necessary if you want to run the script without using the bash command (after you have made it executable):\n./samcount.sh chr1.sam",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Scripting</span>"
    ]
  },
  {
    "objectID": "02_scripting.html#sec-modules",
    "href": "02_scripting.html#sec-modules",
    "title": "2  Introduction to Scripting",
    "section": "2.5 Software Modules",
    "text": "2.5 Software Modules\nOk, we’ve gotten comfortable navigating around the HPC filesystem. Now how do we run executables on files?\nLet’s talk about the two problems:\n\nHow do we find executables on a cluster, and\nhow do we load them up and run them?\n\n\n2.5.1 Is my software already installed?\nSay we want to see if samtools is installed on our HPC. One of the key commands you can use to find software is the which command. If your software is installed, which will give you the path where the software is installed. For example, I can see if bash is installed:\nwhich bash\nWhich gives me the response:\n/bin/bash\nSo, let’s see if samtools is installed:\nwhich samtools\nWhich gives no response, so where is samtools?\nIf we don’t have samtools immediately available, how do we find it on our system? On the HPC system, We can use environment modules to load software.\n\n\n2.5.2 Environment Modules\nBefore you install your own versions of software, it’s important to realize that this problem may be solved for you.\nYour first stop should be looking for environment modules on the HPC. Not all HPCs have these, but if they have them, this should be your first stop to find executables.\nlmod is a system for loading and unloading software modules. It is usually installed on HPCs. The commands all start with module, and there are a number of ones that are useful for you.\n\nmodule avail\nmodule load\nmodule purge\n\nIf you want to see the current list of available modules and their names, check them out here.\nLooking for samtools on that page, we discovered the name of our module:\nSAMtools\nSo, that’s what we’ll use to load up samtools.\n\n\n2.5.3 module load\nHere’s the next line of the script:\nmodule load SAMtools/1.19.2-GCC-13.2.0  #load the module\nOur module name is SAMtools, and the 1.19.2-GCC-13.2.0 after it is the version of that module.\n\n\n\n\n\n\nFor FH Users: Modules benefit everyone\n\n\n\nIf there is a particular bit of software that you need to run on the FH cluster that’s not there, make sure to request it from SciComp. Someone else probably needs it and so making it known so they can add it as a Environment module will help other people.\n\n\n\n\n\n\n\n\nFor FH Users\n\n\n\nOn the FH cluster, ml is a handy command that combines module load and module avail.\nYou can load a module with ml &lt;module_name&gt;.\n\n\n\n\n2.5.4 Tip: Load only as many modules as you need at a time\nOne of the big issues with bioinformatics software is that the toolchain (the software dependencies needed to run the software) can be different for different executables. So when possible, load only one or two modules at a time for each step of your analysis. When you’re done with that step, use module purge to clear out the software environment.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Scripting</span>"
    ]
  },
  {
    "objectID": "02_scripting.html#sec-positional",
    "href": "02_scripting.html#sec-positional",
    "title": "2  Introduction to Scripting",
    "section": "2.6 $1: A Positional argument",
    "text": "2.6 $1: A Positional argument\nThe next line of our script is this:\nsamtools view -c $1 &gt; $1.counts.txt  \nLet’s take a look at the command that we’re running first. We’re going to run samtools view -c, which will give us counts on an incoming bam or sam file and save it in a file. We want to be able to run our script like this:\nbash samtools_count.sh my_file.bam \nWhen we run it like that, samtools_count.sh will run samtools view -c like this:\nsamtools view -c my_file.bam &gt; my_file.bam.counts.txt\nSo what’s going on here is that there is some substitution using common arguments. Let’s look at these.\n\n\n\n\n\n\n&gt; - redirecting outputs to a file\n\n\n\nThe &gt; in the script means that we are going to direct the output of samtools view -c into a file.\nIf we didn’t do this, samtools_count.sh would output everything to console.\nMuch more info about this when we talk about the different outputs to console.\n\n\n\n2.6.1 Positional Arguments such as $1\nHow did the script know where to substitute each of our arguments? It has to do with the argument variables. Arguments (terms that follow our command) are indexed starting with the number 1. We can access the value at the first position using the special variable $1.\nNote that this works even in quotes.\nSo, to unpack our script, we are substituting our first argument for the $1.\nWhen we run this script, it will output into our current directory, which is the top level of the bash_for_bio folder. We can change the behavior of the script by doing the following:\nsamtools view -c $1 &gt; $2 \nWith the added complication that we need to specify a path for our counts file.\n\n\n\n\n\n\nTest yourself\n\n\n\nHow would we rewrite sam_run.sh (shown below) if we wanted to specify the output file as the first argument and the bam file as the second argument?\n#!/bin/bash/\nsamtools view -c $1 &gt; $2\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nFor this script, we would switch the positions of $1 and $2.\n#!/bin/bash/\nsamtools stats $2 &gt; $1",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Scripting</span>"
    ]
  },
  {
    "objectID": "02_scripting.html#module-purge",
    "href": "02_scripting.html#module-purge",
    "title": "2  Introduction to Scripting",
    "section": "2.7 module purge",
    "text": "2.7 module purge\nThe last line of our script is:\nmodule purge\nThis line will unload the modules from memory. It’s good practice to unload modules when you’re done with them, especially since they have complex chains of dependencies, and the versions of these dependencies can interfere with other packages.\n\n\n\n\n\n\nProcessing Files Best Practices\n\n\n\nOne thing to remember is to not touch the raw data. The original files should remain untouched.\nA good way to do this is to have your outputs saved in a different folder.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Scripting</span>"
    ]
  },
  {
    "objectID": "02_scripting.html#sec-bash-variables",
    "href": "02_scripting.html#sec-bash-variables",
    "title": "2  Introduction to Scripting",
    "section": "2.8 Variables in Bash Scripts",
    "text": "2.8 Variables in Bash Scripts\nWe saw a little bit about using $1, which is a variable in our Bash scripts. Let’s talk about declaring variables in bash scripts and using them using variable expansion.\nIn Bash, we can declare a variable by using &lt;variable_name&gt;=&lt;value&gt;. Note there are no spaces between the variable (my_variable), equals sign, and the value (\"ggplot2\").\nmy_variable=\"ggplot2\"\n\necho \"My favorite R package is ${my_variable}\"\nMy favorite R package is ggplot2\nTake a look at line 3 above. We expand the variable (that is, we substitute the actual variable) by using ${my_variable} in our echo statement.\nIn general, when expanding a variable in a quoted string, it is better to use ${my_variable} (the variable name in curly brackets). This is especially important when using the variable name as part of a string:\nmy_var=\"chr1\"\necho \"${my_var}_1.vcf.gz\"\nchr1_1.vcf.gz\nIf we didn’t use the braces here, like this:\necho \"$my_var_1.vcf.gz\"\nBash would look for the variable $my_var_1, which doesn’t exist. So use the curly braces {} when you expand variables. It’s safer overall.\nThere is an alternate method for variable expansion which we will use when we call a sub-shell - a shell within a shell. We need to use parentheses () to expand them within the sub-shell, but not the top-shell. We’ll use this when we process multiple files within a single worker.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Scripting</span>"
    ]
  },
  {
    "objectID": "02_scripting.html#putting-it-all-together",
    "href": "02_scripting.html#putting-it-all-together",
    "title": "2  Introduction to Scripting",
    "section": "2.9 Putting it all together",
    "text": "2.9 Putting it all together\nWe can run the script on a file:\n./scripts/samcount.sh data/my_sam_file.sam\n\n\n\n\n\n\nTry it Out",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Scripting</span>"
    ]
  },
  {
    "objectID": "02_scripting.html#aligning-a-file",
    "href": "02_scripting.html#aligning-a-file",
    "title": "2  Introduction to Scripting",
    "section": "2.10 Aligning a file",
    "text": "2.10 Aligning a file\nHow about running the bwa-mem aligner? Let’s run a FASTQ file into it:\n#!/bin/bash\nmodule load BWA/0.7.17-GCCcore-11.2.0\nREF_GENOME=\"/shared/biodata/reference/iGenomes/Homo_sapiens/UCSC/hg19/Sequence/BWAIndex/genome.fa\"\nbwa mem ${REF_GENOME} $1 &gt; $1.aligned.sam\nmodule purge",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Scripting</span>"
    ]
  },
  {
    "objectID": "03_batch.html",
    "href": "03_batch.html",
    "title": "4  Batch Processing and Submitting Jobs",
    "section": "",
    "text": "4.1 Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Batch Processing and Submitting Jobs</span>"
    ]
  },
  {
    "objectID": "03_batch.html#sec-xargs",
    "href": "03_batch.html#sec-xargs",
    "title": "3  Batch Processing and Submitting Jobs",
    "section": "3.3 Advanced Batch Processing: Iterating using xargs",
    "text": "3.3 Advanced Batch Processing: Iterating using xargs\nA really common pattern is taking a delimited list of files and doing something with them. We can do some useful things such as seeing the first few lines of a set of files, or doing some sort of processing with the set of jobs.\n\n\n\n\n\n\nDon’t xargs for HPC jobs\n\n\n\nYou might be tempted to use xargs with srun to work on a bunch of files. It’s worth trying once so you can see the mechanics of how jobs are processed.\nIn general, I don’t recommend it in practice because if you spawn 1000 jobs using xargs, there’s no real mechanism to terminate that 1000 jobs, except one by one. With sbatch, all your jobs in batch mode run as subjobs, which means you can terminate the parent job to terminate all of the subjobs.\nAgain, this is a good reason to use a workflow runner in your day to day work. You don’t have to worry about jobs and subjobs. It takes a little setup, but it will make your life easier in general.\n\n\nLet’s start out with a list of files:\nsource ~/.bashrc #| hide_line\nls data/*.sh\ndata/batch-on-worker.sh\nNow we have a list of files, let’s look at the first few lines of each of them, and print a separator --- for each.\n#| filename: scripting-basics/xargs_example.sh\nsource ~/.bashrc #| hide_line\nls data/*.sh | xargs -I% sh -c 'head %; echo \"\\n---\\n\"'\n#!/bash/bin\n\ncmd_to_run=\"ls *.vcf.gz | xargs -I% sh -c \"bcftools stats % &gt; %.stats.txt\"\n\ndx run swiss-army-knife \\\n  -iin=\"data/chr1.vcf.gz\" \\\n  -iin=\"data/chr2.vcf.gz\" \\\n  -iin=\"data/chr3.vcf.gz\" \\\n  -icmd=${cmd_to_run}\n---\ndx find data --name \"*.bam\" --brief\n---\nLet’s take this apart piece by piece.\nxargs takes an -I argument that specifies a placeholder. In our case, we are using % as our placeholder in this statement.\nWe’re passing on each filename from ls into the following code:\nsh -c 'head %; echo \"---\\n\"'\nThe sh -c opens a subshell so that we can execute our command for each of the files in our list. We’re using sh -c to run:\n'head %; echo \"---\\n\"'\nSo for our first file, 01-scripting-basics.qmd, we are substituting that for % in our command:\n'head hpc-basics.qmd; echo \"---\\n\"'\nFor our second file, hpc-basics.qmd, we would substitute that for the %:\n'head hpc-basics.qmd; echo \"---\\n\"'\nUntil we cycle through all of the files in our list.\n\n3.3.1 The Basic xargs pattern\n\n\n\n\n\n\n\n\ngraph LR\n  A[\"ls *.bam\"] --&gt; B{\"|\"} \n  B --&gt; C[\"xargs -I% sh -c\"] \n  C --&gt; D[\"command_to_run %\"]\n\n\n\n\n\n\n\n\nFigure 3.1: Basics of using xargs to iterate on a list of files\n\n\n\nAs you cycle through lists of files, keep in mind this basic pattern (Figure 3.1):\nls &lt;wildcard&gt; | xargs -I% sh -c \"&lt;command to run&gt; %\"\n\n\n\n\n\n\nTest Yourself\n\n\n\nHow would we modify the below code to do the following?\n\nList only .json files in our data/ folder using ls\nUse tail instead of head\n\nls *.txt | xargs -I% sh -c \"head %; echo '---\\n'\"\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nls data/*.json | xargs -I% sh -c \"tail %; echo '---\\n'\"\n\n\n\n\n\n\n\n\n\nWhy this is important on HPC\n\n\n\nWe can use xargs to execute small batch jobs on a small number of files. This especially becomes powerful on the cluster when we use ls to list files in our HPC project.\nNote that as we graduate to workflow tools like WDL/Nextflow, there are other mechanisms for running jobs on multiple files (such as WDL/Cromwell) that we should move to.\nTrust me; you don’t want to have to handle iterating through a huge directory and handling when routines give an error, or your jobs get interrupted. Rerunning and resuming failed jobs are what workflow runner tools excel at.\n\n\n\n\n3.3.2 For more information\nhttps://www.baeldung.com/linux/xargs-multiple-arguments",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Batch Processing and Submitting Jobs</span>"
    ]
  },
  {
    "objectID": "03_batch.html#batching-on-hpc",
    "href": "03_batch.html#batching-on-hpc",
    "title": "4  Batch Processing and Submitting Jobs",
    "section": "4.3 Batching on HPC",
    "text": "4.3 Batching on HPC\nNow we can start to do more advanced things on the HPC: use one machine to process each file.\nLet’s start out with “Simple Linux Utility for Resource Management” - it handles scheduling and distributing jobs among the compute nodes in the cluster.SLURM scripts.\n\n4.3.1 SLURM Scripts\nSLURM scripts are a special kind of shell script that contain additional information for the SLURM manager. This includes:\n\nNumber of nodes (machines) to request\nMemory and CPU requirements for each machine\n\nWe specify these using a special kind of comment: SLURM directives. Directives begin a line with #SBATCH:\n#SBATCH --nodes=1 \nIn this example, we are specifying the number of nodes.\n\n\n4.3.2 SLURM Directives\nWe are able to set some configuration on running our jobs.\n#!/bin/bash\n1#SBATCH --nodes=1\n2#SBATCH --array=1-3\n3#SBATCH --mem-per-cpu=1gb\n4#SBATCH --time=00:05:00\n5./samtools_opt sort SRR1576820_000${SLURM_ARRAY_TASK_ID}.bam -o SRR1576820_000${SLURM_ARRAY_TASK_ID}.sorted.bam\n\n1\n\nrequest 1 node\n\n2\n\nstart an array\n\n3\n\nrequest 1 gigabyte per cpu\n\n4\n\nask for 5 minutes on the node\n\n5\n\nRun samtools sort on a bam file, and output it (will do for the whole job array)\n\n\n\n\n\n\n\n\nMore about directives\n\n\n\nMuch more information about the kinds of directives that you can specify in a SLURM script is available here: https://www.osc.edu/supercomputing/batch-processing-at-osc/slurm_directives_summary\nThe most important directive you should be aware of is how\n\n\n\n\n4.3.3 Job Arrays\nThis line:\n#SBATCH --array=1-6 \nWill create a job array. This will create a variable called $SLURM_ARRAY_TASK_ID that will cycle through the numbers 1-6. Each Task ID corresponds to a different subjob. Let’s try a simpler script to show what’s going on:\n#| eval: false\n#| filename: sbatch_test.sh\n#!/bin/bash\n#SBATCH --array=1-3\n#SBATCH --nodes=1\necho \"${SLURM_ARRAY_TASK_ID} job\"\nThis is a minimal script that will execute 3 subjobs. It will cycle through the job array and print the array number for each job.\n#| eval: false\nsbatch sbatch_test.sh\nOn submitting, we will get a message like this (your job number will be different):\nSubmitted batch job 26328834\nAnd if we look for the output files:\nls -l slurm-26328834*\nWe will get the following output:\n-rw-rw---- 1 tladera2 g_tladera2 8 Jul 15 13:50 slurm-26328834_1.out\n-rw-rw---- 1 tladera2 g_tladera2 8 Jul 15 13:50 slurm-26328834_2.out\n-rw-rw---- 1 tladera2 g_tladera2 8 Jul 15 13:50 slurm-26328834_3.out\nTaking a look at one of these files using cat:\ncat slurm-26328834_3.out\nWe’ll see this:\n3 job\n\n\n\n\n\ngraph TD\n  A[\"sbatch sbatch_test.sh\"] --\"1\"--&gt; B\n  B[\"echo 1 job\"]\n  A --\"2\"--&gt; C[\"echo 2 job\"]\n  A --\"3\"--&gt; D[\"echo 3 job\"]\n\n\n\n\n\n\nWhat happened here? sbatch submitted our job array as 3 different subjobs to 3 different nodes under a single job id. Each node then outputs a file with the subjob id that contains the job number.\n\n\n4.3.4 Processing files using Job Arrays\nSo now we know that ${SLURM_ARRAY_TASK_ID} will let us specify a subjob within our script, how do we use it in our script?\n\n\n4.3.5 scanceling a job array\nAs we noted, one of the strengths in using a job array to process multiple files is that they are spawed as sub or child jobs of a parent job id.\nWhat if we made a mistake? We can use the scancel command to cancel the entire set of jobs by giving it our parent job id:\nscancel 26328834\nThis will cancel all sub jobs related to the parent job.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Batch Processing and Submitting Jobs</span>"
    ]
  },
  {
    "objectID": "03_batch.html#containers",
    "href": "03_batch.html#containers",
    "title": "4  Batch Processing and Submitting Jobs",
    "section": "4.4 Containers",
    "text": "4.4 Containers\nWe already learned about software modules (Section 2.5). There is an alternative way to use software: using a The executable software environment actually installed and running on a machine. Runnable. Generate from docker pull from a repository.container.\n\n4.4.1 What is a Container?\nA container is a self-contained unit of software. It contains everything needed to run the software on a variety of machines. If you have the container software installed on your machine, it doesn’t matter whether it is MacOS, Linux, or Windows - the container will behave consistently across different operating systems and architectures.\nThe container has the following contents:\n\nSoftware - The software we want to run in a container. For bioinformatics work, this is usually something like an aligner like bwa, or utilities such as samtools\nSoftware Dependencies - various software packages needed to run the software. For example, if we wanted to run tidyverse in a container, we need to have R installed in the container as well.\nFilesystem - containers have their own isolated filesystem that can be connected to the “outside world” - everything outside of the container. We’ll learn more about customizing these with bind paths (Section 5.1.6).\n\nIn short, the container has everything needed to run the software. It is not a full operating system, but a smaller mini-version that cuts out a lot of cruft.\nContainers are When you are finished with Docker containers, everything that you created in them will disappear when you finish running scripts in the container.ephemeral. They leverage the the file system of their host to manage files. These are called both Volumes (the Docker term) and Bind Paths (the apptainer term).\n\n\n4.4.2 Docker vs. Apptainer\nThere are two basic ways to run Docker containers:\n\nUsing the Docker software\nUsing the Apptainer software (for HPC systems)\n\nIn general, Docker is used on systems where you have a high level of access to the system. This is because docker uses a special user group called docker that has essentially root level privileges. This is not something to be taken lightly.\nThis is not the case for HPC systems, which are shared and granting this level of access to many people is not practical. This is when we use A container software that is often used on HPC systems. Can run Docker containers.Apptainer (which used to be called Singularity), which requires a much lower level of user privileges to execute tasks. For more info, see Section 7.2.1 .\n\n\n\n\n\n\nBe Secure\n\n\n\nBefore we get started, security is always a concern when running containers. The docker group has elevated status on a system, so we need to be careful that when we’re running them, these containers aren’t introducing any system vulnerabilities. Note that on HPC systems, the main mechanism for running containers is apptainer, which is designed to be more secure.\nThese are mostly important when running containers that are web-servers or part of a web stack, but it is also important to think about when running jobs on HPC.\nHere are some guidelines to think about when you are working with a container.\n\nUse vendor-specific Docker Images when possible.\nUse container scanners to spot potential vulnerabilities. DockerHub has a vulnerability scanner that scans your Docker images for potential vulnerabilities. For example, the WILDS Docker library employs a vulnerability scanner and the containers are regularly patched to prevent vulnerabilities.\nAvoid kitchen-sink images. One issue is when an image is built on top of many other images. It makes it really difficult to plug vulnerabilities. When in doubt, use images from trusted people and organizations. At the very least, look at the Dockerfile to see that suspicious software isn’t being installed.\n\n\n\n\n\n4.4.3 Common Containers for Bioinformatics\n\nGATK (the genome analysis toolkit) is one common container that we can use for analysis.\n\n\n\n\n4.4.4 The WILDS Docker Library\nThe Data Science Lab has a set of Docker containers for common Bioinformatics tasks available in the WILDS Docker Library. These include:\n\nsamtools\nbcftools\nmanta\ncnvkit\ndeseq2\n\nAmong many others. Be sure to check it out before you start building your own containers.\n\n\n4.4.5 Pulling a Docker Container\nLet’s pull a docker container from the Docker registry. Note we have to specify docker:// when we pull the container, because Apptainer has its own internal format called SIF.\nmodule load Apptainer/1.1.6\napptainer pull docker://ghcr.io/getwilds/scanpy:latest\napptainer run --bind /path/to/data:/data,/path/to/script:/script docker://getwilds/scanpy:latest python /script/example.py",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Batch Processing and Submitting Jobs</span>"
    ]
  },
  {
    "objectID": "04_containers_workflows.html",
    "href": "04_containers_workflows.html",
    "title": "6  Workflows",
    "section": "",
    "text": "6.1 Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Workflows</span>"
    ]
  },
  {
    "objectID": "04_containers_workflows.html#visual-table-of-contents",
    "href": "04_containers_workflows.html#visual-table-of-contents",
    "title": "4  Containers and Workflows",
    "section": "4.4 Visual Table of Contents",
    "text": "4.4 Visual Table of Contents\n\n\n\n\n\nflowchart TD\n   B[\"Open Shell in Container\\n(with Bindpaths)\"]\n   B --&gt; D[Test Scripts in container]\n   D --&gt; E[Exit Container]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Containers and Workflows</span>"
    ]
  },
  {
    "objectID": "04_containers_workflows.html#sec-open-container",
    "href": "04_containers_workflows.html#sec-open-container",
    "title": "4  Containers and Workflows",
    "section": "4.5 Testing code in a container",
    "text": "4.5 Testing code in a container\nIn this section, we talk about testing scripts in a container using apptainer. We use apptainer (formerly Singularity) in order to run Docker containers on a shared HPC system. This is because Docker itself requires root-level privileges, which is not secure on shared systems.\nIn order to do our testing, we’ll first pull the Docker container, map our bind point (so our container can access files outside of its file system), and then run scripts in the container.\nEven if you aren’t going to frequently use Apptainer in your work, I recommend trying an interactive shell in a container at least once or twice to learn about the container filesystem and conceptually understand how you connect it to the external filesystem.\n\n4.5.1 Pulling a Docker Container\nLet’s pull a docker container from the Docker registry. Note we have to specify docker:// when we pull the container, because Apptainer has its own internal format called SIF.\nmodule load Apptainer/1.1.6\napptainer pull docker://biocontainers/samtools:v1.9-4-deb_cv1",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Containers and Workflows</span>"
    ]
  },
  {
    "objectID": "04_containers_workflows.html#opening-a-shell-in-a-container-with-apptainer-shell",
    "href": "04_containers_workflows.html#opening-a-shell-in-a-container-with-apptainer-shell",
    "title": "4  Containers and Workflows",
    "section": "4.6 Opening a Shell in a Container with apptainer shell",
    "text": "4.6 Opening a Shell in a Container with apptainer shell\nWhen you’re getting started, opening a shell using Apptainer can help you test out things like filepaths and how they’re accessed in the container. It’s hard to get an intuition for how file I/O works with containers until you can see the limited view from the container.\nBy default, apptainers can see your current directory and navigate to the files in it.\nYou can open an Apptainer shell in a container using apptainer shell. Remember to use docker:// before the container name. For example:\nmodule load Apptainer/1.1.6\napptainer shell docker://biocontainers/samtools:v1.9-4-deb_cv1\nThis will load the apptainer module, and then open a Bash shell in the container using apptainer shell. Once you’re in the container, you can test code, especially seeing whether your files can be seen by the container (see Section 4.7). 90% of the issues with using Docker containers has to do with bind paths, so we’ll talk about that next.\nOnce you’re in the shell, you can take a look at where samtools is installed:\nwhich samtools\nNote that the container filesystem is isolated, and we need to explicitly build connections to it (called bind paths) to get files in and out. We’ll talk more about this in the next section.\nOnce we’re done testing scripts in our containers, we can exit the shell and get back into the node.\nexit\n\n\n\n\n\n\nOpening a Shell in a Docker Container with Docker\n\n\n\nFor the most part, due to security reasons, we don’t use docker on HPC systems. In short, the docker group essentially has root-level access to the machine, and it’s not a good for security on a shared resource like an HPC. However, if you have admin level access (for example, on your own laptop), you can open up an interactive shell with docker run:\ndocker run -it biocontainers/samtools:v1.9-4-deb_cv1 /bin/bash\nThis will open a bash shell much like apptainer shell. Note that volumes (the docker equivalent of bind paths) are specified differently in Docker compared to Apptainer.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Containers and Workflows</span>"
    ]
  },
  {
    "objectID": "04_containers_workflows.html#sec-bindpaths",
    "href": "04_containers_workflows.html#sec-bindpaths",
    "title": "4  Containers and Workflows",
    "section": "4.7 Testing out bind paths in containers",
    "text": "4.7 Testing out bind paths in containers\nOne thing to keep in mind is that every container has its own filesystem. One of the hardest things to wrap your head around for containers is how their filesystems work, and how to access files that are outside of the container filesystem. We’ll call any filesystems outside of the container external filesystems to make the discussion a little easier.\nBy default, the containers have access to your current working directory. We could make this where our scripts live (such as /home/tladera2/), but because our data is elsewhere, we’ll need to specify that location (/fh/fast/mylab/) as well.\nThe main mechanism we have in Apptainer to access the external filesystem are bind paths. Much like mounting a drive, we can bind directories from the external filesystem using these bind points.\n\n\n\n\n\nflowchart LR\n   B[\"External Directory\\n/fh/fast/mydata/\"] \n   B --read--&gt; C\n   C --write--&gt; B\n   A[\"Container Filesystem\\n/mydata/\"]--write--&gt;C(\"--bind /fh/fast/mydata/:/mydata/\")\n   C --read--&gt; A\n\n\n\n\n\n\nI think of bind paths as “tunnels” that give access to particular folders in the external filesystem. Once the tunnel is open, we can access data files, process them, and save them using the bind path.\nSay my data lives in /fh/fast/mydata/. Then I can specify a bind point in my apptainer shell and apptainer run commands.\nWe can do this with the --bind option:\napptainer shell --bind /fh/fast/mydata:/mydata docker://biocontainers/samtools:v1.9-4-deb_cv1\nNote that the bind syntax doesn’t have the trailing slash (/). That is, note that it is:\n--bind /fh/fast/mydata: ....\nRather than\n--bind /fh/fast/mydata/: ....\nNow our /fh/fast/mydata/ folder will be available as /mydata/ in my container. We can read and write files to this bind point. For example, I’d refer to the .bam file /fh/fast/mydata/my_bam_file.bam as:\nsamtools view -c /mydata/my_bam_file.bam\n\n\n\n\n\n\nWDL makes this way easier\n\n\n\nA major point of failure with Apptainer scripting is when our scripts aren’t using the right bind paths. It becomes even more complicated when you are running multiple steps.\nThis is one reason we recommend writing WDL Workflows and a A system that works with the cluster manager to orchestrate processing data through a workflowworkflow manager (such as A workflow runner. Currently works with WDL files.Cromwell or Sprocket) to run your workflows. You don’t have to worry that your bind points are setup correctly, because they are handled by the workflow manager.\n\n\n\n4.7.1 Testing in the Apptainer Shell\nOk, now we have a bind point, so now we can test our script in the shell. For example, we can see if we are invoking samtools in the correct way and that our bind points work.\nsamtools view -c /mydata/my_bam_file.bam &gt; /mydata/bam_counts.txt\nAgain, trying out scripts in the container is the best way to understand what the container can and can’t see.\n\n\n4.7.2 Exiting the container when you’re done\nYou can exit, like any shell you open. You should be out of the container. Confirm by using hostname to make sure you’re out of the container.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Containers and Workflows</span>"
    ]
  },
  {
    "objectID": "04_containers_workflows.html#testing-outside-of-the-container",
    "href": "04_containers_workflows.html#testing-outside-of-the-container",
    "title": "4  Containers and Workflows",
    "section": "4.8 Testing outside of the container",
    "text": "4.8 Testing outside of the container\nLet’s take everything that we learned and put it in a script that we can run on the HPC:\n# Script to samtools view -c an input file:\n# Usage: ./run_sam.sh &lt;my_bam_file.bam&gt;\n# Outputs a count file: my_bam_file.bam.counts.txt\n#!/bin/bash\nmodule load Apptainer/1.1.6\napptainer run --bind /fh/fast/mydata:/mydata docker://biocontainers/samtools:v1.9-4-deb_cv1 samtools view -c /mydata/$1 &gt; /mydata/$1.counts.txt\n#apptainer cache clean\nmodule purge\nWe can use this script by the following command:\n./run_sam.sh chr1.bam \nAnd it will output a file called chr1.bam.counts.txt.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Containers and Workflows</span>"
    ]
  },
  {
    "objectID": "04_containers_workflows.html#apptainer-cache",
    "href": "04_containers_workflows.html#apptainer-cache",
    "title": "4  Containers and Workflows",
    "section": "4.4 Apptainer Cache",
    "text": "4.4 Apptainer Cache\nThe apptainer cache is where your docker images live. They are translated to the native apptainer .sif format.\nYou can see what’s in your cache by using\napptainer cache list\nBy default the cache lives at ~/.apptainer/cache.\nIf you need to clear out the cache, you can run\napptainer cache clean\nto clear out the cache.\nThere are a number of environment variables (Section 6.5) that can be set, including login tokens for pulling from a private registry. More information is here.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Containers and Workflows</span>"
    ]
  },
  {
    "objectID": "04_containers_workflows.html#entrypoints",
    "href": "04_containers_workflows.html#entrypoints",
    "title": "4  Containers and Workflows",
    "section": "4.5 Entrypoints",
    "text": "4.5 Entrypoints\nYou can think of an entrypoint as a way of automatically starting something up in your container when you run it. For example, if you have a web stack container, one of the things you want to start is a web server when you start running it.\nThe main reason to be aware of entrypoints is when they exist in a Dockerfile. It is important to know what is started when you start running a container.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Containers and Workflows</span>"
    ]
  },
  {
    "objectID": "04_containers_workflows.html#more-info",
    "href": "04_containers_workflows.html#more-info",
    "title": "4  Containers and Workflows",
    "section": "4.6 More Info",
    "text": "4.6 More Info\n\nCarpentries Section on Apptainer Paths - this is an excellent resource if you want to dive deeper into undestanding container filesystems and bind points.\nApptainer Documentation on Bind Paths. There are a lot of good examples here on how to set up bind paths.\nMore about bind paths and other options.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Containers and Workflows</span>"
    ]
  },
  {
    "objectID": "configuring.html",
    "href": "configuring.html",
    "title": "8  Appendix: Configuring your Shell",
    "section": "",
    "text": "8.1 .bashrc: Where do I put my configuration?\nThere is a file in your home directory called .bashrc. This is where you can customize the way the Bash shell behaves.\nThere are 2 things you should know how to set:",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Appendix: Configuring your Shell</span>"
    ]
  },
  {
    "objectID": "configuring.html#sec-bashrc",
    "href": "configuring.html#sec-bashrc",
    "title": "8  Appendix: Configuring your Shell",
    "section": "",
    "text": "Aliases\nEnvironment Variables, especially $PATH",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Appendix: Configuring your Shell</span>"
    ]
  },
  {
    "objectID": "configuring.html#an-example-.bashrc-file",
    "href": "configuring.html#an-example-.bashrc-file",
    "title": "8  Appendix: Configuring your Shell",
    "section": "8.3 An example .bashrc file",
    "text": "8.3 An example .bashrc file",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Appendix: Configuring your Shell</span>"
    ]
  },
  {
    "objectID": "configuring.html#aliases",
    "href": "configuring.html#aliases",
    "title": "8  Appendix: Configuring your Shell",
    "section": "8.4 Aliases",
    "text": "8.4 Aliases\nAliases are shortcuts for commands. You can specify them using alias as a line in your .bashrc file:\nalias ll='ls -la'\nWe are defining an alias called ll that runs ls -la (long listing for directory for all files) here. Once\nSome people even add aliases for things they mistype frequently.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Appendix: Configuring your Shell</span>"
    ]
  },
  {
    "objectID": "configuring.html#sec-environment",
    "href": "configuring.html#sec-environment",
    "title": "8  Appendix: Configuring your Shell",
    "section": "8.5 Environment Variables",
    "text": "8.5 Environment Variables\nEnvironment variables are variables which can be seen globally in the Linux (or Windows) system across executables.\nYou can get a list of all set environment variables by using the env command. Here’s an example from my own system:\nenv\nSHELL=/bin/bash\nNVM_INC=/home/tladera2/.nvm/versions/node/v21.7.1/include/node\nWSL_DISTRO_NAME=Ubuntu\nNAME=2QM6TV3\nPWD=/home/tladera2\nLOGNAME=tladera2\n[....]\nOne common environment variable you may have seen is $JAVA_HOME, which is used to find the Java Software Development Kit (SDK). (I usually encounter it when a software application yells at me when I haven’t set it.)\nYou can see whether an environment variable is set using echo, such as\necho $PATH\n/home/tladera2/.local/bin:/home/tladera2/gems/bin:/home/tladera2/.nvm/versions/node/v21.7.1/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/ [....]\n\n8.5.1 Setting Environment Variables\nIn Bash, we use the export command to declare an environment variable. For example, if we wanted to declare the environment variable $SAMTOOLS_PATH we’d do the following:\n# works: note no spaces\nexport SAMTOOLS_PATH=\"/home/tladera2/miniconda/bin/\"\nOne thing to note is that spacing matters when you declare environment variables. For example, this won’t declare the $SAMTOOLS_PATH variable:\n# won't work because of spaces\nexport SAMTOOLS_PATH = \"/home/tladera2/miniconda/bin/\"\nAnother thing to note is that we declare environment variables differently than we use them. If we wanted to use SAMTOOLS_PATH in a script, we use a dollar sign ($) in front of it:\n${SAMTOOLS_PATH}/samtools view -c $input_file\nIn this case, the value of $SAMTOOLS_PATH will be expanded (substituted) to give the overall path:\n/home/tladera2/miniconda/bin/samtools view -c $input_file\n\n\n8.5.2 A Very Special Environment Variable: $PATH\nThe most important environment variable is the $PATH variable. This variable is important because it determines where to search for software executables (also called binaries). If you have softwware installed by a package manager (such as miniconda), you may need to add the location of your executables to your $PATH.\nWe can add more directories to the $PATH by appending to it. You might have seen the following bit of code in your .bashrc:\nexport PATH=$PATH:/home/tladera2/samtools/\nIn this line, we are adding the path /home/tladera2/samtools/ to our $PATH environment variable. Note that how we refer to the PATH variable is different depending on which side the variable is on of the equals sign.\n\n\n\n\n\n\nOrder matters in your $PATH\n\n\n\n\n\n\nTLDR: We declare the variable using export PATH (no dollar sign) and we append to the variable using $PATH (with dollar sign). This is something that trips me up all the time.\n\n\n\n\n\n\nFor FH Users\n\n\n\nIn general, when you use environment modules on gizmo, you do not need to modify your $PATH variable. You mostly need to modify it when you are compiling executables so that the system can find them. Be sure to use which to see where the environment module is actually located:\nwhich samtools\n\n\n\n\n8.5.3 Making your own environment variables\nOne of the difficulties with working on a cluster is that your scripts may be in one filesystem (/home/), and your data might be in another filesystem (/fh/fast/). And it might be recommended that you transfer over files to a faster-access filesystem (/fh/temp/) to process them.\nYou can set your own environment variables for use in your own scripts. For example, we might define a $TCR_FILE_HOME variable:\nexport TCR_FILE_HOME=/fh/fast/my_tcr_project/\nto save us some typing across our scripts. We can use this new environment variable like any other existing environment variable:\n#!/bin/Bash\nexport my_file_location=$TCR_FILE_HOME/fasta_files/\n\n\n\n\n\n\n.bashrc versus .bash_profile\n\n\n\nOk, what’s the difference between .bashrc and .bash_profile?\nThe main difference is when these two files are sourced. bash_profile is used when you do an interactive login, and .bashrc is used for non-interactive shells.\n.bashrc should contain the environment variables that you use all the time, such as $PATH and $JAVA_HOME for example.\nYou can get the best of both worlds by including the following line in your .bash_profile:\nsource ~/.bashrc\nThat way, everything in the .bashrc file is loaded when you log in interactively.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Appendix: Configuring your Shell</span>"
    ]
  },
  {
    "objectID": "configuring.html#sec-project",
    "href": "configuring.html#sec-project",
    "title": "8  Appendix: Configuring your Shell",
    "section": "8.6 Project/folder based workflows",
    "text": "8.6 Project/folder based workflows\nOn a particular machine, using absolute paths is safe. However, you do this at the cost of portability - code that you write on one machine may not run on another.\nIf you ever anticipate doing the analysis on a separate machine, using project structures with relative paths is the safest. For example, you may want to move from the on-premise FH system to working with the data in AWS.\nHere’s one example of putting everything into a single folder:\nmy_project\n├── data\n│   ├── chr1.fa.gz\n│   ├── chr2.fa.gz\n│   └── chr3.fa.gz\n├── results\n├── run_workflow.sh\n└── scripts\n    └── run_bowtie.sh\nIn the above example, our project is named my_project, and there are three folders inside it: data/, results/, and scripts/. Our main script for running is my_project/run_workflow.sh. Because this script is in the root folder, we can refer to the data/ folder to process files:\n./scripts/run_bowtie.sh data/*.fa.gz results/\nWhen we run run_workflow.sh, it will execute run_bowtie.sh on all of the files in data/, and save them in results/, resulting in the following updated structure.\nmy_project\n├── data\n│   ├── chr1.fa.gz\n│   ├── chr2.fa.gz\n│   └── chr3.fa.gz\n├── results\n│   ├── chr1.bam\n│   ├── chr2.bam\n│   └── chr3.bam\n├── run_workflow.sh\n└── scripts\n    └── run_bowtie.sh\nYou may have seen relative paths such as ../another_directory/ - the .. means to go up a directory in the file hierarchy, and then look in that directory for the another_directory/ directory. I try to avoid using relative paths like these.\nIn general for portability and reproducibility, you will want to use relative paths within a directory, and avoid using relative paths like ../../my_folder, where you are navigating up. In general, use relative paths to navigate down.\n\n\n\n\n\n\nWhy This is Important\n\n\n\nWhen you start executing scripts, it’s important to know where the results go. When you execute SAMtools on a file in /fh/temp/, for example, where does the output go?\nWorkflow Runners such as Cromwell and Nextflow will output into certain file structures by default. This can be changed, but knowing the default behavior is super helpful when you don’t specify an output directory.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Appendix: Configuring your Shell</span>"
    ]
  },
  {
    "objectID": "miscellaneous.html",
    "href": "miscellaneous.html",
    "title": "9  Miscellaneous",
    "section": "",
    "text": "9.1 hostname What Machine am I on?\nOne of the most confusing things about working on HPC is that sometimes you have a shell open on the head node, but oftentimes, you are on a worker node.\nYour totem for telling which node you’re in is hostname, which will give you the host name of the machine you’re on.\nFor example, if I used grabnode to grab a gizmo node for interactive work, I can check which node I’m in by using:\nIf you’re confused about which node you’re in, remember hostname. It will save you from making mistakes, especially when using utilities like screen.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Miscellaneous</span>"
    ]
  },
  {
    "objectID": "miscellaneous.html#workflows",
    "href": "miscellaneous.html#workflows",
    "title": "9  Miscellaneous",
    "section": "9.2 Workflows",
    "text": "9.2 Workflows\n\n9.2.1 One Workflow: /fh/fast/ and /hpc/temp/\nOne approach is to have your scripts also live in your project folder in fast. Then, you can sync the project in /fh/fast/ over to /hpc/temp/, run the scripts in /hpc/temp/, and then sync the two folders again. You can do the file sync’ing in both directions with Motuz (?sec-motuz), which has its own advantages.\nIf you want to go this route, you should think about using a Folder Based Workflow (Section 8.6), where everything lives in a folder.\nAnother thing to consider is to have a backup of the scripts that is either on your own machine or in GitHub. You can do this by using your .gitignore to exclude the data and results.\n\n\n\n\n\ngraph LR\n    A[\"Fast\\n/fh/fast/my_lab/project/\\nRaw Data & Scripts\"] --\"a. Sync Data & scripts\"--&gt;B\n    B[\"Temp\\n/hpc/temp/my_lab/project\\nb. Run Scripts here\"] --\"c. Sync results\"--&gt;A\n\n\n\n\n\n\n\n\n9.2.2 Another Approach\nBelow is a a diagram with another way to work with these multiple filesystems.\n\nWe transfer the raw files to be processed from /fh/fast/ to our directory /fh/temp/. For example, a set of .bam files.\nWe run our scripts from /home/, on the raw files in /fh/temp/ and produce results in /fh/temp/.\nWe transfer our results from /fh/temp/ to /fh/fast/.\n\n\n\n\n\n\ngraph TD\n    A[\"Home Directory\\n/home/tladera2/\\nScripts\"] --\"b. run scripts\"--&gt; C\n    B[\"Fast\\n/fh/fast/tladera2\\nResearch Data\"] --\"a. transfer raw files\"--&gt; C\n    C[\"Temp\\n/fh/temp/tladera2\"] --\"c. transfer results\"--&gt; B",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Miscellaneous</span>"
    ]
  },
  {
    "objectID": "miscellaneous.html#quoting-and-escaping-filenames-in-bash",
    "href": "miscellaneous.html#quoting-and-escaping-filenames-in-bash",
    "title": "9  Miscellaneous",
    "section": "9.3 Quoting and Escaping Filenames in Bash",
    "text": "9.3 Quoting and Escaping Filenames in Bash\nOne point of confusion is when do you quote things in Bash? When do you use single quotes (') versus double-quotes (\")? When do you use \\ to escape characters?\nLet’s talk about some quoting rules in Bash. I’ve tried to make things as simplified and generalized as possible, rather than stating all of the rules for each quote.\n\nIf you have spaces in a filename, use double quotes (\"chr 1.bam\")\nIf you have a single quote in the filename, use double quotes to wrap it (\"ted's file.bam\")\nOnly escape characters when necessary - if you can solve a problem with quotes, use them\nIf you need to preserve an escaped character, use single quotes\n\nLet’s go over each of these with an example.\n\n9.3.1 If you have spaces in a filename, use double quotes (Most common)\nFor example, if your filename is chr 1 file.bam, then use double quotes in your argument\nsamtools view -c \"chr 1 file.bam\"\n\n\n9.3.2 If you have a single quote in the name, use double quotes to wrap it (less common)\nSay you have a file called ted's new file.bam. This can be a problem when you are calling it, especially because of the single quote.\nIn this case, you can do this:\nsamtools view -c \"ted's new file.bam\"\n\n\n9.3.3 Only escape characters when necessary (less common)\nThere are a number of special characters (such as Tab, and Newline) that can be specified as escape characters. In double quotes, characters such as $ are signals to Bash to expand or evaluate code.\nSay that someone had a $ in their file name such as Thi$file is money.bam\nHow do we refer to it? We can escape the character with a backslash \\:\nsamtools view -c \"Thi\\$file is money.bam\"\nThe backslash is a clue to Bash that we don’t want variable expansion in this case. Without it, bash would look for a variable called $file.\n\n\n9.3.4 If you need to preserve an escaped character, use single quotes (least common)\nThis is rarely used, but if you need to keep an escaped character in your filename, you can use single quotes. Say we have a filename called Thi\\$file.bam and you need that backslash in the file name (btw, please don’t do this), you can use single quotes to preserve that backslash:\nsamtools view -c 'Thi\\$file.bam'\nAgain, hopefully you won’t need this.\n\n\n9.3.5 For More Info\nhttps://www.grymoire.com/Unix/Quote.html#uh-3\n\n\n\n\n\n\nWhat about backticks?\n\n\n\nBackticks (`) are an old way to do command evaluation in Bash. For example, if we run the following on the command-line:\necho \"there are `ls -l | wc -l` files in this directory\"\nWill produce:\nthere are       36 files in this directory\nTheir use is deprecated, so you should be using $() in your command evaluations instead:\necho \"there are $(ls -l | wc -l) files in this directory\"\n\n\n\n\n\n\n\n\nWhat about X use case?\n\n\n\nThere are a lot of rules for Bash variable expansion and quoting that I don’t cover here. I try to show you a way to do things that work in multiple situations on the cluster.\nThat’s why I focus on double quotes for filenames and ${} for variable expansion in general. They will work whether your Bash script is on the command line or in an App, or in WDL.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Miscellaneous</span>"
    ]
  },
  {
    "objectID": "miscellaneous.html#using-pipes-stdin-stdout-stderr",
    "href": "miscellaneous.html#using-pipes-stdin-stdout-stderr",
    "title": "9  Miscellaneous",
    "section": "9.4 Using pipes: STDIN, STDOUT, STDERR",
    "text": "9.4 Using pipes: STDIN, STDOUT, STDERR\nWe will need to use pipes to chain our commands together. Specifically, we need to take a command that generates a list of files on the cluster shared filesystem, and then spawns individual jobs to process each file. For this reason, understanding a little bit more about how pipes (|) work in Bash is helpful.\nIf we want to understand how to chain our scripts together into a pipeline, it is helpful to know about the different streams that are available to the utilities.\n\n\n\n\n\n\n\n\ngraph LR\n  A(STDIN) --&gt; E[run_samtools.sh]\n  E --&gt; B(STDOUT)\n  E --&gt; C(STDERR)\n\n\n\n\n\n\n\n\nFigure 9.1: Inputs/outputs to a script\n\n\n\nEvery script has three streams available to it: Standard In (STDIN), Standard Out (STDOUT), and Standard Error (STDERR) (Figure 9.1).\nSTDIN contains information that is directed to the input of a script (usually text output via STDOUT from another script).\nWhy do these matter? To work in a Unix pipeline, a script must be able to utilize STDIN, and generate STDOUT, and STDERR.\nSpecifically, in pipelines, STDOUT of a script (here it’s run_samtools) is directed into STDIN of another command (here wc, or word count)\n\n\n\n\n\n\n\n\ngraph LR\n  E[run_samtools.sh] --&gt; B(STDOUT)\n  B --&gt; F{\"|\"}\n  E --&gt; C(STDERR)\n  F --&gt; D(\"STDIN (wc)\")\n  D --&gt; G[wc]\n\n\n\n\n\n\n\n\nFigure 9.2: Piping a script run_samtools.sh into another command (wc)\n\n\n\nWe will mostly use STDOUT in our bash scripts, but STDERR can be really helpful in debugging what’s going wrong.\n\n\n\n\n\n\nWhy this is important on the Cluster\n\n\n\nWe’ll use pipes and pipelines not only in starting a bunch of jobs using batch scripting on our home computer, but also when we are processing files within a job.\n\n\n\n9.4.1 For more info about pipes and pipelines\nhttps://swcarpentry.github.io/shell-novice/04-pipefilter/index.html https://datascienceatthecommandline.com/2e/chapter-2-getting-started.html?q=stdin#combining-command-line-tools",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Miscellaneous</span>"
    ]
  },
  {
    "objectID": "miscellaneous.html#basename-can-be-very-handy-when-on-workers",
    "href": "miscellaneous.html#basename-can-be-very-handy-when-on-workers",
    "title": "9  Miscellaneous",
    "section": "9.5 basename can be very handy when on workers",
    "text": "9.5 basename can be very handy when on workers\nIf we are processing a bunch of files on a worker, we need a way to get the bare filename from a path. We will take advantage of this when we run process multiple files on the worker.\nFor example:\nbasename /mnt/project/worker_scripts/srun-script.sh\nThis will return:\nsrun-script.sh\nWhich can be really handy when we name our outputs. This command is so handy it is used in WDL.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Miscellaneous</span>"
    ]
  },
  {
    "objectID": "03_batch.html#learning-objectives",
    "href": "03_batch.html#learning-objectives",
    "title": "4  Batch Processing and Submitting Jobs",
    "section": "",
    "text": "Execute a script to run over a list of files on one system\nBatch Process files to run on the HPC cluster",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Batch Processing and Submitting Jobs</span>"
    ]
  },
  {
    "objectID": "03_batch.html#using-for-loops-to-cycle-through-files",
    "href": "03_batch.html#using-for-loops-to-cycle-through-files",
    "title": "4  Batch Processing and Submitting Jobs",
    "section": "4.2 Using for loops to cycle through files",
    "text": "4.2 Using for loops to cycle through files\nA very common pattern is cycling through multiple files in a folder and applying the same script or command to them.\nThere is a simple method for batch processing a bunch of files: a for loop.\n#!/bin/bash\n1for file in *.qmd\n2do\n3  wc $file\n4done\n\n1\n\nStart for loop and cycle through all .qmd files\n\n2\n\nStart of instructions\n\n3\n\nCount the words in each .qmd file using wc\n\n4\n\nEnd of instructions\n\n\nIf we run this in our repository, we get something similar to this:\n      22      92     634 01_assignment.qmd\n     462    2417   15396 01_basics.qmd\n       5       8      49 02_assignment.qmd\n     303    1577    9509 02_scripting.qmd\n     303    1667   10233 03_batch.qmd\n     198    1368    9027 04_containers_workflows.qmd\n     205    1151    7858 configuring.qmd\n      53     363    2516 index.qmd\n       9      25     157 intro.qmd\n     214    1314    8001 miscellaneous.qmd\nThe *.qmd (the wildcard operator, also known as a a pattern-matching mechanism used primarily in shells (like Bash) and various programming languages to match filenames and paths. It is also known as “filename expansion” or “wildcard matching.”glob) can be used in various ways. For example, if our files are in a folder called raw_data/, we could specify:\nfor file in raw_data/*.fq\ndo\n  bwa mem \ndone\n\n\n\n\n\n\nA common pattern: a folder with only one type of file in them\n\n\n\nOne thing that makes it easier to process a bunch of files is to have the data be in the same folder, with nothing else in them.\nI might create a fastq/ file where I store my data, so I can pass the wildcard fastq/* to process the file.\n#!/bin/bash\nmodule load BWA\nmodule load SAMtools\nFASTA_LOCATION=\"\"\nOUTPUT_FOLDER=\"/hpc/temp/my_lab/project_x/bam_files/\"\nfor file in fastq/*\ndo\n  bwa mem ${FASTA_LOCATION} file &gt; ${OUTPUT_FOLDER}/${file}.bam \n  samtools sort ${OUTPUT_FOLDER}/${file}.bam |&gt; ${OUTPUT_FOLDER}/${file}.sorted.bam\ndone\nmodule purge\n\n\n\n4.2.1 For more info on globs\nSee page 12 in Bite Size Bash.\n\n\n\n\n\n\nSelecting files with complicated patterns: Regular Expressions\n\n\n\n\n\n\n\n\n4.2.2 Using file manifests\nOne approach that I use a lot is using file manifests to process multiple sets of files. Each line of the file manifest will contain all of the related files I need to process.\nFor example, if I am aligning paired-end reads, then I can have a tab-separated column for the first strand, and a column for the second strand.\nread           read2\nsample1_1.fq   sample1_2.fq\nsample2_1.fq   sample2_2.fq\nThe one trick with using file manifests in bash is that we need to change the what’s called the internal field separator (IFS), which specifies how to split up a string with a for loop. By default, bash uses an IFS of ” ” (a space), which means that the for loop will cycle through words (strings separated by spaces), instead of lines.\nWe can change this behavior by setting the IFS at the beginning of our script:\nfor file in (cat manifest.txt)\n1IFS=\"\"\ndo\n  bwa mem ${FASTA_LOCATION} file &gt; ${OUTPUT_FOLDER}/${file}.bam \n  samtools sort ${OUTPUT_FOLDER}/${file}.bam |&gt; ${OUTPUT_FOLDER}/${file}.sorted.bam\ndone\n\n2unset IFS\n\n1\n\nChange IFS to be \"\" (no space), to process a file line by line.\n\n2\n\nReset IFS to original behavior.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Batch Processing and Submitting Jobs</span>"
    ]
  },
  {
    "objectID": "04_containers_workflows.html#sec-containers",
    "href": "04_containers_workflows.html#sec-containers",
    "title": "6  Containers and Workflows",
    "section": "6.4 Working with containers",
    "text": "6.4 Working with containers\nI think the hardest thing about working with containers is wrapping your head around the indirectness of them. You are running software with its own internal filesystem and the challenges are getting the container to read files in folders/paths outside of its own filesytem, as well as outputting files into those outside folders.\n\n6.4.1 Testing code in a container\nIn this section, we talk about testing scripts in a container using apptainer. We use apptainer (formerly Singularity) in order to run Docker containers on a shared HPC system. This is because Docker itself requires root-level privileges, which is not secure on shared systems.\nIn order to do our testing, we’ll first pull the Docker container, map our bind point (so our container can access files outside of its file system), and then run scripts in the container.\nEven if you aren’t going to frequently use Apptainer in your work, I recommend trying an interactive shell in a container at least once or twice to learn about the container filesystem and conceptually understand how you connect it to the external filesystem.\n\n\n6.4.2 Pulling a Docker Container\nLet’s pull a docker container from the Docker registry. Note we have to specify docker:// when we pull the container, because Apptainer has its own internal format called SIF.\nmodule load Apptainer/1.1.6\napptainer pull docker://biocontainers/samtools:v1.9-4-deb_cv1\n\n\n6.4.3 Opening a Shell in a Container with apptainer shell\nWhen you’re getting started, opening a shell using Apptainer can help you test out things like filepaths and how they’re accessed in the container. It’s hard to get an intuition for how file I/O works with containers until you can see the limited view from the container.\nBy default, apptainers can see your current directory and navigate to the files in it.\nYou can open an Apptainer shell in a container using apptainer shell. Remember to use docker:// before the container name. For example:\nmodule load Apptainer/1.1.6\napptainer shell docker://biocontainers/samtools:v1.9-4-deb_cv1\nThis will load the apptainer module, and then open a Bash shell in the container using apptainer shell. Once you’re in the container, you can test code, especially seeing whether your files can be seen by the container (see Section 5.1.6). 90% of the issues with using Docker containers has to do with bind paths, so we’ll talk about that next.\nOnce you’re in the shell, you can take a look at where samtools is installed:\nwhich samtools\nNote that the container filesystem is isolated, and we need to explicitly build connections to it (called bind paths) to get files in and out. We’ll talk more about this in the next section.\nOnce we’re done testing scripts in our containers, we can exit the shell and get back into the node.\nexit\n\n\n\n\n\n\nOpening a Shell in a Docker Container with Docker\n\n\n\nFor the most part, due to security reasons, we don’t use docker on HPC systems. In short, the docker group essentially has root-level access to the machine, and it’s not a good for security on a shared resource like an HPC.\nHowever, if you have admin level access (for example, on your own laptop), you can open up an interactive shell with docker run -it:\ndocker run -it biocontainers/samtools:v1.9-4-deb_cv1 /bin/bash\nThis will open a bash shell much like apptainer shell. Note that volumes (the docker equivalent of bind paths) are specified differently in Docker compared to Apptainer.\n\n\n\n\n\n\n\n\nWDL makes this way easier\n\n\n\nA major point of failure with Apptainer scripting is when our scripts aren’t using the right bind paths. It becomes even more complicated when you are running multiple steps.\nThis is one reason we recommend writing WDL Workflows and a A system that works with the cluster manager to orchestrate processing data through a workflowworkflow manager (such as A workflow runner. Currently works with WDL files.Cromwell or Sprocket) to run your workflows. You don’t have to worry that your bind points are setup correctly, because they are handled by the workflow manager.\n\n\n\n\n6.4.4 Testing in the Apptainer Shell\nOk, now we have a bind point, so now we can test our script in the shell. For example, we can see if we are invoking samtools in the correct way and that our bind points work.\nsamtools view -c /mydata/my_bam_file.bam &gt; /mydata/bam_counts.txt\nAgain, trying out scripts in the container is the best way to understand what the container can and can’t see.\n\n\n6.4.5 Exiting the container when you’re done\nYou can exit, like any shell you open. You should be out of the container. Confirm by using hostname to make sure you’re out of the container.\n\n\n6.4.6 Testing outside of the container\nLet’s take everything that we learned and put it in a script that we can run on the HPC:\n# Script to samtools view -c an input file:\n# Usage: ./run_sam.sh &lt;my_bam_file.bam&gt;\n# Outputs a count file: my_bam_file.bam.counts.txt\n#!/bin/bash\nmodule load Apptainer/1.1.6\napptainer run --bind /fh/fast/mydata:/mydata docker://biocontainers/samtools:v1.9-4-deb_cv1 samtools view -c /mydata/$1 &gt; /mydata/$1.counts.txt\n#apptainer cache clean\nmodule purge\nWe can use this script by the following command:\n./run_sam.sh chr1.bam \nAnd it will output a file called chr1.bam.counts.txt.\n\n\n\n\n\n\nApptainer Cache\n\n\n\nThe apptainer cache is where your docker images live. They are translated to the native apptainer .sif format.\nYou can see what’s in your cache by using\napptainer cache list\nBy default the cache lives at ~/.apptainer/cache.\nIf you need to clear out the cache, you can run\napptainer cache clean\nto clear out the cache.\nThere are a number of environment variables (Section 6.5) that can be set, including login tokens for pulling from a private registry. More information is here.\n\n\n\n\n6.4.7 More Info\n\nCarpentries Section on Apptainer Paths - this is an excellent resource if you want to dive deeper into undestanding container filesystems and bind points.\nApptainer Documentation on Bind Paths. There are a lot of good examples here on how to set up bind paths.\nMore about bind paths and other options.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Containers and Workflows</span>"
    ]
  },
  {
    "objectID": "02_scripting.html#arguments-in-r-scripts",
    "href": "02_scripting.html#arguments-in-r-scripts",
    "title": "2  Introduction to Scripting",
    "section": "2.11 Arguments in R Scripts",
    "text": "2.11 Arguments in R Scripts\nTo make our R Script more useful, it must also accept arguments. Let’s explore how to achieve this.\nAs we learned in (Section 1.14), We can use Rscript to execute our command non-interactively. Rscript, which is the command line executable, will run our Rs cript on the command line.\nNote that we have a named argument called input_file and it is done differently than in Bash - how do we use this in our R Script?\n\n2.11.1 Using Named Arguments in an R script\nWe can pass arguments from our bash script to our R script by using commandArgs() - this will populate a list of named arguments (such as CSVFILE) that are passed into the R Script. We assign the output of commandArgs() into the args object.\nWe refer to our CSVFILE argument as args$CSVFILE in our script.\n\n\n\nscripts/week2/process_data.R\n\nlibrary(tidyverse)\n\nargs &lt;- commandArgs()\n\n# Use arg$CSVFILE in read.csv\n1csv_file &lt;- read.csv(file=args$CSVFILE)\n\n# Do some work with csv_file\ncsv_filtered &lt;- csv_file |&gt; janitor::clean_names()\n\n# Write output\n2write.csv(csv_filtered, file = paste0(args$CSVFILE, \"_filtered.csv\"))\n\n\n\n1\n\nUse args$CSVFILE as an input for read.csv()\n\n2\n\nUse args$CSVFILE to specify the output name.\n\n\n\n\n\n\n2.11.2 Running our R Script\nNow that we’ve set it up, we can run the R script from the command line as follows:\n\nRscript process_data.R CSVFILE=\"LUSC_clinical.csv\"\n\n\n\n2.11.3 Integrating into a Bash Script\nSay you have an R Script you need to run on the command line in combination with other unix commands. In our bash script, we can do the following:\n\n\n\nscripting-basics/wrap_r_script.sh\n\n#!/bin/bash\nmodule load fhR\nRscript process_data.R CSVFILE=\"${1}\"\nmodule purge\n\n\nThen we can run\n\nbash wrap_r_script.sh my_csvfile.csv \n\nIn our bash script, my_bash_script.sh, we’re using positional argument (for simplicity) to specify our csvfile, and then passing the positional argument to named ones (CSVFILE) for my_r_script.R.\n\n\n2.11.4 Summary\n\nR scripts will accept named arguments when called by Rscript:\n\nRscript process_csv.R CSVFILE=\"my_csvfile.csv\"",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Scripting</span>"
    ]
  },
  {
    "objectID": "02_scripting.html#arguments-in-python-scripts",
    "href": "02_scripting.html#arguments-in-python-scripts",
    "title": "2  Introduction to Scripting",
    "section": "2.12 Arguments in Python Scripts",
    "text": "2.12 Arguments in Python Scripts\nSimilarly, we will need Python scripts to accept arguments as well.\n\n2.12.1 Using sys.argv\nThe sys module (built into Python) will let us access arguments by position:\n\n\n\nprocess_file.py\n\n1import sys\nimport pandas as pd\n2file = sys.argv[1]\ndf = pd.read_csv(file)\n\n\n\n1\n\nImport sys module\n\n2\n\nUse sys.argv to access the first argument sys.argv[1]\n\n\n\n\n\n\n2.12.2 Running our Python script on the command line\nNow we have set up our python script, we can run it on the command line on rhino:\n\nmodule load fhPython\npython3 process_file.py lusc_file.csv\n\nAnd we could wrap it up in a bash script:\n\n\n\nwrap_py_script.sh\n\n#!/bin/bash\nmodule load fhPython\npython3 process_file.py ${1}\nmodule purge",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Scripting</span>"
    ]
  },
  {
    "objectID": "configuring.html#when-does-the-bashrc-file-get-loaded",
    "href": "configuring.html#when-does-the-bashrc-file-get-loaded",
    "title": "8  Appendix: Configuring your Shell",
    "section": "8.2 When does the bashrc file get loaded?",
    "text": "8.2 When does the bashrc file get loaded?\nYou can force reloading of the",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Appendix: Configuring your Shell</span>"
    ]
  },
  {
    "objectID": "hpc-basics.html",
    "href": "hpc-basics.html",
    "title": "3  Reading: HPC Basics",
    "section": "",
    "text": "3.1 Learning Objectives\nWe all need to start somewhere when we work with High Performance Computing (HPC).\nThis chapter is a review of how HPC works and the basic concepts. If you haven’t used HPC before, no worries! This chapter will get you up to speed.\nAfter reading this chapter, you should be able to:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reading: HPC Basics</span>"
    ]
  },
  {
    "objectID": "hpc-basics.html#learning-objectives",
    "href": "hpc-basics.html#learning-objectives",
    "title": "3  Reading: HPC Basics",
    "section": "",
    "text": "Define key players in both local computing and HPC\nArticulate key differences between local computing and HPC\nDescribe the sequence of events in launching jobs in the HPC cluster\nDifferentiate local storage from shared storage and articulate the advantages of shared storage.\nDescribe the differences between a single job and a batch job.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reading: HPC Basics</span>"
    ]
  },
  {
    "objectID": "hpc-basics.html#important-terminology",
    "href": "hpc-basics.html#important-terminology",
    "title": "3  Reading: HPC Basics",
    "section": "3.2 Important Terminology",
    "text": "3.2 Important Terminology\nLet’s establish the terminology we need to talk about HPC computing.\n\nHigh Performance Computing - A type of computing that uses higher spec machines, or multiple machines that are joined together in a cluster. These machines can either be on-premise (also called on-prem), or in the cloud (such as Amazon EC machines, or Azure Batch).\nCluster - a group of machines networked such that users can use one or more machines at once.\nAllocation - a temporary set of one or more computers requested from a cluster.\nToolchain - a piece of software and its dependencies needed to build a tool on a computer. For example, cromwell (a workflow runner), and java.\nSoftware Environment - everything needed to run a piece of software on a brand new computer. For example, this would include installing tidyverse, but also all of its dependencies (R) as well. A toolchain is similar, but might not contain all system dependencies.\nExecutable - software that is available on the HPC.\nShared Filesystem - Part of the HPC that stores our files and other objects. We’ll see that these other objects include applets, databases, and other object types.\nSLURM - The workload manager of the HPC. Commands in SLURM such as srun, (see Section 3.4) kick off the processes on executing jobs on the worker nodes.\nInteractive Analysis - Any analysis that requires interactive input from the user. Using RStudio and JupyterLab are two examples of interactive analysis. As opposed to non-interactive analysis, which is done via scripts.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reading: HPC Basics</span>"
    ]
  },
  {
    "objectID": "hpc-basics.html#understanding-the-key-players",
    "href": "hpc-basics.html#understanding-the-key-players",
    "title": "3  Reading: HPC Basics",
    "section": "3.3 Understanding the key players",
    "text": "3.3 Understanding the key players\nIn order to understand what’s going on with HPC, we will have to change our mental model of computing.\nLet’s contrast the key players in local computing with the key players in HPC.\n\n3.3.1 Key Players in Local Computing\n\n\n\n\n\n\nFigure 3.1: Local Computing\n\n\n\n\nOur Machine\n\nWhen we run an analysis or process files on our computer, we are in control of all aspects of our computer. We are able to install a software environment, such as R or Python, and then execute scripts/notebooks that reside on our computer on data that’s on our computer.\nOur main point of access to either the HPC cluster is going to be our computer.\n\n\n3.3.2 Key Players in HPC\nLet’s contrast our view of local computing with the key players in the HPC cluster (Figure 3.2).\n\n\n\n\n\n\nFigure 3.2: Key Players in HPC\n\n\n\n\nOur Machine - We interact with the HPC via the terminal installed on our machine. When we utilize HPC resources, we request them from our own computer using commands from the dx toolkit.\nHead Node - The “boss” of the cluster. It keeps tract of which worker nodes is doing what, and which nodes are available for allocations. Our request gets sent to the HPC cluster, and given availability, it will grant access to temporary worker.\nWorker Node - A temporary machine that comes from a pool of available machines in the cluster. We’ll see that it starts out as a blank slate, and we need to establish a software environment to run things on a worker.\nShared Filesystem A distributed filesystem that can be seen by all of the nodes in the cluster. Our scripts and data live here.\n\n\n\n3.3.3 Further Reading\n\nWorking on a remote HPC system is also a good overview of the different parts of HPC.\n\n\n\n\n\n\n\nFor Fred Hutch Users\n\n\n\nThe gizmo cluster at Fred Hutch actually has 3 head nodes called rhino (rhino01, rhino02, rhino03) that are high spec machines (70+ cores, lots of memory). You can run jobs on these nodes, but be aware that others may be running jobs here as well.\nThe worker nodes on the gizmo cluster all have names like gizmoj6, depending on their architecture. You can request certain kinds of nodes in an allocation in several ways:\n\nWhen you use grabnode (Section 7.1.1)\nIn your request when you run srun or sbatch\nIn your WDL or Nextflow Workflow.\n\n\n\n\n\n\n\n\n\nFH Users: Launching Jobs on gizmo\n\n\n\nIn a SLURM cluster like gizmo, SLURM (Simple Linux Utility for Resource Management) is software that runs on the head node that manages jobs on individual nodes.\nThe two main mechanisms for running SLURM jobs are srun (used for single jobs) and sbatch (used for multiple related jobs, such as aligning a set of FASTA files).\nWhen scaling to a larger number of files, we do recommend that you use a workflow manager such as Cromwell, or PROOF, or Nextflow for batching files.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reading: HPC Basics</span>"
    ]
  },
  {
    "objectID": "hpc-basics.html#sec-srun",
    "href": "hpc-basics.html#sec-srun",
    "title": "3  Reading: HPC Basics",
    "section": "3.4 Sequence of Events of Running a Job",
    "text": "3.4 Sequence of Events of Running a Job\nLet’s run through the order of operations of running a job on the HPC cluster. Let’s focus on running an aligner (BWA-MEM) on a FASTQ file. Our output will be a .BAM (aligned reads) file.\nLet’s go over the order of operations needed to execute our job on the HPC cluster (Figure 3.3).\n\n\n\n\n\n\nFigure 3.3: Order of Operations\n\n\n\nA. Start a job using srun to send a request to the cluster. In order to start a job, we will need two things: software (samtools), and a file to process from the shared filesystem (not shown). When we use srun, a request is sent to the cluster.\nB. Head node requests for a worker from available workers; worker made available on cluster. In this step, the head node looks for a set of workers that can meet our needs. Then the computations run on the worker; output files are generated.** Once our app is ready and our file is transferred, we can run the computation on the worker.\nC. Output files transferred back to project storage. Any files that we generate during our computation (53525342.bam) must be transferred back into the shared filesystem.\nWhen you are working with an HPC cluster, especially with batch jobs, keep in mind this order of execution. Being familiar with how the key players interact on the cluster is key to running efficient jobs.\n\n3.4.1 Key Differences with local computing\nAs you might have surmised, running a job on the HPC cluster is very different from computing on your local computer. Here are a few key differences:\n\nWe don’t own the worker machine, we only have temporary access to it. A lot of the complications of running HPC computations comes from this.\nWe have to be explicit about what kind of machine we want. We’ll talk much more about this in terms of machine types and classifieds.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reading: HPC Basics</span>"
    ]
  },
  {
    "objectID": "hpc-basics.html#the-shared-filesystem",
    "href": "hpc-basics.html#the-shared-filesystem",
    "title": "3  Reading: HPC Basics",
    "section": "3.5 The Shared Filesystem",
    "text": "3.5 The Shared Filesystem\nClusters often have a shared filesystem to make things easier. These filesystems can be accessed by all the nodes in a cluster and are designed for fast file transfers and reading. One example of a filesystem is Lustre.\nThink about that: it’s like having an SSD attached to all of the nodes. But how does the shared storage work?\nThe filesystem is distributed such that each set of nodes has a relatively fast access to the files on the system. The data itself is sharded, or broken up, and distributed among the storage servers that provide access to the files.\n\n\n\n\n\n\nFor FH Users\n\n\n\nThere are three main filesystems you will probably use:\n\n/home/ - usually where your scripts will live\n/fh/fast/ - Where data lives. You will usually transfer data files over to /fh/temp/ and when you generate results, transfer them from /fh/temp/ back to /fh/fast/\n/fh/temp/ - A temporary filesystem. Don’t store files here long term - mostly use this as a faster system to do computations on.\n\n\n\n\n3.5.1 Further Reading\n\nTransferring Files is a nice overview of the ways to transfer files to and from a remote system.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reading: HPC Basics</span>"
    ]
  },
  {
    "objectID": "hpc-basics.html#requesting-machines",
    "href": "hpc-basics.html#requesting-machines",
    "title": "3  Reading: HPC Basics",
    "section": "3.6 Requesting Machines",
    "text": "3.6 Requesting Machines\nHow do you request a set of machines on the HPC? There are multiple ways to do so:\n\nOpen an Interactive shell on a Node\nAs part of a job using srun or sbatch\nUsing Cromwell or Nextflow\n\nIn general, we recommend\n\n3.6.1 Scattering: Distribute the Work\n\n\n\n\n\n\nFigure 3.4: The scattering process\n\n\n\nSo far, everything we’ve seen so far can be run on a single computer. In the cluster, we have access to higher spec’ed machines, but using the cluster in this way doesn’t take advantage of the efficiency of distributed computing, or dividing the work up among multiple worker nodes.\nWe can see an example of this in Figure 3.4. In distributed computing, we break our job up into smaller parts. One of the easiest way to do this is to split up a list of files (file1.bam, file2.bam, file3.bam) that we need to process, process each file separately on a different node, and then bring the results back together. Each node is only doing part of the work, but because we have multiple nodes, it is getting done 3 times faster.\nYou can orchestrate this process yourself with tools such as sbatch, but it is usually much easier to utilize workflow runners such as Cromwell/PROOF (for .wdl files) or Nextflow (for .nf files), because they automatically handle saving the results of intermediate steps.\nTrust me, it is a little more of a learning curve to learn Cromwell or Nextflow, but once you know more about it, the automatic file management and node management makes it much easier in the long run.\n\n\n\n\n\n\nNodes versus CPUs\n\n\n\nOne thing that confused me was understanding the difference between requesting a system with multiple cores versus requesting multiple nodes.\nCores roughly correspond to processors, so a 24-core allocation is a single node that has 24 CPUs.\nNodes correspond to machines - so a 24 node allocation is 24 machines.\nThe reason why this is important is that you use them differently - we use scatter to utilize a 24 node allocation, whereas we can use multicore packages such as {parallel} and mcapply() to utilize a multi-core system.\nIn general, scatter over multiple nodes is handled by sbatch or your workflow runner.\n\n\n\n\n\n\n\n\nFor FH Users: Running Workflows\n\n\n\nAt Fred Hutch, we have two main ways to run workflows on gizmo: Cromwell and NextFlow. Cromwell users have a nifty GUI to run their workflows called PROOF.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reading: HPC Basics</span>"
    ]
  },
  {
    "objectID": "hpc-basics.html#scattering-distribute-the-work",
    "href": "hpc-basics.html#scattering-distribute-the-work",
    "title": "3  Reading: HPC Basics",
    "section": "3.7 Scattering: Distribute the Work",
    "text": "3.7 Scattering: Distribute the Work\n\n\n\n\n\n\nFigure 3.4: The scattering process\n\n\n\nSo far, everything we’ve seen so far can be run on a single computer. In the cluster, we have access to higher spec’ed machines, but using the cluster in this way doesn’t take advantage of the efficiency of distributed computing, or dividing the work up among multiple worker nodes.\nWe can see an example of this in Figure 3.4. In distributed computing, we break our job up into smaller parts. One of the easiest way to do this is to split up a list of files (file1.bam, file2.bam, file3.bam) that we need to process, process each file separately on a different node, and then bring the results back together. Each node is only doing part of the work, but because we have multiple nodes, it is getting done 3 times faster.\nYou can orchestrate this process yourself with tools such as sbatch, but it is usually much easier to utilize workflow runners such as Cromwell/PROOF (for .wdl files) or Nextflow (for .nf files), because they automatically handle saving the results of intermediate steps.\nTrust me, it is a little more of a learning curve to learn Cromwell or Nextflow, but once you know more about it, the automatic file management and node management makes it much easier in the long run.\n\n\n\n\n\n\nNodes versus CPUs\n\n\n\nOne thing that confused me was understanding the difference between requesting a system with multiple cores versus requesting multiple nodes.\nCores roughly correspond to processors, so a 24-core allocation is a single node that has 24 CPUs.\nNodes correspond to machines - so a 24 node allocation is 24 machines.\nThe reason why this is important is that you use them differently - we use scatter to utilize a 24 node allocation, whereas we can use multicore packages such as {parallel} and mcapply() to utilize a multi-core system.\nIn general, scatter over multiple nodes is handled by sbatch or your workflow runner.\n\n\n\n\n\n\n\n\nFor FH Users: Running Workflows\n\n\n\nAt Fred Hutch, we have two main ways to run workflows on gizmo: Cromwell and NextFlow. Cromwell users have a nifty GUI to run their workflows called PROOF.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reading: HPC Basics</span>"
    ]
  },
  {
    "objectID": "hpc-basics.html#whats-next",
    "href": "hpc-basics.html#whats-next",
    "title": "3  Reading: HPC Basics",
    "section": "3.7 What’s Next?",
    "text": "3.7 What’s Next?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reading: HPC Basics</span>"
    ]
  },
  {
    "objectID": "01_basics.html#sec-rpy",
    "href": "01_basics.html#sec-rpy",
    "title": "1  Navigating the Bash Filesystem",
    "section": "1.14 Running an R or Python Script on the command line",
    "text": "1.14 Running an R or Python Script on the command line\n\n1.14.1 Loading the fhR or fhPython modules\nBefore we can run our software, we’ll need to load up either R or\nWe’ll talk more about software modules next week (Section 2.5).\n\n\n1.14.2 R Users\nYou might not be aware that there are multiple ways to run R:\n\nas an interactive console, which is what we usually use in an IDE such as RStudio\non the command line using the Rscript command.\n\nRscript my_r_script.R\n\n\n1.14.3 Python Users\nPython users are much more aware that you can run Python scripts on the command line:\npython3 my_python_script.py\nWithin a shell script, you can also use a shebang (Section 2.4) to make your script executable by providing the location of python3:\n#!/bin/python3\npython3 my_python_script.py",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Navigating the Bash Filesystem</span>"
    ]
  },
  {
    "objectID": "index.html#reference-texts",
    "href": "index.html#reference-texts",
    "title": "Bash for Bioinformatics",
    "section": "Reference Texts",
    "text": "Reference Texts\n\nWe will be using Julia Evan’s Bite Size Bash as our reference text. Julia’s explanations are incredibly clear and it will be a valuable reference even beyond this course. You will receive the PDF as part of class, and I will refer to it throughout the course.\nIf you want to know the true power of the command line, I recommend Data Science at the Command Line. This book showcases how much you can get done with just command line.",
    "crumbs": [
      "Bash for Bio"
    ]
  },
  {
    "objectID": "miscellaneous.html#sec-xargs",
    "href": "miscellaneous.html#sec-xargs",
    "title": "9  Miscellaneous",
    "section": "9.6 Advanced Batch Processing: Iterating using xargs",
    "text": "9.6 Advanced Batch Processing: Iterating using xargs\nA really common pattern is taking a delimited list of files and doing something with them. We can do some useful things such as seeing the first few lines of a set of files, or doing some sort of processing with the set of jobs.\n\n\n\n\n\n\nDon’t xargs for HPC jobs\n\n\n\nYou might be tempted to use xargs with srun to work on a bunch of files. It’s worth trying once so you can see the mechanics of how jobs are processed.\nIn general, I don’t recommend it in practice because if you spawn 1000 jobs using xargs, there’s no real mechanism to terminate that 1000 jobs, except one by one. With sbatch, all your jobs in batch mode run as subjobs, which means you can terminate the parent job to terminate all of the subjobs.\nAgain, this is a good reason to use a workflow runner in your day to day work. You don’t have to worry about jobs and subjobs. It takes a little setup, but it will make your life easier in general.\n\n\nLet’s start out with a list of files:\nsource ~/.bashrc #| hide_line\nls data/*.sh\ndata/batch-on-worker.sh\nNow we have a list of files, let’s look at the first few lines of each of them, and print a separator --- for each.\n#| filename: scripting-basics/xargs_example.sh\nsource ~/.bashrc #| hide_line\nls data/*.sh | xargs -I% sh -c 'head %; echo \"\\n---\\n\"'\n#!/bash/bin\n\ncmd_to_run=\"ls *.vcf.gz | xargs -I% sh -c \"bcftools stats % &gt; %.stats.txt\"\n\ndx run swiss-army-knife \\\n  -iin=\"data/chr1.vcf.gz\" \\\n  -iin=\"data/chr2.vcf.gz\" \\\n  -iin=\"data/chr3.vcf.gz\" \\\n  -icmd=${cmd_to_run}\n---\ndx find data --name \"*.bam\" --brief\n---\nLet’s take this apart piece by piece.\nxargs takes an -I argument that specifies a placeholder. In our case, we are using % as our placeholder in this statement.\nWe’re passing on each filename from ls into the following code:\nsh -c 'head %; echo \"---\\n\"'\nThe sh -c opens a subshell so that we can execute our command for each of the files in our list. We’re using sh -c to run:\n'head %; echo \"---\\n\"'\nSo for our first file, 01-scripting-basics.qmd, we are substituting that for % in our command:\n'head hpc-basics.qmd; echo \"---\\n\"'\nFor our second file, hpc-basics.qmd, we would substitute that for the %:\n'head hpc-basics.qmd; echo \"---\\n\"'\nUntil we cycle through all of the files in our list.\n\n9.6.1 The Basic xargs pattern\n\n\n\n\n\n\n\n\ngraph LR\n  A[\"ls *.bam\"] --&gt; B{\"|\"} \n  B --&gt; C[\"xargs -I% sh -c\"] \n  C --&gt; D[\"command_to_run %\"]\n\n\n\n\n\n\n\n\nFigure 9.3: Basics of using xargs to iterate on a list of files\n\n\n\nAs you cycle through lists of files, keep in mind this basic pattern (Figure 9.3):\nls &lt;wildcard&gt; | xargs -I% sh -c \"&lt;command to run&gt; %\"\n\n\n\n\n\n\nTest Yourself\n\n\n\nHow would we modify the below code to do the following?\n\nList only .json files in our data/ folder using ls\nUse tail instead of head\n\nls *.txt | xargs -I% sh -c \"head %; echo '---\\n'\"\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nls data/*.json | xargs -I% sh -c \"tail %; echo '---\\n'\"\n\n\n\n\n\n\n\n\n\nWhy this is important on HPC\n\n\n\nWe can use xargs to execute small batch jobs on a small number of files. This especially becomes powerful on the cluster when we use ls to list files in our HPC project.\nNote that as we graduate to workflow tools like WDL/Nextflow, there are other mechanisms for running jobs on multiple files (such as WDL/Cromwell) that we should move to.\nTrust me; you don’t want to have to handle iterating through a huge directory and handling when routines give an error, or your jobs get interrupted. Rerunning and resuming failed jobs are what workflow runner tools excel at.\n\n\n\n\n9.6.2 For more information\nhttps://www.baeldung.com/linux/xargs-multiple-arguments",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Miscellaneous</span>"
    ]
  },
  {
    "objectID": "04_containers_workflows.html#running-workflows",
    "href": "04_containers_workflows.html#running-workflows",
    "title": "6  Workflows",
    "section": "6.2 Running Workflows",
    "text": "6.2 Running Workflows\nSo far, we’ve been batch processing using job arrays in SLURM. However, when you graduate to processing in multiple steps, you should consider using a workflow manager for processing files.\nA workflow manager will take a workflow, which can consist of multiple steps and allow you to batch process files.\nA good workflow manager will allow you to:\n\nRestart failed subjobs in the workflow\nAllow you to customize where intermediate and final outputs go\nSwap and customize modules in your workflow\nAdapt to different computing architectures (HPC/cloud/etc)\n\nMany bioinformaticists have used workflow managers to process and manage hundreds or thousands of files at a time. They are well worth learning.\nHere is an overview of some of the common bioinformatics workflow managers. We will be using cromwell, which runs WDL files.\n\n\n\nManager Software\nWorkflow Formats\nNotes\n\n\n\n\nCromwell\nWDL/CWL\nMade for HPC Jobs\n\n\nSprocket\nWDL\nMade for HPC Jobs\n\n\nMiniWDL\nWDL\nUsed for local testing of workflows\n\n\nDNANexus\nWDL/CWL\nUsed for systems such as AllOfUs\n\n\nNextflow\n.nf files\nOwned by seqera\n\n\nSnakemake\nmake files\n\n\n\n\n\n6.2.1 Grabbing a WDL workflow from GETWILDS\ngit clone https://github.com/getwilds/ww-star-deseq2/\n\n\n6.2.2 Executing a WDL workflow\nSay someone has given us a WDL file - how do we set it up to run on our own data?\nWe’ll use cromwell to run our WDL workflow.\nmodule load cromwell/87\nWe will get the repsonse:\nTo execute cromwell, run: java -jar $EBROOTCROMWELL/cromwell.jar\n\nTo execute womtool, run: java -jar $EBROOTCROMWELL/womtool.jar\nWe’ll investigate using cromwell run as an initial way to interact with cromwell.\njava -jar $EBROOTCROMWELL/cromwell.jar run ww-star2-deseq2.wdl \\\n   --inputs input_file.json\n\n\n\n\n\n\ncromwell serve\n\n\n\nThe other way to run Cromwell is to run it in server mode. You can start the server by using:\njava -jar $EBROOTCROMWELL/cromwell.jar serve\nWhile you can run the server on the command line, I don’t recommend it. I would check out the PROOF application for a GUI based way to execute workflows. It handles cromwell server management automatically.\n\n\n\n\n6.2.3 Input Files as JSON\nIf you want to work with the JSON format (?sec-json), there is a trick to generating a template .json file for a workflow. There is an additional utility called womtools that will generate a template file from a workflow file.\njava -jar $EBROOTCROMWELL/womtool.jar inputs ww-star2-deseq2.wdl &gt; ww-star2-deseq2-inputs.json\nThis will generate a file called ww-star2-deseq2-inputs.json that contains all of the inputs:\n{\n  \"star_deseq2.samples\": \"Array[WomCompositeType {\\n name -&gt; String\\nr1 -&gt; File\\nr2 -&gt; File\\ncondition -&gt; String \\n}]\",\n  \"star_deseq2.rnaseqc_cov.cpu_cores\": \"Int (optional, default = 2)\",\n  ...\n}\nThis can be a good head start to making your .json files.\n\n\n6.2.4 Working with file manifests in WDL\nLast week, we worked with a file manifest to process a list of files.\nThere is an example workflow that shows how to work with a file manifest. This can be helpful for those who aren’t necessarily good at working with JSON.\nThis workflow has a single input, which is the location of the file manifest. It will then cycle through the manifest, line by line.\nThis is what the file manifest contains. Notice there are three named columns for this file: sampleName, bamLocation, and bedLocation.\n\n\n\n\n\n\n\n\nsampleName\nbamLocation\nbedLocation\n\n\n\n\nsmallTestData\n/fh/fast/paguirigan_a/pub/ReferenceDataSets/workflow_testing_data/WDL/unpaired-panel-consensus-variants-human/smallTestData.unmapped.bam\n/fh/fast/paguirigan_a/pub/ReferenceDataSets/reagent_specific_data/sequencing_panel_bed/TruSight-Myeloid-Illumina/trusight-myeloid-amplicon-v4-track_interval-liftOver-hg38.bed\n\n\nsmallTestData-reference\n/fh/fast/paguirigan_a/pub/ReferenceDataSets/workflow_testing_data/WDL/paired-panel-consensus-variants-human/smallTestData-reference.unmapped.bam\n/fh/fast/paguirigan_a/pub/ReferenceDataSets/reagent_specific_data/sequencing_panel_bed/TruSight-Myeloid-Illumina/trusight-myeloid-amplicon-v4-track_interval-liftOver-hg38.bed\n\n\n\nHere’s the workflow part of the file:\nversion 1.0\n#### WORKFLOW DEFINITION\n\nworkflow ParseBatchFile {\n  input {\n    File batch_file\n  }\n\n1  Array[Object] batch_info = read_objects(batch_file)\n\n2  scatter (job in batch_info){\n3    String sample_name = job.sampleName\n4    File bam_file = job.bamLocation\n5    File bed_file = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN YOUR BATCH FILE HERE!!!!\n6    call Test {\n      input: in1=sample_name, in2=bam_file, in3=bed_file\n    }\n  }  # End Scatter over the batch file\n\n  # Outputs that will be retained when execution is complete\n  output {\n    Array[File] output_array = Test.item_out\n  }\n\n  parameter_meta {\n    batch_file: \"input tsv containing details about each sample in the batch\"\n    output_array: \"array containing details about each samples in the batch\"\n  }\n} # End workflow\n\n1\n\nRead in file manifest line by line, and store in the array called batch_info.\n\n2\n\nCycle through the manifest line by line, scattering to multiple nodes\n\n3\n\nGet sample_name input from job.sample_name\n\n4\n\nGet bam_file input from job.bam_file\n\n5\n\nGet bed_file input from job.bed_file\n\n6\n\nDo something with the inputs.\n\n\n\n\n6.2.5 Let’s start with WDL Tasks\nWe will start with the lower level of abstraction in WDL: The task.\n\n\n6.2.6 Anatomy of a Task\ntask build_star_index {\n  meta {\n      ...\n  }\n\n  parameter_meta {\n    reference_fasta: \"Reference genome FASTA file\"\n    reference_gtf: \"Reference genome GTF annotation file\"\n    sjdb_overhang: \"Length of the genomic sequence around the annotated junction to be used in constructing the splice junctions database\"\n    genome_sa_index_nbases: \"Length (bases) of the SA pre-indexing string, typically between 10-15 (scales with genome size)\"\n    memory_gb: \"Memory allocated for the task in GB\"\n    cpu_cores: \"Number of CPU cores allocated for the task\"\n  }\n\n1  input {\n    File reference_fasta\n    File reference_gtf\n    Int sjdb_overhang = 100\n    Int genome_sa_index_nbases = 14\n    Int memory_gb = 64\n    Int cpu_cores = 8\n  }\n\n2  command &lt;&lt;&lt;\n    set -eo pipefail\n    \n    mkdir star_index\n\n    echo \"Building STAR index...\"\n    STAR \\\n      --runMode genomeGenerate \\\n      --runThreadN ~{cpu_cores} \\\n      --genomeDir star_index \\\n      --genomeFastaFiles \"~{reference_fasta}\" \\\n      --sjdbGTFfile \"~{reference_gtf}\" \\\n      --sjdbOverhang ~{sjdb_overhang} \\\n      --genomeSAindexNbases ~{genome_sa_index_nbases}\n\n    tar -czf star_index.tar.gz star_index/\n  &gt;&gt;&gt;\n\n3  output {\n    File star_index_tar = \"star_index.tar.gz\"\n  }\n\n4  runtime {\n    docker: \"getwilds/star:2.7.6a\"\n    memory: \"~{memory_gb} GB\"\n    cpu: cpu_cores\n  }\n}\n\n1\n\nInput for our task.\n\n2\n\nBash commands to execute in task\n\n3\n\nDescription of output\n\n4\n\nRuntime requirements for execution. This is a lot like the #SBATCH directives.\n\n\nEverything between the &lt;&lt;&lt; and &gt;&gt;&gt; is essentially a bash script. WDL has its own variables\n\n\n6.2.7 Architecture of a WDL file\nThe best way to read WDL files is to read them top down. We’ll focus on the basic sections of a WDL file before we see how they work together.\nThe code below is from the WILDs WDL Repo.\nworkflow SRA_STAR2Pass {\n   meta{\n   ...\n   }\n\n   parameter_meta{\n   ...\n   }\n\n1  input {\n    ...\n  }\n\n2  if (!defined(reference_genome)) {\n    call download_reference {}\n  }\n\n  RefGenome ref_genome_final = select_first([reference_genome, download_reference.genome])\n\n3  call build_star_index {\n    ...\n  }\n\n  # Outputs that will be retained when execution is complete\n4  output {\n    ...\n  }\n\n} \n\n1\n\nInputs for workflow\n\n2\n\nIf/then statement of workflow\n\n3\n\ncall a task\n\n4\n\nOutputs of workflow\n\n\nLet’s go through each of these in detail (we’ll get back to the meta and parameters-meta sections)\nThe structure of the workflow\nworkflow SRA_STAR2Pass {\n  input {\n    Array[SampleInfo] samples\n    RefGenome? reference_genome\n    String reference_level = \"\"\n    String contrast = \"\"\n  }\n\n  if (!defined(reference_genome)) {\n    call download_reference {}\n  }\n\n  RefGenome ref_genome_final = select_first([reference_genome, download_reference.genome])\n\n  call build_star_index { input:\n      reference_fasta = ref_genome_final.fasta,\n      reference_gtf = ref_genome_final.gtf\n  }\n\n  # Outputs that will be retained when execution is complete\n  output {\n    ...\n  }\n\n}",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Workflows</span>"
    ]
  },
  {
    "objectID": "04_containers_workflows.html#testing-out-scripts",
    "href": "04_containers_workflows.html#testing-out-scripts",
    "title": "6  Containers and Workflows",
    "section": "6.2 Testing out scripts",
    "text": "6.2 Testing out scripts\nOne of the hard things to understand is what can be run on a compute node versus the head node, and what file systems are accessible via a compute node.\nA lot of the issues you might have is because you need to understand the mental model of how cluster computing works. And the best way to understand that is to test your code on a compute node.\nLet’s explore how we can do that.\n\n6.2.1 Testing code on a compute node\nFred Hutch users have the advantage of grabnode, which is a custom command that lets you request an interactive instance of a compute node.\nWhy would you want to do this? A good part of this is about testing software and making sure that your paths are correct.\n\n\n\n\n\n\nDon’t rely on grabnode/interactive for your work\n\n\n\nWe see users that will request a multicore node with higher memory, and do their processing on that node.\nThis doesn’t take advantage of all of the machines that are available on a cluster, and thus is a suboptimal way to utilize the cluster.\nThe other disadvantage is that you may be waiting a very long time to get that multicore node, whereas if you batch across a bunch of nodes, you will get your work done much faster.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Containers and Workflows</span>"
    ]
  },
  {
    "objectID": "container-basics.html",
    "href": "container-basics.html",
    "title": "5  Reading: Container Basics",
    "section": "",
    "text": "5.1 Containers\nWe already learned about software modules (Section 2.5) on the gizmo cluster. There is an alternative way to use software: pulling and running a software The executable software environment actually installed and running on a machine. Runnable. Generate from docker pull from a repository.container.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reading: Container Basics</span>"
    ]
  },
  {
    "objectID": "container-basics.html#containers",
    "href": "container-basics.html#containers",
    "title": "5  Reading: Container Basics",
    "section": "",
    "text": "5.1.1 What is a Container?\nA container is a self-contained unit of software. It contains everything needed to run the software on a variety of machines. If you have the container software installed on your machine, it doesn’t matter whether it is MacOS, Linux, or Windows - the container will behave consistently across different operating systems and architectures.\nThe container has the following contents:\n\nSoftware - The software we want to run in a container. For bioinformatics work, this is usually something like an aligner like bwa, or utilities such as samtools\nSoftware Dependencies - various software packages needed to run the software. For example, if we wanted to run tidyverse in a container, we need to have R installed in the container as well.\nFilesystem - containers have their own isolated filesystem that can be connected to the “outside world” - everything outside of the container. We’ll learn more about customizing these with bind paths (Section 5.1.6).\n\nIn short, the container has everything needed to run the software. It is not a full operating system, but a smaller mini-version that cuts out a lot of cruft.\nContainers are When you are finished with Docker containers, everything that you created in them will disappear when you finish running scripts in the container.ephemeral. They leverage the the file system of their host to manage files. These are called both Volumes (the Docker term) and Bind Paths (the apptainer term).\n\n\n5.1.2 Docker vs. Apptainer\nThere are two basic ways to run Docker containers:\n\nUsing the Docker software\nUsing the Apptainer software (for HPC systems)\n\nIn general, Docker is used on systems where you have a high level of access to the system. This is because docker uses a special user group called docker that has essentially root level privileges. This is not something to be taken lightly.\nThis is not the case for HPC systems, which are shared and granting this level of access to many people is not practical. This is when we use A container software that is often used on HPC systems. Can run Docker containers.Apptainer (which used to be called Singularity), which requires a much lower level of user privileges to execute tasks. For more info, see Section 7.2.1 .\n\n\n\n\n\n\nBe Secure\n\n\n\nBefore we get started, security is always a concern when running containers. The docker group has elevated status on a system, so we need to be careful that when we’re running them, these containers aren’t introducing any system vulnerabilities. Note that on HPC systems, the main mechanism for running containers is apptainer, which is designed to be more secure.\nThese are mostly important when running containers that are web-servers or part of a web stack, but it is also important to think about when running jobs on HPC.\nHere are some guidelines to think about when you are working with a container.\n\nUse vendor-specific Docker Images when possible.\nUse container scanners to spot potential vulnerabilities. DockerHub has a vulnerability scanner that scans your Docker images for potential vulnerabilities. For example, the WILDS Docker library employs a vulnerability scanner and the containers are regularly patched to prevent vulnerabilities.\nAvoid kitchen-sink images. One issue is when an image is built on top of many other images. It makes it really difficult to plug vulnerabilities. When in doubt, use images from trusted people and organizations. At the very least, look at the Dockerfile to see that suspicious software isn’t being installed.\n\n\n\n\n\n5.1.3 Common Containers for Bioinformatics\n\nGATK (the genome analysis toolkit) is one common container that we can use for analysis.\n\n\n\n\n5.1.4 The WILDS Docker Library\nThe Data Science Lab has a set of Docker containers for common Bioinformatics tasks available in the WILDS Docker Library. These include:\n\nsamtools\nbcftools\nmanta\ncnvkit\ndeseq2\n\nAmong many others. Be sure to check it out before you start building your own containers.\n\n\n5.1.5 Pulling a Docker Container\nLet’s pull a docker container from the Docker registry. Note we have to specify docker:// when we pull the container, because Apptainer has its own internal format called SIF.\nmodule load Apptainer/1.1.6\napptainer pull docker://ghcr.io/getwilds/scanpy:latest\napptainer run --bind /path/to/data:/data,/path/to/script:/script docker://getwilds/scanpy:latest python /script/example.py\n\n\n5.1.6 Testing out bind paths in containers\nOne thing to keep in mind is that every container has its own filesystem. One of the hardest things to wrap your head around for containers is how their filesystems work, and how to access files that are outside of the container filesystem. We’ll call any filesystems outside of the container external filesystems to make the discussion a little easier.\nBy default, the containers have access to your current working directory. We could make this where our scripts live (such as /home/tladera2/), but because our data is elsewhere, we’ll need to specify that location (/fh/fast/mylab/) as well.\nThe main mechanism we have in Apptainer to access the external filesystem are bind paths. Much like mounting a drive, we can bind directories from the external filesystem using these bind points.\n\n\n\n\n\nflowchart LR\n   B[\"External Directory\\n/fh/fast/mydata/\"] \n   B --read--&gt; C\n   C --write--&gt; B\n   A[\"Container Filesystem\\n/mydata/\"]--write--&gt;C(\"--bind /fh/fast/mydata/:/mydata/\")\n   C --read--&gt; A\n\n\n\n\n\n\nI think of bind paths as “tunnels” that give access to particular folders in the external filesystem. Once the tunnel is open, we can access data files, process them, and save them using the bind path.\nSay my data lives in /fh/fast/mydata/. Then I can specify a bind point in my apptainer shell and apptainer run commands.\nWe can do this with the --bind option:\napptainer shell --bind /fh/fast/mydata:/mydata docker://biocontainers/samtools:v1.9-4-deb_cv1\nNote that the bind syntax doesn’t have the trailing slash (/). That is, note that it is:\n--bind /fh/fast/mydata: ....\nRather than\n--bind /fh/fast/mydata/: ....\nNow our /fh/fast/mydata/ folder will be available as /mydata/ in my container. We can read and write files to this bind point. For example, I’d refer to the .bam file /fh/fast/mydata/my_bam_file.bam as:\nsamtools view -c /mydata/my_bam_file.bam",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reading: Container Basics</span>"
    ]
  },
  {
    "objectID": "04_containers_workflows.html#section",
    "href": "04_containers_workflows.html#section",
    "title": "6  Containers and Workflows",
    "section": "6.2 ",
    "text": "6.2",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Containers and Workflows</span>"
    ]
  },
  {
    "objectID": "04_containers_workflows.html#developing-and-testing-out-scripts",
    "href": "04_containers_workflows.html#developing-and-testing-out-scripts",
    "title": "6  Containers and Workflows",
    "section": "6.3 Developing and Testing out scripts",
    "text": "6.3 Developing and Testing out scripts\nOne of the hard things to understand is what can be run on a compute node versus the head node, and what file systems are accessible via a compute node.\nA lot of the issues you might have is because you need to understand the mental model of how cluster computing works. And the best way to understand that is to test your code on a compute node.\nLet’s explore how we can do that.\n\n6.3.1 Testing code on a compute node\nFred Hutch users have the advantage of grabnode, which is a custom command that lets you request an interactive instance of a compute node.\nWhy would you want to do this? A good part of this is about testing software and making sure that your paths are correct.\n\n\n\n\n\n\nDon’t rely on grabnode/interactive mode for your work\n\n\n\nWe see users that will request a multicore node with higher memory, and do their processing on that node.\nThis doesn’t take advantage of all of the machines that are available on a cluster, and thus is a suboptimal way to utilize the cluster.\nThe other disadvantage is that you may be waiting a very long time to get that multicore node, whereas if you batch across a bunch of nodes, you will get your work done much faster.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Containers and Workflows</span>"
    ]
  },
  {
    "objectID": "container-basics.html#what-is-json",
    "href": "container-basics.html#what-is-json",
    "title": "5  Reading: Container Basics",
    "section": "5.2 What is JSON?",
    "text": "5.2 What is JSON?\nOne requirement for running workflows is basic knowledge of JSON.\nJSON is short for JavaScript Object Notation. It is a format used for storing information on the web and for interacting with APIs.\n\n5.2.1 How is JSON used?\nJSON is used in multiple ways:\n\nSubmitting Jobs with complex parameters/inputs\n\nSo having basic knowledge of JSON can be really helpful. JSON is the common language of the internet.\n\n\n5.2.2 Elements of a JSON file\nHere are the main elements of a JSON file:\n\nKey:Value Pair. Example: \"name\": \"Ted Laderas\". In this example, our key is “name” and our value is “Ted Laderas”\nList [] - a collection of values. All values have to be the same data type. Example: [\"mom\", \"dad\"]\nObject {} - A collection of key/value pairs, enclosed with curly brackets ({}).\n\n\n\n\n\n\n\nCheck Yourself\n\n\n\nWhat does the names value contain in the following JSON? Is it a list, object or key:value pair?\n{\n  \"names\": [\"Ted\", \"Lisa\", \"George\"]\n}\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nIt is a list. We know this because the value contains a [].\n{\n  \"names\": [\"Ted\", \"Lisa\", \"George\"]\n}\n\n\n\n\n\n5.2.3 JSON Input Files\nWhen you are working with WDL, it is easiest to manage files using JSON files. Here’s the example we’re going to use from the ww-fastq-to-cram workflow.\n#| eval: false\n#| filename: \"json_data/example.json\"\n{\n  \"PairedFastqsToUnmappedCram.batch_info\": [\n    {\n      \"dataset_id\": \"TESTFASTQ1\",\n      \"sample_name\": \"HG02635\",\n      \"library_name\": \"SRR581005\",\n      \"sequencing_center\": \"1000-Genomes\",\n      \"filepaths\": [{\n        \"flowcell_name\": \"20121211\",\n        \"fastq_r1_locations\": [\"tests/data/SRR581005_1.ds.fastq.gz\"],\n        \"fastq_r2_locations\": [\"tests/data/SRR581005_2.ds.fastq.gz\"]\n      }]\n    },\n    {\n      \"dataset_id\": \"TESTFASTQ2\",\n      \"sample_name\": \"HG02642\",\n      \"library_name\": \"SRR580946\",\n      \"sequencing_center\": \"1000-Genomes\",\n      \"filepaths\": [{\n        \"flowcell_name\": \"20121211\",\n        \"fastq_r1_locations\": [\"tests/data/SRR580946_1.ds.fastq.gz\"],\n        \"fastq_r2_locations\": [\"tests/data/SRR580946_2.ds.fastq.gz\"]\n      }]\n    }\n  ]\n}\nThis might seem overwhelming, but let’s look at the top-level structures first:\n1{\n2  \"PairedFastqsToUnmappedCram.batch_info\": [\n   ...   \n  ]\n}\n\n1\n\nThe top level of the file is a JSON object\n\n2\n\nThe next level down (“PairedFastqsToUnmappedCram.batch_info”) is a list.\n\n\nThis workflow specifies the file inputs using the PairedFastqsToUnmappedCram.batch_info object, which is a list.\nEach sample in the PairedFastqsToUnmappedCram.batch_info list is its own object:\n  \"PairedFastqsToUnmappedCram.batch_info\": [\n    {\n      \"dataset_id\": \"TESTFASTQ1\",\n      \"sample_name\": \"HG02635\",\n      \"library_name\": \"SRR581005\",\n      \"sequencing_center\": \"1000-Genomes\",\n      \"filepaths\": [{\n        \"flowcell_name\": \"20121211\",\n        \"fastq_r1_locations\": [\"tests/data/SRR581005_1.ds.fastq.gz\"],\n        \"fastq_r2_locations\": [\"tests/data/SRR581005_2.ds.fastq.gz\"]\n      }]\n    },\n    ....\nBecause we are aligned paired-end data, notice there are two keys, fastq_r1_locations and fastq_r2_locations.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reading: Container Basics</span>"
    ]
  },
  {
    "objectID": "container-basics.html#how-is-json-used-on-the-dnanexus-platform",
    "href": "container-basics.html#how-is-json-used-on-the-dnanexus-platform",
    "title": "5  Reading: Container Basics",
    "section": "5.3 How is JSON used on the DNAnexus Platform?",
    "text": "5.3 How is JSON used on the DNAnexus Platform?\nJSON is used in multiple ways on the DNAnexus Platform, including:\n\nSubmitting Jobs with complex parameters/inputs\nSpecifying parameters of an app or workflow (dxapp.json and dxworkflow.json)\nOutput of commands such as dx find data or dx find jobs with the --json flag\nExtracting environment variables from dx env\n\nUnderneath it all, all interactions with the DNAnexus API server are JSON submissions.\nYou can see that JSON is used in many places on the DNAnexus platforms, and for many purposes. So having basic knowledge of JSON can be really helpful.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reading: Container Basics</span>"
    ]
  },
  {
    "objectID": "container-basics.html#elements-of-a-json-file",
    "href": "container-basics.html#elements-of-a-json-file",
    "title": "5  Reading: Container Basics",
    "section": "5.4 Elements of a JSON file",
    "text": "5.4 Elements of a JSON file\nHere are the main elements of a JSON file:\n\nKey:Value Pair. Example: \"name\": \"Ted Laderas\". In this example, our key is “name” and our value is “Ted Laderas”\nList [] - a collection of values. All values have to be the same data type. Example: [\"mom\", \"dad\"]\nObject {} - A collection of key/value pairs, enclosed with curly brackets ({})\n\nHere’s the example we’re going to use. We’ll do most of our processing of JSON on our own machine.\n#| eval: false\n#| filename: \"json_data/example.json\"\n{\n  \"report_html\": {\n    \"dnanexus_link\": \"file-G4x7GX80VBzQy64k4jzgjqgY\"\n  },\n  \"stats_txt\": {\n    \"dnanexus_link\": \"file-G4x7GXQ0VBzZxFxz4fqV120B\"\n  },\n  \"users\": [\"laderast\", \"ted\", \"tladeras\"]\n}\n\n\n\n\n\n\nCheck Yourself\n\n\n\nWhat does the names value contain in the following JSON? Is it a list, object or key:value pair?\n{\n  \"names\": [\"Ted\", \"Lisa\", \"George\"]\n}\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nIt is a list. We know this because the value contains a [].\n{\n  \"names\": [\"Ted\", \"Lisa\", \"George\"]\n}",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reading: Container Basics</span>"
    ]
  },
  {
    "objectID": "container-basics.html#nestedness",
    "href": "container-basics.html#nestedness",
    "title": "5  Reading: Container Basics",
    "section": "5.3 Nestedness",
    "text": "5.3 Nestedness\nJSON wouldn’t be helpful if it were only limited to a single level or key:values. Values can be lists or objects as well. For example, in our example JSON, we can see that the value of report_html is a JSON object:\n\"report_html\": {\n    \"dnanexus_link\": \"file-G4x7GX80VBzQy64k4jzgjqgY\"\n  }\nThe object is:\n{\n    \"dnanexus_link\": \"file-G4x7GX80VBzQy64k4jzgjqgY\"\n  }\nWhen we work with extracting information, we’ll have to take this nested structure in mind.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reading: Container Basics</span>"
    ]
  },
  {
    "objectID": "04_containers_workflows.html#learning-objectives",
    "href": "04_containers_workflows.html#learning-objectives",
    "title": "6  Workflows",
    "section": "",
    "text": "Execute a workflow on the Fred Hutch Cluster\nModify a workflow and test it on a compute node\nUtilize a container and test inside it on a compute node",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Workflows</span>"
    ]
  },
  {
    "objectID": "testing.html",
    "href": "testing.html",
    "title": "7  Testing Scripts",
    "section": "",
    "text": "7.1 Developing and Testing scripts\nOne of the hard things to understand is what can be run on a compute node versus the head node, and what file systems are accessible via a compute node.\nA lot of the issues you might have is because you need to understand the mental model of how cluster computing works. And the best way to understand that is to test your code on a compute node.\nLet’s explore how we can do that.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Testing Scripts</span>"
    ]
  },
  {
    "objectID": "testing.html#developing-and-testing-scripts",
    "href": "testing.html#developing-and-testing-scripts",
    "title": "7  Testing Scripts",
    "section": "",
    "text": "7.1.1 Testing code on a compute node\nFred Hutch users have the advantage of grabnode, which is a custom command that lets you request an interactive instance of a compute node.\nWhy would you want to do this? A good part of this is about testing software and making sure that your paths are correct.\n\n\n\n\n\n\nDon’t rely on grabnode/interactive mode for your batch work\n\n\n\nWe often see users that will request a multicore node with higher memory, and do their processing on that node.\nThis doesn’t take advantage of all of the machines that are available on the cluster, and thus is a suboptimal way to utilize the cluster.\nWhen you are doing interactive analysis, such as working in JupyterLab or RStudio, that is a valid way to work. But when you have tasks you can scatter amongst many nodes, requesting a high-spec node isn’t a great way to optimally achieve things.\nThe other disadvantage is that you may be waiting a very long time to get that multicore node, whereas if you batch across a bunch of nodes, you will get your work done much faster.\n\n\n\n\n7.1.2 Grabbing an interactive shell on a worker\nWhen you’re testing code that’s going to run on a worker node, you need to be aware of what the worker node sees.\nIt’s also important in estimating how long our tasks are going to run since we can test how long a task runs for a representative dataset.\nOn a SLURM system, the way to open interactive shells on a node has changed. Check your version first:\nsrun --version\nIf you’re on a version before 20.11, you can use srun -i --pty bash to open an interactive terminal on a worker:\nsrun -i --pty bash\nIf the version is past 20.11, we can open an interactive shell on a worker with salloc.\nsalloc bash\n\n\n\n\n\n\nFor FH Users: grabnode\n\n\n\nOn the FH system, we can use a command called grabnode, which will let us request a node. It will ask us for our requirements (numbers of cores, memory, etc.) for our node.\ntladera2@rhino01:~$ grabnode\ngrabnode will then ask us for what kind of instance we want, in terms of CPUs, Memory, and GPUs. Here, I’m grabbing a node with 8 cores, 8 Gb of memory, using it for 1 day, and no GPU.\nHow many CPUs/cores would you like to grab on the node? [1-36] 8\nHow much memory (GB) would you like to grab? [160] 8\nPlease enter the max number of days you would like to grab this node: [1-7] 1\nDo you need a GPU ? [y/N]n\n\nYou have requested 8 CPUs on this node/server for 1 days or until you type exit.\n\nWarning: If you exit this shell before your jobs are finished, your jobs\non this node/server will be terminated. Please use sbatch for larger jobs.\n\nShared PI folders can be found in: /fh/fast, /fh/scratch and /fh/secure.\n\nRequesting Queue: campus-new cores: 8 memory: 8 gpu: NONE\nsrun: job 40898906 queued and waiting for resources\nAfter a little bit, you’ll arrive at a new prompt:\n(base) tladera2@gizmok164:~$\nNow you can test your batch scripts, in order to make sure your file paths are correct. It is also helpful in profiling your job.\nIf you’re doing interactive analysis that is going to span over a few days, I recommend that you use screen or tmux.\n\n\n\n\n\n\n\n\nRemember hostname\n\n\n\nWhen you are doing interactive analysis, it is easy to forget in which node you’re working in. Just as a quick check, I use hostname to remind myself whether I’m in rhino, gizmo, or within an apptainer container.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Testing Scripts</span>"
    ]
  },
  {
    "objectID": "testing.html#sec-containers",
    "href": "testing.html#sec-containers",
    "title": "7  Testing Scripts",
    "section": "7.2 Working with containers",
    "text": "7.2 Working with containers\nI think the hardest thing about working with containers is wrapping your head around the indirectness of them. You are running software with its own internal filesystem and the challenges are getting the container to read files in folders/paths outside of its own filesytem, as well as outputting files into those outside folders.\n\n7.2.1 Testing code in a container\nIn this section, we talk about testing scripts in a container using apptainer. We use apptainer (formerly Singularity) in order to run Docker containers on a shared HPC system. This is because Docker itself requires root-level privileges, which is not secure on shared systems.\nIn order to do our testing, we’ll first pull the Docker container, map our bind point (so our container can access files outside of its file system), and then run scripts in the container.\nEven if you aren’t going to frequently use Apptainer in your work, I recommend trying an interactive shell in a container at least once or twice to learn about the container filesystem and conceptually understand how you connect it to the external filesystem.\n\n\n7.2.2 Pulling a Docker Container\nLet’s pull a docker container from the Docker registry. Note we have to specify docker:// when we pull the container, because Apptainer has its own internal format called SIF.\nmodule load Apptainer/1.1.6\napptainer pull docker://biocontainers/samtools:v1.9-4-deb_cv1\n\n\n7.2.3 Opening a Shell in a Container with apptainer shell\nWhen you’re getting started, opening a shell using Apptainer can help you test out things like filepaths and how they’re accessed in the container. It’s hard to get an intuition for how file I/O works with containers until you can see the limited view from the container.\nBy default, apptainers can see your current directory and navigate to the files in it.\nYou can open an Apptainer shell in a container using apptainer shell. Remember to use docker:// before the container name. For example:\nmodule load Apptainer/1.1.6\napptainer shell docker://biocontainers/samtools:v1.9-4-deb_cv1\nThis will load the apptainer module, and then open a Bash shell in the container using apptainer shell. Once you’re in the container, you can test code, especially seeing whether your files can be seen by the container (see Section 5.1.6). 90% of the issues with using Docker containers has to do with bind paths, so we’ll talk about that next.\nOnce you’re in the shell, you can take a look at where samtools is installed:\nwhich samtools\nNote that the container filesystem is isolated, and we need to explicitly build connections to it (called bind paths) to get files in and out. We’ll talk more about this in the next section.\nOnce we’re done testing scripts in our containers, we can exit the shell and get back into the node.\nexit\n\n\n\n\n\n\nOpening a Shell in a Docker Container with Docker\n\n\n\nFor the most part, due to security reasons, we don’t use docker on HPC systems. In short, the docker group essentially has root-level access to the machine, and it’s not a good for security on a shared resource like an HPC.\nHowever, if you have admin level access (for example, on your own laptop), you can open up an interactive shell with docker run -it:\ndocker run -it biocontainers/samtools:v1.9-4-deb_cv1 /bin/bash\nThis will open a bash shell much like apptainer shell. Note that volumes (the docker equivalent of bind paths) are specified differently in Docker compared to Apptainer.\n\n\n\n\n\n\n\n\nWDL makes this way easier\n\n\n\nA major point of failure with Apptainer scripting is when our scripts aren’t using the right bind paths. It becomes even more complicated when you are running multiple steps.\nThis is one reason we recommend writing WDL Workflows and a A system that works with the cluster manager to orchestrate processing data through a workflowworkflow manager (such as A workflow runner. Currently works with WDL files.Cromwell or Sprocket) to run your workflows. You don’t have to worry that your bind points are setup correctly, because they are handled by the workflow manager.\n\n\n\n\n7.2.4 Testing in the Apptainer Shell\nOk, now we have a bind point, so now we can test our script in the shell. For example, we can see if we are invoking samtools in the correct way and that our bind points work.\nsamtools view -c /mydata/my_bam_file.bam &gt; /mydata/bam_counts.txt\nAgain, trying out scripts in the container is the best way to understand what the container can and can’t see.\n\n\n7.2.5 Exiting the container when you’re done\nYou can exit, like any shell you open. You should be out of the container. Confirm by using hostname to make sure you’re out of the container.\n\n\n7.2.6 Testing outside of the container\nLet’s take everything that we learned and put it in a script that we can run on the HPC:\n# Script to samtools view -c an input file:\n# Usage: ./run_sam.sh &lt;my_bam_file.bam&gt;\n# Outputs a count file: my_bam_file.bam.counts.txt\n#!/bin/bash\nmodule load Apptainer/1.1.6\napptainer run --bind /fh/fast/mydata:/mydata docker://biocontainers/samtools:v1.9-4-deb_cv1 samtools view -c /mydata/$1 &gt; /mydata/$1.counts.txt\n#apptainer cache clean\nmodule purge\nWe can use this script by the following command:\n./run_sam.sh chr1.bam \nAnd it will output a file called chr1.bam.counts.txt.\n\n\n\n\n\n\nApptainer Cache\n\n\n\nThe apptainer cache is where your docker images live. They are translated to the native apptainer .sif format.\nYou can see what’s in your cache by using\napptainer cache list\nBy default the cache lives at ~/.apptainer/cache.\nIf you need to clear out the cache, you can run\napptainer cache clean\nto clear out the cache.\nThere are a number of environment variables (Section 8.5) that can be set, including login tokens for pulling from a private registry. More information is here.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Testing Scripts</span>"
    ]
  },
  {
    "objectID": "miscellaneous.html#sec-hostname",
    "href": "miscellaneous.html#sec-hostname",
    "title": "9  Miscellaneous",
    "section": "",
    "text": "hostname\ngizmok164\n\n\n9.1.1 Try it out\nAfter logging into rhino, try running hostname. What host are you on?",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Miscellaneous</span>"
    ]
  },
  {
    "objectID": "04_containers_workflows.html#where-next",
    "href": "04_containers_workflows.html#where-next",
    "title": "6  Workflows",
    "section": "6.3 Where Next?",
    "text": "6.3 Where Next?\nNow that you understand the basics of working with Bash and WDL, you are now ready to start working with WDL workflows.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Workflows</span>"
    ]
  },
  {
    "objectID": "03_batch.html#redirection-and-pipes",
    "href": "03_batch.html#redirection-and-pipes",
    "title": "4  Batch Processing and Submitting Jobs",
    "section": "4.4 Redirection and Pipes",
    "text": "4.4 Redirection and Pipes\nOk, we’ve executed",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Batch Processing and Submitting Jobs</span>"
    ]
  },
  {
    "objectID": "miscellaneous.html#screen",
    "href": "miscellaneous.html#screen",
    "title": "9  Miscellaneous",
    "section": "9.7 screen",
    "text": "9.7 screen",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Miscellaneous</span>"
    ]
  },
  {
    "objectID": "miscellaneous.html#sec-screen",
    "href": "miscellaneous.html#sec-screen",
    "title": "9  Miscellaneous",
    "section": "9.7 screen or tmux: keep your session open",
    "text": "9.7 screen or tmux: keep your session open\n\nOftentimes, when you are running something interactive on a system, you’ll have to leave your shell open. Otherwise, your running job will terminate.\nYou can use screen or tmux, which are known as window managers, to keep your sessions open on a remote machine. We’ll talk about screen.\n\nscreen works by starting a new bash shell. You can tell this because your bash prompt will change.\nThe key of working remotely with screen is that you can then request an hpc node.\nFor FH users, you can request a gizmo node using grabnode. We can then check we’re on the gizmo node by using hostname.\nIf we have something running on this node, we can keep it running by detaching the screen session. Once we are detached, we should check that we’re back in rhino by using hostname. Now we can log out and our job will keep running.\n\nIf we need to get back into that screen session, we can use:\nscreen -ls\nTo list the number of sessions:\nThere is a screen on:\n        37096.pts-321.rhino01   (05/10/2024 10:21:54 AM)        (Detached)\n1 Socket in /run/screen/S-tladera2.\nOnce we’ve found the id for our screen session (in this case it’s 37096), we can reattach to the screen session using:\nscreen -r 37096\nAnd we’ll be back in our screen session! Handy, right?\n\n\n\n\n\n\nFor FH Users\n\n\n\nNote that if you logout from rhino, you’ll need to log back into the same rhino node to access your screen session.\nFor example, if my screen session was on rhino01, I’d need to ssh back into rhino01, not rhino02 or rhino03. This means you will need to ssh into rhino01 specifically to get back into your screen session.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Miscellaneous</span>"
    ]
  },
  {
    "objectID": "container-basics.html#sec-json",
    "href": "container-basics.html#sec-json",
    "title": "5  Reading: Container Basics",
    "section": "5.2 What is JSON?",
    "text": "5.2 What is JSON?\nOne requirement for running workflows is basic knowledge of JSON.\nJSON is short for JavaScript Object Notation. It is a format used for storing information on the web and for interacting with APIs.\n\n5.2.1 How is JSON used?\nJSON is used in multiple ways:\n\nSubmitting Jobs with complex parameters/inputs\n\nSo having basic knowledge of JSON can be really helpful. JSON is the common language of the internet.\n\n\n5.2.2 Elements of a JSON file\nHere are the main elements of a JSON file:\n\nKey:Value Pair. Example: \"name\": \"Ted Laderas\". In this example, our key is “name” and our value is “Ted Laderas”\nList [] - a collection of values. All values have to be the same data type. Example: [\"mom\", \"dad\"]\nObject {} - A collection of key/value pairs, enclosed with curly brackets ({}).\n\n\n\n\n\n\n\nCheck Yourself\n\n\n\nWhat does the names value contain in the following JSON? Is it a list, object or key:value pair?\n{\n  \"names\": [\"Ted\", \"Lisa\", \"George\"]\n}\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nIt is a list. We know this because the value contains a [].\n{\n  \"names\": [\"Ted\", \"Lisa\", \"George\"]\n}\n\n\n\n\n\n5.2.3 JSON Input Files\nWhen you are working with WDL, it is easiest to manage files using JSON files. Here’s the example we’re going to use from the ww-fastq-to-cram workflow.\n#| eval: false\n#| filename: \"json_data/example.json\"\n{\n  \"PairedFastqsToUnmappedCram.batch_info\": [\n    {\n      \"dataset_id\": \"TESTFASTQ1\",\n      \"sample_name\": \"HG02635\",\n      \"library_name\": \"SRR581005\",\n      \"sequencing_center\": \"1000-Genomes\",\n      \"filepaths\": [{\n        \"flowcell_name\": \"20121211\",\n        \"fastq_r1_locations\": [\"tests/data/SRR581005_1.ds.fastq.gz\"],\n        \"fastq_r2_locations\": [\"tests/data/SRR581005_2.ds.fastq.gz\"]\n      }]\n    },\n    {\n      \"dataset_id\": \"TESTFASTQ2\",\n      \"sample_name\": \"HG02642\",\n      \"library_name\": \"SRR580946\",\n      \"sequencing_center\": \"1000-Genomes\",\n      \"filepaths\": [{\n        \"flowcell_name\": \"20121211\",\n        \"fastq_r1_locations\": [\"tests/data/SRR580946_1.ds.fastq.gz\"],\n        \"fastq_r2_locations\": [\"tests/data/SRR580946_2.ds.fastq.gz\"]\n      }]\n    }\n  ]\n}\nThis might seem overwhelming, but let’s look at the top-level structures first:\n1{\n2  \"PairedFastqsToUnmappedCram.batch_info\": [\n   ...   \n  ]\n}\n\n1\n\nThe top level of the file is a JSON object\n\n2\n\nThe next level down (“PairedFastqsToUnmappedCram.batch_info”) is a list.\n\n\nThis workflow specifies the file inputs using the PairedFastqsToUnmappedCram.batch_info object, which is a list.\nEach sample in the PairedFastqsToUnmappedCram.batch_info list is its own object:\n  \"PairedFastqsToUnmappedCram.batch_info\": [\n    {\n      \"dataset_id\": \"TESTFASTQ1\",\n      \"sample_name\": \"HG02635\",\n      \"library_name\": \"SRR581005\",\n      \"sequencing_center\": \"1000-Genomes\",\n      \"filepaths\": [{\n        \"flowcell_name\": \"20121211\",\n        \"fastq_r1_locations\": [\"tests/data/SRR581005_1.ds.fastq.gz\"],\n        \"fastq_r2_locations\": [\"tests/data/SRR581005_2.ds.fastq.gz\"]\n      }]\n    },\n    ....\nBecause we are aligned paired-end data, notice there are two keys, fastq_r1_locations and fastq_r2_locations.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reading: Container Basics</span>"
    ]
  }
]