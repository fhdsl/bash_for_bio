[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bash for Bioinformatics",
    "section": "",
    "text": "Bash for Bio",
    "crumbs": [
      "Bash for Bio"
    ]
  },
  {
    "objectID": "index.html#who-this-course-is-for",
    "href": "index.html#who-this-course-is-for",
    "title": "Bash for Bioinformatics",
    "section": "Who this course is for",
    "text": "Who this course is for\n\nHave you needed to align a folder of FASTA files and not know how to do it?\nDo you want to automate an R or Python script you wrote to work on a bunch of files?\nDo you want to do all of this on a high performance cluster (HPC)?\n\nIf so, this course is for you! We will learn enough bash scripting to do useful things on the Fred Hutch computing cluster (affectionately called “gizmo”) and automate the boring parts.",
    "crumbs": [
      "Bash for Bio"
    ]
  },
  {
    "objectID": "index.html#learning-objectives",
    "href": "index.html#learning-objectives",
    "title": "Bash for Bioinformatics",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nApply bash scripting to execute alignment, and Python/R scripts\nNavigate and process data on the different filesystems available at FH\nManage software dependencies reproducibly using container-based technologies such as Docker/Apptainer containers or EasyBuild modules\nArticulate basic HPC architecture concepts and why they’re useful in your work\nLeverage bash scripting to execute Running the same task on a group of files. Can be done locally (on your computer) or on a cluster.batch jobs on a high performance cluster.\nUtilize workflow managers such as cromwell to process multiple files in a multi-step Short for Workflow Description Language. A standard for specifying a workflow, which includes describing inputs, saving intermediate outputs, and outputting processed files.WDL workflow.\n\n\n\n\n\n\n\nNoteWasn’t there another Bash for Bioinformatics book?\n\n\n\nI originally wrote a book that was called Bash for Bioinformatics, which was about learning enough bash to use the cloud-based DNANexus platform effectively.\nI have renamed that book Bash for DNANexus, and named this course Bash for Bioinformatics.\nThis book shares bones with Bash for DNANexus, but has more of a focus on running tasks on high performance computing systems (HPC).",
    "crumbs": [
      "Bash for Bio"
    ]
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Bash for Bioinformatics",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou will need an account on rhino and know how to connect to it through VPN. If you have taken the Intro to Fred Hutch Cluster Computing workshop, then you will be ready.\nWe highly recommend reviewing Intro to Command Line and Intro to Fred Hutch Cluster Computing.\nBasic knowledge of the following commands:\n\nls\ncd and basic directory navigation\nmv/cp/mkdir/rm\n\n\nWe will assume that you will do all of your work in your home directory on Rhino. We will not be using that much space in your home directory.\n\n\n\n\n\n\nNoteTerminology\n\n\n\nWe know that not all of us have the same vocabulary. We try to define terminology as much as possible. These are indicated by double underlines such as this:\nA task that we have assigned a computer to run. This computer can be a compute node in a cluster, or our own machine.Compute Job\nYou can click and hold on the term to define it.\n\n\n## Instructors / TAs\nIf you need to schedule some time to talk, please schedule with Ted.\n\nTed Laderas (Main Instructor), Director of Training and Community, Office of the Chief Data Officer\nTaylor Firman (TA), Research Informatics Lead, Office of the Chief Data Officer\nScott Chamberlain (TA), Software Developer, Office of the Chief Data Officer\nChris Lo (TA), Data Science Trainer, Office of the Chief Data Officer\n\nWe all have experience running jobs on HPC and gizmo. Please reach out if you have any questions.",
    "crumbs": [
      "Bash for Bio"
    ]
  },
  {
    "objectID": "index.html#introductions",
    "href": "index.html#introductions",
    "title": "Bash for Bioinformatics",
    "section": "Introductions",
    "text": "Introductions\nIn chat, please introduce yourself:\n\nYour Name & Your Group\nWhat you want to learn in this course\nFavorite Fall activity",
    "crumbs": [
      "Bash for Bio"
    ]
  },
  {
    "objectID": "index.html#culture-of-the-course",
    "href": "index.html#culture-of-the-course",
    "title": "Bash for Bioinformatics",
    "section": "Culture of the course",
    "text": "Culture of the course\nIt is hard work teaching an online/hybrid course. If you can, please turn your camera on - it is difficult to teach to a group of blank screens.\n\nLearning on the job is challenging\n\nI will move at learner’s pace; we are learning together.\nTeach not for mastery, but teach for empowerment to learn effectively.\n\n\nWe sometimes struggle with our data science in isolation, unaware that someone two doors down from us has gone through the same struggle.\n\nWe learn and work better with our peers.\nKnow that if you have a question, other people will have it.\nAsking questions is our way of taking care of others.\n\nWe ask you to follow Participation Guidelines and Code of Conduct.\nPlease note that this is the first time this course has been given - we have done our best to edit all mistakes out there, but there may be mistakes. So be patient and reach out if something isn’t working.\nIf you do find a mistake, please report it to Ted. I’ll add you to the acknowledgements below.",
    "crumbs": [
      "Bash for Bio"
    ]
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Bash for Bioinformatics",
    "section": "Schedule",
    "text": "Schedule\nClass is on Thursdays, 12:00 - 1:30 PM. There will be an office hour 1/2 hour after class if you need help.\nYou should complete the readings before class for weeks 3 and 4, so we can hit the ground running.\n\n\n\nWeek\nDate\nTopics\nReading\n\n\n\n\nPreclass\n\nReview Intro to Command Line and Cluster 101\n\n\n\nWeek 1\nOctober 9\nFilesystem Basics\nBite Size Bash\n\n\nWeek 2\nOctober 16\nWriting and Running Bash Scripts\nBite Size Bash\n\n\nNo Class\nOctober 23\nOCDO Retreat\n\n\n\nWeek 3\nOctober 30\nBatch Processing and HPC Jobs\nHPC Basics\n\n\nWeek 4\nNovember 6\nTesting Scripts/Workflow Managers\nContainer Basics\n\n\nOn your own time\nTesting Scripts\n\n\n\n\nOn your own time\nConfiguring your Bash Shell",
    "crumbs": [
      "Bash for Bio"
    ]
  },
  {
    "objectID": "index.html#reference-texts",
    "href": "index.html#reference-texts",
    "title": "Bash for Bioinformatics",
    "section": "Reference Texts",
    "text": "Reference Texts\n\nWe will be using Julia Evan’s Bite Size Bash as our reference text. Julia’s explanations are incredibly clear and it will be a valuable reference even beyond this course. The PDF is available in the Google Classroom materials. Please do not share with others - we have a group rate and it is only $12 for individual purchases.\nIf you want to know the true power of the command line, I recommend Data Science at the Command Line. This book showcases how much you can get done with just command line.",
    "crumbs": [
      "Bash for Bio"
    ]
  },
  {
    "objectID": "index.html#badge-of-completion",
    "href": "index.html#badge-of-completion",
    "title": "Bash for Bioinformatics",
    "section": "Badge of completion",
    "text": "Badge of completion\n\nWe offer a badge of completion when you finish the course!\nWhat it is:\n\nA display of what you accomplished in the course, shareable in your professional networks such as LinkedIn, similar to online education services such as Coursera. A way for you to be accountable for your learning.\n\nWhat it isn’t:\n\nAccreditation through an university or degree-granting program.\n\nRequirements:\n\nSign up on the badging spreadsheet (will send link out in class).\nComplete badge-required sections of the exercises for 3 out of 4 assignments. We’ll cover this in class.",
    "crumbs": [
      "Bash for Bio"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Bash for Bioinformatics",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis course would not be live without the efforts of:\n\nEmma Bishop\nScott Chamberlain\nTaylor Firman\nChris Lo\nSonu Mishra\nSitapriya Moorthi\nDan Tenenbaum\n\nThanking you for all your help testing and editing this course.",
    "crumbs": [
      "Bash for Bio"
    ]
  },
  {
    "objectID": "01_basics.html",
    "href": "01_basics.html",
    "title": "1  Navigating the Bash Filesystem",
    "section": "",
    "text": "1.1 Learning Objectives\nBy the end of this session, you should be able to:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Navigating the Bash Filesystem</span>"
    ]
  },
  {
    "objectID": "01_basics.html#learning-objectives",
    "href": "01_basics.html#learning-objectives",
    "title": "1  Navigating the Bash Filesystem",
    "section": "",
    "text": "Navigate and copy data to the different filesystems available at Fred Hutch.\nExplain the difference between absolute and relative file paths.\nSet Permissions on and execute a bash script\nExecute scripts written in Python and R on the command line\nFind help on the system and on the web",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Navigating the Bash Filesystem</span>"
    ]
  },
  {
    "objectID": "01_basics.html#exercises",
    "href": "01_basics.html#exercises",
    "title": "1  Navigating the Bash Filesystem",
    "section": "1.2 Exercises",
    "text": "1.2 Exercises\nOpen up the exercises here or in Google Classroom.\n\n\n\n\n\n\nNoteReminder about Terminology\n\n\n\nDefined words are double underlined. You can click and hold on them to see the definition. Try it below!\nInformation about a file or dataset, such as the filename, or date created.metadata",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Navigating the Bash Filesystem</span>"
    ]
  },
  {
    "objectID": "01_basics.html#navigating-the-bash-terminal",
    "href": "01_basics.html#navigating-the-bash-terminal",
    "title": "1  Navigating the Bash Filesystem",
    "section": "1.3 Navigating the Bash Terminal",
    "text": "1.3 Navigating the Bash Terminal\n\nWe recommend that you review the material for Intro to Command Line and know the following: Changing directories and relative paths.\n\nBy default, when you log into a remote system such as rhino, you are in a Short for Bourne Again Shell - it is the command processor that is typically run in a text window, has its own programming language and syntax.bash A command processor that we interact with via the system prompt. In our case, the Shell we’re interacting with is on the server.shell.\nWhy is it a bash shell? Bash is the default shell for linux systems, especially for high performance clusters (HPCs), and there are some quirks about navigating around the command line you should be aware of.\n\n\n\n\n\n\nNoneA helpful key: &lt;Up Arrow&gt;\n\n\n\nThe  key will let you cycle through your history, or previous executed commands. This can be super helpful if you have typed a long command with a syntax error. You can use &lt;Up Arrow&gt; to fix mistakes and run that command again.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Navigating the Bash Filesystem</span>"
    ]
  },
  {
    "objectID": "01_basics.html#setting-yourself-up-for-success",
    "href": "01_basics.html#setting-yourself-up-for-success",
    "title": "1  Navigating the Bash Filesystem",
    "section": "1.4 Setting Yourself Up for Success",
    "text": "1.4 Setting Yourself Up for Success\nMake Sure you:\n\nConnect to VPN\nKeep the Bash for Bioformatics page open\nOpen the assignment in Google Classroom to take notes\nConnect to rhino using terminal or PuTTY or On Demand (see below).\n\nI will demo how to connect to rhino using the Scicomp On Demand dashboard. This site has a handy “Rhino Shell Access” menu item under “Clusters”.\nSo now we have logged into rhino. Now what?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Navigating the Bash Filesystem</span>"
    ]
  },
  {
    "objectID": "01_basics.html#navigating-the-filesystems",
    "href": "01_basics.html#navigating-the-filesystems",
    "title": "1  Navigating the Bash Filesystem",
    "section": "1.5 Navigating the Filesystems",
    "text": "1.5 Navigating the Filesystems\n\n1.5.1 pwd Where Am I?\nThe pwd command (short for present working directory) will let you know your current location in the filesystem. Knowing your current directory is critical when using relative file paths.\nIf I run pwd right after signing into rhino I get:\n/home/tladera2\nYou should have a similar path, except with your user name. This is your home directory - where you have a limited amount of space to store scripts and other files. Don’t worry, the majority of your data is stored elsewhere ()",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Navigating the Bash Filesystem</span>"
    ]
  },
  {
    "objectID": "01_basics.html#sec-home",
    "href": "01_basics.html#sec-home",
    "title": "1  Navigating the Bash Filesystem",
    "section": "1.6 Going /home: ~/",
    "text": "1.6 Going /home: ~/\nThere is one important file alias you should always remember: ~/ is shorthand for your own home directory.\nDepending on the linux distribution, this can be a different location. On the FH filesystem, when I use ~/, it maps to:\n/home/tladera2/\nThe home directory is also important because it is where your configuration files live, such as .bashrc (see Section 11.1).\nDepending on how you work, you may want to store your scripts and workflows in /home/. Some people prefer to keep their scripts, data, and results in a single folder. This is not really practical for most genomics projects, unless you are saving processed data. For more info, see Section 12.10.\n\n\n\n\n\n\nNoneYour current working directory: .\n\n\n\nThere is an alias for your current directory: . (the period sign).\nThis becomes useful when you want to output files to your current location. We’ll use this later.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Navigating the Bash Filesystem</span>"
    ]
  },
  {
    "objectID": "01_basics.html#grabbing-stuff-from-github",
    "href": "01_basics.html#grabbing-stuff-from-github",
    "title": "1  Navigating the Bash Filesystem",
    "section": "1.7 Grabbing Stuff from GitHub",
    "text": "1.7 Grabbing Stuff from GitHub\nFor the rest of the exercises for today, we’ll be grabbing the scripts from github using git clone.\ngit clone https://github.com/fhdsl/bash_for_bio\nThis will create a folder called bash_for_bio/ in our current directory. This directory has all of the course materials, including the scripts.\n\n\n\n\n\n\nImportantStay in bash_for_bio/\n\n\n\nThroughout this course, I expect you to run code in the base bash_for_bio/ folder, not in scripts or in data. All of the code is tested with this in mind.\nIf you are having problems executing the code, please make sure you are in the base bash_for_bio folder, or adjust your file paths when running the script.\n\n\n\n1.7.1 du: How much space?\nOne of the first things we can do is check for disk usage with the du command. If I run du by itself on the command line, it will give me the disk usage of all folders and files in our current directory, which is a lot of output.\nThere is an option called -d that lets us specify the depth. -d 1 will give us only the file sizes of the top level folders in our directory.\nMake sure you are in the bash_for_bio/ directory. Then try the following command:\ndu -d 1 -h .\nHere are the first few lines of my du output within the bash_for_bio folder:\n240K    ./_extensions\n192K    ./.quarto\n616K    ./scripts\n1.9M    ./data\n8.6M    ./.git\n6.7M    ./docs\n10M ./images\n30M .\n\nIf we want to specify du to scan only a single folder, we can give the folder name.\ndu -d 1 scripts\nAnd I will get the following output:\n144K    scripts/week1\n56K scripts/__pycache__\n128K    scripts/week3\n232K    scripts/week2\n616K    scripts\n\n\n\n\n\n\nNoneTry it out\n\n\n\nTry checking the disk usage using du for the bash_for_bio/ folder in your /home directory (mine is /home/tladera2/bash_for_bio/).\ndu -d 1 bash_for_bio/\nTry out using du -d 2 on your home directory:\ndu -d 2 ~/",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Navigating the Bash Filesystem</span>"
    ]
  },
  {
    "objectID": "01_basics.html#sec-filesystems",
    "href": "01_basics.html#sec-filesystems",
    "title": "1  Navigating the Bash Filesystem",
    "section": "1.8 FH users: the main filesystems",
    "text": "1.8 FH users: the main filesystems\nWhen working on the Fred Hutch HPC, there are four main filesystems you should consider:\n\n/home/ - The home filesystem. Your scripts can live here. Also where your configuration files (such as .bashrc) live. Can be accessed using ~/.\n/fh/fast/ (also known as fast) - Research storage. Raw files and processed results should live here.\n/hpc/temp/ (also known as temp) - The temporary filesystem. This filesystem is faster to access for gizmo nodes on the cluster, so files can be copied to for computation. The output files you generate should be moved back into an appropriate folder on /fh/fast/. Note that files on /hpc/temp/ will be deleted after 30 days.\n/fh/regulated/ - A secure filesystem meant for NIH regulated data. If you are processing data that is regulated under the current NIH guidelines, you will process it here.\n\nSo, how do we utilize these filesystems? We will be running commands like this:\n1ml BWA\n2bwa mem -M -t 2 \\\n3  /fh/fast/reference_data/chr20 \\\n4  /fh/fast/laderas_t/raw_data/na12878_1.fq \\\n  /fh/fast/laderas_t/raw_data/na12878_2.fq &gt; \\\n5  /hpc/temp/laderas_t/aligned_data/na12878_1.sam\n\n1\n\nLoad bwa software\n\n2\n\nStart bwa mem (aligner)\n\n3\n\npath of genome index\n\n4\n\npath of paired end reads files\n\n5\n\npath of output\n\n\nTo understand the above, We first have to familiarize ourselves with absolute vs relative paths.\n\n\n\n\n\n\nNoneWhen you need to span multiple lines: \\\n\n\n\nSometimes it’s hard to read code that is a single line. You can break up a very long line of code using the \\ (backslash) character.\nFor example, instead of:\nbwa mem -M -t 2 /fh/fast/reference_data/chr20 /fh/fast/laderas_t/raw_data/na12878_1.fq /fh/fast/laderas_t/raw_data/na12878_2.fq &gt; /hpc/temp/laderas_t/aligned_data/na12878_1.sam  \nWe can rewrite it as:\nml BWA                                                \nbwa mem -M -t 2 \\                                     \n1  /fh/fast/reference_data/chr20 \\\n2  /fh/fast/laderas_t/raw_data/na12878_1.fq \\\n3  /fh/fast/laderas_t/raw_data/na12878_2.fq &gt; \\\n4  /hpc/temp/laderas_t/aligned_data/na12878_1.sam\n\n1\n\nPath of reference genome\n\n2\n\n1st paired end read FASTQ file\n\n3\n\n2nd paired end read FASTQ file\n\n4\n\nOutput file location\n\n\nWe’ll use this throughout the book so the code is easier to read.\n\n\n\n1.8.1 More about the FH Filesystems\nhttps://sciwiki.fredhutch.org/scicomputing/store_posix/",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Navigating the Bash Filesystem</span>"
    ]
  },
  {
    "objectID": "01_basics.html#sec-paths",
    "href": "01_basics.html#sec-paths",
    "title": "1  Navigating the Bash Filesystem",
    "section": "1.9 Absolute versus relative paths",
    "text": "1.9 Absolute versus relative paths\nYou may have muddled with file paths, and maybe have used absolute paths to specify the location of a file. When you are processing files, it is important to understand the difference.\nAbsolute paths contain all the information needed to find a file in a file system from the root / directory. For example, this would be an absolute path:\n/fh/fast/laderast/immuno_project/raw_data/chr2.fa.gz\nAbsolute paths always start with /, because that is the root directory, where all the top folders and files live.\nIn terms of folder structure, this is what this looks like:\n1/\n2├── fh\n│   └──fast\n│       └──laderast\n|            └──immuno_project\n│                 └──raw_data\n│                    └──chr2.fa.gz\n\n1\n\nRoot directory\n\n2\n\nFolders in root directory\n\n\nRelative paths break up an absolute path into two pieces of information: 1) your current directory and 2) the path relative to that directory. Relative paths are really helpful because things don’t break when you move your folder or files.\nIf my current working directory is the directory /fh/fast/laderas_t/immuno_project/, then the relative path to that same file would be:\nraw_data/chr2.fa.gz\nWe can visualize the relative path like this, where our working directory is indicated by a star:\n1/\n2├── fh/fast/laderast/immuno_project/\n3|                                   └──raw_data\n│                                      └──chr2.fa.gz\n                                    \n\n1\n\nThe root directory\n\n2\n\nOur working directory\n\n3\n\nOur relative path\n\n\nNote that this relative path does not start with a /, because our current directory isn’t the root directory. Relative paths are incredibly useful when scripting in a reproducible manner, such as using project-based workflows to process files in a single folder.\n\n\n\n\n\n\nNone&lt;TAB&gt; is for autocompletion of paths\n\n\n\nNever underestimate the usefulness of the &lt;TAB&gt; key, which triggers autocompletion on the command line. It can help you complete paths to files and save you a lot of typing.\nFor example, say I have a path that I want to navigate to\n/home/tladera2/my_long_path\nI can type in the first part of the path and then hit &lt;TAB&gt;:\n/home/tladera2/my_&lt;TAB&gt;\nAnd if the prefix my_ is unique in my folder, it will autocomplete the path:\n/home/tladera2/my_long_path\nNote that we need to use enough of the folder name so that completing it is unambiguous. If there are multiple choices, then autocomplete will list all of them.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Navigating the Bash Filesystem</span>"
    ]
  },
  {
    "objectID": "01_basics.html#sec-permissions",
    "href": "01_basics.html#sec-permissions",
    "title": "1  Navigating the Bash Filesystem",
    "section": "1.10 File Permissions",
    "text": "1.10 File Permissions\nFile permissions are Information about a file or dataset, such as the filename, or date created.metadata that are attached to file objects. They are how the system prevents certain files from being modified or restricting access of these files to certain people or groups.\nAll files have the following level of access permissions:\n\n\n\nLevel\nDescription\n\n\n\n\nOwner-level\nThe owner of the file\n\n\nGroup-level\nThe group of the file\n\n\nEveryone\nThe rest of the world\n\n\n\nFor example, if I’m the owner of the file, I can restrict the type of access to only myself (owner-level), the group I’m in (Group-level), or make the file freely available to everyone on the system (Everyone).\nEach level has the following type of access:\n\n\n\n\n\n\n\n\n\nType\nDescription\nAbbreviation\nExample\n\n\n\n\nRead\nLevel can only read contents of file\nr\nA list of users in a text file\n\n\nWrite\nLevel can write to the file\nw\nAppending an entry to the end of a log\n\n\nExecute\nLevel can run the file as an executable\nx\nsamtools\n\n\n\nYou can see the permissions for a file using the ls -l &lt;FILENAME&gt;. For example:\nls -l -h .\nwill give me the following line:\n-rw-rw---- 1 tladera2 g_tladera2 1.8K Oct  2 15:21 01_assignment.qmd\n-rw-rw---- 1 tladera2 g_tladera2  21K Oct  2 15:21 01_basics.qmd\n-rw-rw---- 1 tladera2 g_tladera2 2.3K Oct  2 15:21 02_assignment.qmd\n-rw-rw---- 1 tladera2 g_tladera2  21K Oct  2 15:21 02_scripting.qmd\n-rw-rw---- 1 tladera2 g_tladera2 1.2K Oct  2 15:21 03_assignment.qmd\n-rw-rw---- 1 tladera2 g_tladera2  13K Oct  2 15:21 03_batch.qmd\n-rw-rw---- 1 tladera2 g_tladera2    0 Oct  2 15:21 04_assignment.qmd\nIt’s this first part that we want to examine:\nfolder  everyone_else\n|       |     \n-rw-rw----\n |  |\n you group\nThe 2nd to 4th letters define your own access to the files. The\nrw-\nMeans that you have read and write access. If we had the following:\nrwx\nThat would mean we have read, write, and execute access.\nSimilarly, the 5th to 7th letters are the permissions for your group. In many cases at Fred Hutch, the group level maps to your lab or group.\nThe last 3 letters correspond to everyone else on the system. Think about whether you want to make your scripts accessible to everyone.\nIn this case, since the permissions are:\n---\nThat means that everyone else cannot read, write, or execute your files. Your files are locked down.\nThe cardinal rule to remember is that:\n\nIf you want to run a file as an executable, you (or your group) needs to have executable level permission.\n\nFor example, if I want to run a script called run_samtools.sh in my directory like this:\n./run_samtools.sh my_bam_file.bam\nI will need to have execute privileges at the user, group, or others level for run_samtools.sh.\nWe can change the permissions of our files using the chmod command.\n\n\n\n\n\n\nNoneHelpful unix permissions situations\n\n\n\nI tend to just go by memory when setting file permissions. These are the common situations that I’ve encountered that I use regularly:\n\n\n\n\n\n\n\nSituation\nCommand\n\n\n\n\nOnly I can execute/read/write a file\nchmod 700 &lt;filename&gt;\n\n\nOnly I and my group can read a file\nchmod 110 &lt;filename&gt;\n\n\nOnly I and my group can read/write/execute a file\nchmod 770 &lt;filename&gt;\n\n\nGrant my group read permissions\nchmod 710 &lt;filename&gt;\n\n\nMake executable/read/write by all\nchmod 777 &lt;filename&gt;\n\n\n\nFor right now, let’s set scripts to be read/write/executable by us:\nchmod -R 700 scripts/\nThe -R in this case is the recursive argument, which means apply chmod to everything within the scripts/ folder.\nThis will save us some headaches when we try to execute our scripts.\nIf you have different permission situations that you need to set, please refer to: https://www.redhat.com/en/blog/linux-file-permissions-explained\n\n\n\n\n\n\n\n\nNoneEven if you don’t have execute permissions\n\n\n\nWith bash scripts, you can still run them if you have read permissions. You can still run bash scripts by using the bash command:\nbash run_samtools.sh my_bam_file.bam\nThis is also the case for scripts that use she-bangs (Section 3.4) for R or Python or any other executable.\n\n\n\n1.10.1 Try it out\nWhat are the permissions for the GitHub repo (bash_for_bio) that you just downloaded?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Navigating the Bash Filesystem</span>"
    ]
  },
  {
    "objectID": "01_basics.html#sec-moving",
    "href": "01_basics.html#sec-moving",
    "title": "1  Navigating the Bash Filesystem",
    "section": "1.11 Moving Things Around",
    "text": "1.11 Moving Things Around\nA lot of the time, we need to move files between shared filesystems. One filesystem might be good at storage and be backed up on a regular basis, while another filesystem might be better for temporary work on the cluster.\nYou might be familiar with mv, which lets you move files around in Unix. One thing to keep in mind when you’re mving things to a new folder that there is a difference between:\nmv log.txt my_folder   ## renames log.txt to my_folder\nand\nmv log.txt my_folder/  ## moves log.txt to be in my_folder\nThis is one thing that still trips me up all the time.\nThis is one situation where using a GUI such as Motuz can be very helpful. You don’t have to worry about accidentally renaming files.\nOther tools for sync’ing between filesystems include rsync, which requires careful reading of documentation.\n\n\n\n\n\n\nNoneThings I always forget: the difference between /home/mydir/ and home/mydir/\n\n\n\nSome things that trip me up all the time. The difference between\n/home/mydir/    #absolute path\nand\nhome/mydir/     #relative path\nThe first one is an absolute path, and the second is a relative path. Your clue is the leading / at the beginning of a path. If you’re getting file not found messages, check to make sure the path is the right format.\n\n\n\n1.11.1 Keep Everything in Folders\nWe need to talk about code and data organization. For the FH system, we have a /home/ directory, and if we have generated research data, a /fh/fast/ directory. If we want our scripts to live in /home/ and our data is in /hpc/temp/, we’ll need to refer to each of these file locations.\nIdeally, we want to make the naming conventions of our code and our data as similar as possible.\n\n\n\n\n\n\nNoteTry it Out\n\n\n\nCopy the script tell_the_time.sh in the scripts/week1/ directory to the top directory of bash_for_bio.\nMake the script executable.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Navigating the Bash Filesystem</span>"
    ]
  },
  {
    "objectID": "01_basics.html#whats-in-the-script",
    "href": "01_basics.html#whats-in-the-script",
    "title": "1  Navigating the Bash Filesystem",
    "section": "1.12 What’s in the script",
    "text": "1.12 What’s in the script\nWe can see what’s in the script by using cat:\ncat tell_the_time.sh\nAnd you’ll get the following:\n#!/bin/bash\ndate",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Navigating the Bash Filesystem</span>"
    ]
  },
  {
    "objectID": "01_basics.html#running-a-bash-script",
    "href": "01_basics.html#running-a-bash-script",
    "title": "1  Navigating the Bash Filesystem",
    "section": "1.13 Running a Bash Script",
    "text": "1.13 Running a Bash Script\nOk, now we have a bash script tell_the_time.sh in our current directory, how do we run it?\nBecause the script is not on our $PATH (Section 11.5.2), we’ll need to use ./ to execute it. ./ is an alias for the current folder, and it is an indicator to bash that the command we want to execute is in our current folder.\ntladera2$ ./tell_the_time.sh\nIf we haven’t set the permissions (Section 1.10) correctly, we’ll get this message:\nbash: ./scripts/tell_the_time.sh: Permission denied\nBut if we have execute access, we’ll get something like this:\nFri Jul 11 13:27:47 PDT 2025\nWhich is the current date and time.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Navigating the Bash Filesystem</span>"
    ]
  },
  {
    "objectID": "01_basics.html#editing-on-a-linux-machine",
    "href": "01_basics.html#editing-on-a-linux-machine",
    "title": "1  Navigating the Bash Filesystem",
    "section": "1.14 Editing on a Linux Machine",
    "text": "1.14 Editing on a Linux Machine\nOn the rhino machines, we have the option to use the nano editor. nano is the most like a word processor or code editors.\n\nOpen a file in nano: nano &lt;filename&gt;\nSave and quit: &lt;CTRL&gt; + x and then yes\nNavigate in file: using the arrow keys will work\nFind in file: &lt;CTRL&gt; + w\nCopy from outside the terminal (dependent on terminal program)\n\nPasting will depend on your terminal program. On macs it is Command-V, and on PuTTY it is using Right Click on the mouse\n\n1.14.1 Try it Out\nTry making your own file called my_file.txt:\nnano my_file.txt\nAdd some text to it.\nUse CTRL-X to exit, and make sure to select “Yes” to save.\n\n\n1.14.2 Editing on your own machine\nOne thing you can do to make your life easier is to edit scripts with an editor on your own machine that is connected to rhino through Samba. You will able to open up files you create in the editor of you choice on your system, and when you save the file, the changes will be make on rhino.\nInstructions are here: https://sciwiki.fredhutch.org/scicomputing/store_posix/#how-to-access-fred-hutch-storage and follow the instructions for mapping your network drive (Mac) and (Windows).\nYou will want to use the connection strings specified for home.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Navigating the Bash Filesystem</span>"
    ]
  },
  {
    "objectID": "01_basics.html#sec-rpy",
    "href": "01_basics.html#sec-rpy",
    "title": "1  Navigating the Bash Filesystem",
    "section": "1.15 Running an R or Python Script on the command line",
    "text": "1.15 Running an R or Python Script on the command line\n\n1.15.1 Loading the fhR or fhPython modules\nBefore we can run our scripts in R or Python, we’ll need to load up either R or Python on the cluster. We can do this with the module load command:\n1module load fhR\n2module load fhPython\n\n1\n\nLoad up fhR module - has R and most packages installed\n\n2\n\nLoad up fhPython module - has Python and most packages installed.\n\n\nWe’ll talk more about software modules next week (Section 3.5).\n\n\n1.15.2 R Users\nYou might not be aware that there are multiple ways to run R:\n\nas an interactive console, which is what we usually use in an IDE such as RStudio\non the command line using the Rscript command.\n\nRscript my_r_script.R\nTo run this script, we’ll need to first load fhR:\nmodule load fhR\nRscript my_r_script.R\nmodule purge\n\n\n1.15.3 Python Users\nPython users are much more aware that you can run Python scripts on the command line:\npython3 my_python_script.py\nTo execute this on gizmo, we’ll first need to load fhPython:\nmodule load fhPython\npython3 my_python_script.py\nmodule purge\nWithin a shell script, you can also use a shebang (Section 3.4) to make your script executable by providing the location of python3:\n#!/bin/python3\npython3 my_python_script.py",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Navigating the Bash Filesystem</span>"
    ]
  },
  {
    "objectID": "01_basics.html#getting-help",
    "href": "01_basics.html#getting-help",
    "title": "1  Navigating the Bash Filesystem",
    "section": "1.16 Getting Help",
    "text": "1.16 Getting Help\nYou may have heard about man pages. You can usually get help by using the man command:\nman wc\nThis is the first part of the output:\nNAME\n     wc – word, line, character, and byte count\n\nSYNOPSIS\n     wc [--libxo] [-Lclmw] [file ...]\n\nDESCRIPTION\n     The wc utility displays the number of lines, words, and bytes contained in each input file, or standard input (if no file is\n     specified) to the standard output.  A line is defined as a string of characters delimited by a ⟨newline⟩ character.\n     Characters beyond the final ⟨newline⟩ character will not be included in the line count.\n\n     A word is defined as a string of characters delimited by white space characters.  White space characters are the set of\n     characters for which the iswspace(3) function returns true.  If more than one input file is specified, a line of cumulative\n     counts for all the files is displayed on a separate line after the output for the last file.\n\n     The following options are available:\n\n         --libxo\n             Generate output via libxo(3) in a selection of different human and machine readable formats.  See xo_parse_args(3)\n             for details on command line arguments.\n\n     -L      Write the length of the line containing the most bytes (default) or characters (when -m is provided) to standard\n             output.  When more than one file argument is specified, the longest input line of all files is reported as the value\n             of the final “total”.\n\nI personally find man pages very hard to read, especially when there are lots of options for a command.\nInstead, I use tldr, which contain examples of the most commonly used options in a command. It is not installed on gizmo, but you can use the page at https://tldr.inbrowser.app/, which has all of the tldr help pages.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Navigating the Bash Filesystem</span>"
    ]
  },
  {
    "objectID": "01_basics.html#recap",
    "href": "01_basics.html#recap",
    "title": "1  Navigating the Bash Filesystem",
    "section": "1.17 Recap",
    "text": "1.17 Recap\nWe learned the following this week:\n\nNavigate and copy data to the different filesystems available at Fred Hutch.\nExplain the difference between absolute and relative file paths.\nSet Permissions on and execute a bash script\nFind help on the system",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Navigating the Bash Filesystem</span>"
    ]
  },
  {
    "objectID": "01_basics.html#next-week",
    "href": "01_basics.html#next-week",
    "title": "1  Navigating the Bash Filesystem",
    "section": "1.18 Next Week",
    "text": "1.18 Next Week\nWe’ll focus on adding arguments to our scripts, and more about software modules",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Navigating the Bash Filesystem</span>"
    ]
  },
  {
    "objectID": "01_assignment.html",
    "href": "01_assignment.html",
    "title": "2  Week 1 Exercises",
    "section": "",
    "text": "2.1 In class code examples\nChange directories:\nTry out using du -d -h 2 on your home directory:\nAdd some text to it.\nUse CTRL-X to exit, and make sure to select “Yes” to save.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1 Exercises</span>"
    ]
  },
  {
    "objectID": "01_assignment.html#in-class-code-examples",
    "href": "01_assignment.html#in-class-code-examples",
    "title": "2  Week 1 Exercises",
    "section": "",
    "text": "Clone the bash_for_bio repository into your home directory:\n\ngit clone https://github.com/fhdsl/bash_for_bio\n\ncd bash_for_bio\n\nTry checking the disk usage using du for the bash_for_bio folder in your /home directory (mine is /home/tladera2/bash_for_bio).\n\ndu -d 1 -h /home/tladera2/bash_for_bio/\n\ndu -d 2 ~/\n\nWhat are the permissions for the GitHub repo (bash_for_bio) that you just downloaded?\nCopy the script tell_the_time.sh in the scripts/week1/ directory to the top directory of bash_for_bio. Make the script executable. Run the script.\n\n#put code here\n\nTry making your own file called my_file.txt:\n\nnano my_file.txt\n\n\n\nChange permissions for the scripts/ folder:\n\nchmod -R 700 scripts/",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1 Exercises</span>"
    ]
  },
  {
    "objectID": "01_assignment.html#homework-exercises",
    "href": "01_assignment.html#homework-exercises",
    "title": "2  Week 1 Exercises",
    "section": "2.2 Homework Exercises",
    "text": "2.2 Homework Exercises\nAll exercises are required for the badge.\n\nUse du to check the disk usage of two folders in your /bash_for_bio directory. Set the depth (-d) option to 1. Put your code in the box below\n\ndu -d 1 -h -------\n\nCopy a file from the scripts/week1/ directory in bash_for_bio/ to your own home directory.\n\n/home/tladera2/bash_for_bio/scripts/week1/run_this.sh\nCheck the permissions. If necessary, change the script to read/write/executable for yourself:\nchmod --- run_this.sh\nTry running it in your home directory - did it work?\n./run_this.sh\n\n(pick python or R) Take a look at scripts/week1/rnorm.R or scripts/week1/random_num.py. Load up the fhR or fhPython modules on rhino using module load. Run it on the command line with Rscript or python3.\n\nDid you need to make this script executable before you ran it?\nmodule load _____\n--------\nmodule purge",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1 Exercises</span>"
    ]
  },
  {
    "objectID": "02_scripting.html",
    "href": "02_scripting.html",
    "title": "3  Introduction to Scripting",
    "section": "",
    "text": "3.1 Exercises\nOpen up the exercises here.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Scripting</span>"
    ]
  },
  {
    "objectID": "02_scripting.html#what-were-working-towards",
    "href": "02_scripting.html#what-were-working-towards",
    "title": "3  Introduction to Scripting",
    "section": "3.2 What we’re working towards",
    "text": "3.2 What we’re working towards\nBy the end of this session, you should be able to understand and run this shell script.\n#| filename: ./scripts/week2/sam_count.sh\n1#!/bin/bash\n2module load SAMtools/1.19.2-GCC-13.2.0\n3samtools view -c $1 &gt; $1.counts.txt\n4module purge\n\n1\n\nSpecify the shell language (bash)\n\n2\n\nload the module\n\n3\n\nrun the script\n\n4\n\npurge the module.\n\n\nIt seems a little intimidating, but we will take this apart line by line.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Scripting</span>"
    ]
  },
  {
    "objectID": "02_scripting.html#motivation",
    "href": "02_scripting.html#motivation",
    "title": "3  Introduction to Scripting",
    "section": "3.3 Motivation",
    "text": "3.3 Motivation\nThere is a rule in programming: if you do something more than 3 times, you should consider making it into a script or function.\nFor example, imagine that you use samtools view -c all the time with certain options and you want to save the output. You can put this command and options into a shell script that takes named files as an argument (such as samcount.sh. Instead of typing samtools view -c over and over again, you can run\n./sam_count.sh my_file.bam\nand it will output a file my_file.bam.counts.txt that has the counts we are interested in.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Scripting</span>"
    ]
  },
  {
    "objectID": "02_scripting.html#sec-shebang",
    "href": "02_scripting.html#sec-shebang",
    "title": "3  Introduction to Scripting",
    "section": "3.4 The first line: the she-bang",
    "text": "3.4 The first line: the she-bang\nWhat’s this first line?\n#!/bin/bash\nthe #! is known as a she-bang - it’s a signal to Linux what language interpreter to use when running the script on the command line. In our case, we want to use bash. But we can also make python scripts executable by putting the path to python:\n#!/\nThe she-bang is necessary if you want to run the script without using the bash command (after you have made it executable):\n./samcount.sh chr1.sam\nNote that you can always use bash to execute a file if you don’t have execute access:\nbash samcount.sh chr1.sam",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Scripting</span>"
    ]
  },
  {
    "objectID": "02_scripting.html#sec-modules",
    "href": "02_scripting.html#sec-modules",
    "title": "3  Introduction to Scripting",
    "section": "3.5 Software Modules",
    "text": "3.5 Software Modules\nOk, we’ve gotten comfortable navigating around the HPC filesystem. Now how do we run executables on files?\nLet’s talk about the two problems:\n\nHow do we find executables on a cluster, and\nhow do we load them up and run them?\n\n\n3.5.1 Is my software already installed?\nSay we want to see if samtools is installed on our HPC. One of the key commands you can use to find software is the which command. If your software is installed, which will give you the path where the software is installed. For example, I can see if bash is installed:\nwhich bash\nWhich gives me the response:\n/bin/bash\nSo, let’s see if samtools is installed:\nwhich samtools\nWhich gives no response, so where is samtools?\nIf we don’t have samtools immediately available, how do we find it on our system? On the HPC system, We can use environment modules to load software.\n\n\n3.5.2 Environment Modules\nBefore you install your own versions of software, it’s important to realize that this problem may be solved for you.\nYour first stop should be looking for environment modules on the HPC. Not all HPCs have these, but if they have them, this should be your first stop to find executables.\nlmod is a system for loading and unloading software modules. It is usually installed on HPCs. The commands all start with module, and there are a number of ones that are useful for you.\n\nmodule avail\nmodule load\nmodule purge\n\nIf you want to see the current list of available modules and their names, check them out here.\nLooking for samtools on that page, we discovered the name of our module:\nSAMtools\nSo, that’s what we’ll use to load up samtools.\n\n\n3.5.3 module avail\nWe can find the versions of SAMtools available to us by using:\nmodule avail SAMtools\nAnd we’ll get a list of versions:\n------------------------------- /app/modules/all -------------------------------\n   SAMtools/1.10-GCC-8.3.0        SAMtools/1.16.1-GCC-11.3.0\n   SAMtools/1.10-GCCcore-8.3.0    SAMtools/1.17-GCC-12.2.0\n   SAMtools/1.11-GCC-10.2.0       SAMtools/1.18-GCC-12.3.0\n   SAMtools/1.14-GCC-11.2.0       SAMtools/1.19.2-GCC-13.2.0\n   SAMtools/1.15.1-GCC-11.2.0     SAMtools/1.21-GCC-13.3.0   (D)\n   SAMtools/1.16.1-GCC-11.2.0\nThe (D) is the default version. It is the version we use if we just use:\nmodule load SAMtools\nWithout a version. In general, it is good to fix the module version in your scripts for reproducibility reasons. The reason for that is that the default version may change in the future, and there may be changes that break your code.\n\n\n3.5.4 module load\nHere’s the next line of the script:\nmodule load SAMtools/1.19.2-GCC-13.2.0  #load the module\nOur module name is SAMtools, and the 1.19.2-GCC-13.2.0 after it is the version of that module.\n\n\n\n\n\n\nNoteFor FH Users: Modules benefit everyone\n\n\n\nIf there is a particular bit of software that you need to run on the FH cluster that’s not there, make sure to request it from SciComp. Someone else probably needs it and so making it known so they can add it as a Environment module will help other people.\n\n\n\n\n\n\n\n\nNoneFor FH Users\n\n\n\nOn the FH cluster, ml is a handy command that combines module load and module avail.\nYou can load a module with ml &lt;module_name&gt;.\n\n\n\n\n3.5.5 Tip: Load only as many modules as you need at a time\nOne of the big issues with bioinformatics software is that the toolchain (the software dependencies needed to run the software) can be different for different executables. This is also called dependency hell. So when possible, load only one or two modules at a time for each step of your analysis. When you’re done with that step, use module purge to clear out the software environment.\n\n\n3.5.6 For more information\nPlease refer to https://sciwiki.fredhutch.org/scicomputing/compute_environments/ for more information about the available modules.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Scripting</span>"
    ]
  },
  {
    "objectID": "02_scripting.html#sec-positional",
    "href": "02_scripting.html#sec-positional",
    "title": "3  Introduction to Scripting",
    "section": "3.6 $1: A Positional argument",
    "text": "3.6 $1: A Positional argument\nThe next line of our script is this:\nsamtools view -c $1 &gt; $1.counts.txt  \nLet’s take a look at the command that we’re running first. We’re going to run samtools view -c, which will give us counts on an incoming bam or sam file and save it in a file. We want to be able to run our script like this:\nbash sam_count.sh my_file.bam \nWhen we run it like that, sam_count.sh will run samtools view -c like this:\nsamtools view -c my_file.bam &gt; my_file.bam.counts.txt\nSo what’s going on here is that there is some substitution using common arguments. Let’s look at these.\n\n\n\n\n\n\nNone&gt; - redirecting outputs to a file\n\n\n\nThe &gt; in the script means that we are going to direct the output of samtools view -c into a file.\nIf we didn’t do this, samtools_count.sh would output everything to console.\nMuch more info about this when we talk about the different outputs to console (Section 3.12).\n\n\n\n3.6.1 Positional Arguments such as $1\nHow did the script know where to substitute each of our arguments? It has to do with the argument variables. Arguments (terms that follow our command) are indexed starting with the number 1. We can access the value at the first position using the special variable $1.\nNote that this works even in quotes.\nSo, to unpack our script, we are substituting our first argument for the $1.\nWhen we run this script, it will output into our current directory, which is the top level of the bash_for_bio folder. We can change the behavior of the script by doing the following:\nsamtools view -c $1 &gt; $2 \nWith the added complication that we need to specify a path for our counts file.\n\n\n\n\n\n\nNoteTest yourself\n\n\n\nHow would we rewrite sam_run.sh (shown below) if we wanted to specify the output file as the first argument and the bam file as the second argument?\n#!/bin/bash/\nsamtools view -c $1 &gt; $2\n\n\n\n\n\n\n\n\nNoteAnswer\n\n\n\n\n\nFor this script, we would switch the positions of $1 and $2.\n#!/bin/bash/\nsamtools view -c $2 &gt; $1\n\n\n\n\n\n3.6.2 For More Info\nPlease refer to Bite Size Bash page 9 for more info.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Scripting</span>"
    ]
  },
  {
    "objectID": "02_scripting.html#module-purge",
    "href": "02_scripting.html#module-purge",
    "title": "3  Introduction to Scripting",
    "section": "3.7 module purge",
    "text": "3.7 module purge\nThe last line of our script is:\nmodule purge\nThis line will unload the modules from memory. It’s good practice to unload modules when you’re done with them, especially since they have complex chains of dependencies, and the versions of these dependencies can interfere with other packages.\n\n\n\n\n\n\nNoneProcessing Files Best Practices\n\n\n\nOne thing to remember is to not touch the raw data. The original files should remain untouched. That is, reading the raw data files is fine, but avoid overwriting the orignal data files.\nA good way to do this is to have your outputs saved in a different folder.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Scripting</span>"
    ]
  },
  {
    "objectID": "02_scripting.html#shellcheck",
    "href": "02_scripting.html#shellcheck",
    "title": "3  Introduction to Scripting",
    "section": "3.8 shellcheck",
    "text": "3.8 shellcheck\nWhen you are writing shell scripts, shellcheck is a really handy way to check that you have syntax correctly.\n1#/bin/bash\necho \"blah blah\"\n\n1\n\nA mistake! should be #!/bin/bash.\n\n\nIf we run shellcheck on this script, it will catch the mistake:\nml shellcheck   #load the shellcheck module\nshellcheck scripts/week2/bad_script.sh \nshellcheck will give the following response:\nIn bad_script.sh line 1:\n#/bin/bash\n ^-- SC1113 (error): Use #!, not just #, for the shebang.\nshellcheck is available on rhino via ml, you can also install the VSCode Extension if you are editing in VSCode.\nFor more info about shellcheck, refer to Bite Sized Bash, page 6.\n\n\n\n\n\n\nNoteWhen you have spaces in file names: escaping characters or double quotes\n\n\n\nIn general, I try not to use spaces in my file names. They can be unwieldy.\nThe general solution is to use double quotes or an escape character \\\nFor example, if my file name is CALU final.fasta\nI can refer to it as:\n./scripts/week2/run_bwa.sh \"CALU final.fasta\"\nThe escape character (\\) is a way to put spaces in a bash command without bash interpreting everything after the space as an argument. Essentially, it tells bash to literally interpret the character that follows it.\n./scripts/week2/run_bwa.sh CALU\\ final.fasta\nI prefer to use double quotes. You’ll notice that all file names in this course use an underscore (_) or dashes (-) instead of spaces, which makes them easier to handle.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Scripting</span>"
    ]
  },
  {
    "objectID": "02_scripting.html#sec-bash-variables",
    "href": "02_scripting.html#sec-bash-variables",
    "title": "3  Introduction to Scripting",
    "section": "3.9 Variables in Bash Scripts",
    "text": "3.9 Variables in Bash Scripts\nWe saw a little bit about using $1, which is a variable in our Bash scripts. Let’s talk about declaring variables in bash scripts and using them using variable expansion.\nIn Bash, we can declare a variable by using &lt;variable_name&gt;=&lt;value&gt;. Note there are no spaces between the variable (my_variable), equals sign, and the value (\"ggplot2\").\nmy_variable=\"ggplot2\"\n\necho \"My favorite R package is ${my_variable}\"\nMy favorite R package is ggplot2\nTake a look at line 3 above. We expand the variable (that is, we substitute the actual variable) by using ${my_variable} in our echo statement.\nIn general, when expanding a variable in a quoted string, it is better to use ${my_variable} (the variable name in curly brackets). This is especially important when using the variable name as part of a string:\nmy_var=\"chr1\"\necho \"${my_var}_1.vcf.gz\"\nchr1_1.vcf.gz\nIf we didn’t use the braces here, like this:\necho \"$my_var_1.vcf.gz\"\nBash would look for the variable $my_var_1, which doesn’t exist. So use the curly braces {} when you expand variables. It’s safer overall.\n\n\n\n\n\n\nNoteSubshells\n\n\n\nThere is an alternate method for variable expansion which we will use when we call a sub-shell - a shell within a shell. We need to use parentheses () to expand these commands within the sub-shell, but not the top-shell. We’ll use this when we process multiple files within a single worker.\n\n\n\n3.9.1 For more information\nPlease refer to Bite Size Bash page 7 for more information.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Scripting</span>"
    ]
  },
  {
    "objectID": "02_scripting.html#putting-it-all-together",
    "href": "02_scripting.html#putting-it-all-together",
    "title": "3  Introduction to Scripting",
    "section": "3.10 Putting it all together",
    "text": "3.10 Putting it all together\nWe can run the script on a file (assuming we are in bash_for_bio/):\n./scripts/week2/sam_count.sh ./data/my_file.bam\n\n3.10.1 Try it Out\nTry out the above script on ./data/CALU1_combined_final.sam\nMake sure you are in the top folder (bash_for_bio) when you do it.\n\n\n\n\n\n\nNoteWhen should you hard code a path?\n\n\n\nIn general, hard coding absolute paths (such as /home/laderast/bash_for_bio/scripts/week1/run_this.sh) makes your code less reproducible. Using a project-based workflow (where your paths mostly refer within a single folder) makes it more reproducible.\nOf course, there are cases (such as hard coding a genome reference) which should use absolute paths, since the location of these differs across systems. Just make a note when sharing which paths are hard-coded.\nI talk about the benefits of project based workflows here: (Section 12.10).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Scripting</span>"
    ]
  },
  {
    "objectID": "02_scripting.html#aligning-a-file",
    "href": "02_scripting.html#aligning-a-file",
    "title": "3  Introduction to Scripting",
    "section": "3.11 Aligning a file",
    "text": "3.11 Aligning a file\nHow about running the bwa-mem aligner? Let’s run a FASTQ file into it using ${1}\n#| filename: scripts/week2/run_bwa.sh\n#!/bin/bash\n1module load BWA/0.7.17-GCCcore-11.2.0\n2input_fastq=${1}\n# strip path and suffix\nbase_file_name=\"${input_fastq%.fastq}\"\nbase_file_name=${base_file_name##*/}\necho \"running $input_fastq\"\nsample_name=\"SM:${base_file_name}\"\nread_group_id=\"ID:${base_file_name}\"\nplatform_info=\"PL:Illumina\"\nref_fasta_local=\"/shared/biodata/reference/iGenomes/Homo_sapiens/UCSC/hg19/Sequence/BWAIndex/genome.fa\" &lt;2&gt;\n\n3bwa mem \\\n      -p -v 3 -M \\\n      -R \"@RG\\t${read_group_id}\\t${sample_name}\\t${platform_info}\" \\\n      \"${ref_fasta_local}\" \"${input_fastq}\" &gt; \\\n      \"${base_file_name}.sam\"\nmodule purge\n\n1\n\nLoad BWA module. Note that we are using a specific module version.\n\n2\n\nSet a variable called $ref_fasta_local, using the location of the hg19 reference.\n\n3\n\nRun bwa mem on the $input_fastq file, using the $ref_fasta_local variable as the location of the reference genome, and output to ${base_file_name}.sam.\n\n\n\n3.11.1 Try it Out\nWe can run this script using:\n./scripts/week2/run_bwa.sh ./data/CALU1_combined_final.fasta\nWhere does the output file end up?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Scripting</span>"
    ]
  },
  {
    "objectID": "02_scripting.html#sec-std",
    "href": "02_scripting.html#sec-std",
    "title": "3  Introduction to Scripting",
    "section": "3.12 Using pipes: STDIN, STDOUT, STDERR",
    "text": "3.12 Using pipes: STDIN, STDOUT, STDERR\nWe will need to use pipes to chain our commands together. Specifically, we need to take a command that generates a list of files on the cluster shared filesystem, and then spawns individual jobs to process each file. For this reason, understanding a little bit more about how pipes (|) work in Bash is helpful.\nIf we want to understand how to chain our scripts together into a pipeline, it is helpful to know about the different streams that are available to the utilities.\n\n\n\n\n\n\n\n\ngraph LR\n  A(STDIN) --&gt; E[run_samtools.sh]\n  E --&gt; B(STDOUT)\n  E --&gt; C(STDERR)\n\n\n\n\n\n\n\n\nFigure 3.1: Inputs/outputs to a script\n\n\n\nEvery script has three streams available to it: Standard In (STDIN), Standard Out (STDOUT), and Standard Error (STDERR) (Figure 12.1).\nSTDIN contains information that is directed to the input of a script (usually text output via STDOUT from another script). This includes arguments we pass onto the script.\nWhy do these matter? To work in a Unix pipeline, a script must be able to utilize STDIN, and generate STDOUT, and STDERR.\nSpecifically, in pipelines, STDOUT of a script (here it’s run_bwa.sh) is directed into STDIN of another command (here wc, or word count)\n\n\n\n\n\n\n\n\ngraph LR\n  E[run_bwa.sh] --&gt; B(STDOUT)\n  B --&gt; F{\"|\"}\n  E --&gt; C(STDERR)\n  F --&gt; D(\"STDIN (wc)\")\n  D --&gt; G[wc]\n\n\n\n\n\n\n\n\nFigure 3.2: Piping a script run_samtools.sh into another command (wc)\n\n\n\nWe will mostly use STDOUT in our bash scripts, but STDERR can be really helpful in debugging what’s going wrong.\n\n3.12.1 &gt; (redirects)\nSometimes you want to direct the output of a script (STDOUT) to a text file. A lot of bioinformatics tools output to STDOUT, and we need a way to save the results, or pass the results onto another program.\nEnter &gt;, a redirect. We can put &gt; after our command followed by a file path to save the output.\nsamtools view -c my_bam_file.bam &gt; counts.txt\nHere we are redirecting the output of samtools view into the counts.txt file. Note that everytime that we run the script, we will overwrite the current contents of the counts.txt file. Sometimes that is not what you want.\nThere is another kind of redirect, &gt;&gt;, that will append (that is, add to the end of a file). If the file does not exist, it will be created. But if it does exist, the output will be added to the end of the file. I rarely use this, however.\nMuch more information about redirects can be found here: https://www.geeksforgeeks.org/linux-unix/input-output-redirection-in-linux/ and in Bite Sized Bash (page 13).\n\n\n\n\n\n\nNoteWhy this is important on the Cluster\n\n\n\nWe’ll use pipes and pipelines not only in starting a bunch of jobs using batch scripting on our home computer, but also when we are processing files within a job.\nPipes are at the heart of multi-stage workflows. They allow us to specify multiple steps in processing a file.\n\n\n\n\n3.12.2 For more info about pipes and pipelines\n\nBite Size Bash Page 13 for more about redirects\nBite Size Bash Page 20 for more about pipes\nhttps://swcarpentry.github.io/shell-novice/04-pipefilter/index.html\nhttps://datascienceatthecommandline.com/2e/chapter-2-getting-started.html?q=stdin#combining-command-line-tools",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Scripting</span>"
    ]
  },
  {
    "objectID": "02_scripting.html#arguments-in-r-scripts",
    "href": "02_scripting.html#arguments-in-r-scripts",
    "title": "3  Introduction to Scripting",
    "section": "3.13 Arguments in R Scripts",
    "text": "3.13 Arguments in R Scripts\nTo make our R Script more useful, it must also accept arguments. Let’s explore how to achieve this.\nAs we learned in (Section 1.15), We can use Rscript to execute our command non-interactively. Rscript, which is the command line executable, will run our Rs cript on the command line.\nNote that we have a named argument called input_file and it is done differently than in Bash - how do we use this in our R Script?\n\n3.13.1 Using Named Arguments in an R script\nWe can pass arguments from our bash script to our R script by using commandArgs() - this will populate a list of named arguments (such as input_file) that are passed into the R Script. We assign the output of commandArgs() into the args object.\nWe refer to our input_file argument as args$input_file in our script.\n\n\n\nscripts/week2/process_data.R\n\nlibrary(tidyverse)\n\nargs &lt;- commandArgs()\n\n# Use arg$CSVFILE in read.csv\n1csv_file &lt;- read.csv(file=args$input_file)\n\n# Do some work with csv_file\ncsv_filtered &lt;- csv_file |&gt; janitor::clean_names()\n\n# Write output\n2write.csv(csv_filtered, file = paste0(args$CSVFILE, \"_cleaned.csv\"))\n\n\n\n1\n\nUse args$input_file as an input for read.csv()\n\n2\n\nUse args$input_file to specify the output name.\n\n\n\n\n\n\n3.13.2 Try it out: Running our R Script\nNow that we’ve set it up, we can run the R script from the command line as follows:\n\nmodule load fhR\nRscript process_data.R input_file=\"LUSC_clinical.csv\"\nmodule purge\n\n\n\n3.13.3 Integrating an R script into a Bash Script\nSay you have an R Script you need to run on the command line in combination with other unix commands. In our bash script, we can do the following:\n\n\n\n./scripts/week2/wrap_r_script.sh\n\n#!/bin/bash\nmodule load fhR\nRscript process_data.R input_file=\"${1}\"\nmodule purge\n\n\nThen we can run\n\nbash wrap_r_script.sh .data/PRAD.csv \n\nIn our bash script, my_bash_script.sh, we’re using a positional argument (for simplicity) to specify our csvfile, and then passing the positional argument to named ones (input_file) for my_r_script.R.\n\n\n3.13.4 Summary\n\nR scripts will accept named arguments when called by Rscript:\n\nRscript process_csv.R input_file=\"my_csvfile.csv\"",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Scripting</span>"
    ]
  },
  {
    "objectID": "02_scripting.html#arguments-in-python-scripts",
    "href": "02_scripting.html#arguments-in-python-scripts",
    "title": "3  Introduction to Scripting",
    "section": "3.14 Arguments in Python Scripts",
    "text": "3.14 Arguments in Python Scripts\nSimilarly, we will need Python scripts to accept arguments as well.\n\n3.14.1 Using sys.argv\nThe sys module (built into Python) will let us access arguments by position:\n[argparse]\n\n\n\nprocess_file.py\n\n1import sys\nimport pandas as pd\n2file = sys.argv[1]\ndf = pd.read_csv(file)\n\n\n\n1\n\nImport sys module\n\n2\n\nUse sys.argv to access the first argument sys.argv[1]\n\n\n\n\nIf you know a little bit about Python, you’ll know that arrays in python are zero-indexed, that is, they start at [0], but we have started here at sys.argv[1] - what is at sys.argv[0]?\nIt turns out that sys.argv[0] is the name of the script. In the above script process_file.py, it contains the string \"process_file.py\". Having the name of the script can come in handy, especially when outputting things.\n\n\n3.14.2 Running our Python script on the command line\nNow we have set up our python script, we can run it on the command line on rhino:\n\nmodule load fhPython\npython3 process_file.py lusc_file.csv\nmodule purge\n\nAnd we could wrap it up in a bash script:\n\n\n\n./scripts/week2/wrap_py_script.sh\n\n#!/bin/bash\nmodule load fhPython\npython3 process_file.py ${1}\nmodule purge",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Scripting</span>"
    ]
  },
  {
    "objectID": "02_scripting.html#next-time",
    "href": "02_scripting.html#next-time",
    "title": "3  Introduction to Scripting",
    "section": "3.15 Next Time",
    "text": "3.15 Next Time\nWe’ll work on batch processing, both on a local system and the HPC cluster. Make sure to read the HPC Basic chapter before class.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Scripting</span>"
    ]
  },
  {
    "objectID": "02_assignment.html",
    "href": "02_assignment.html",
    "title": "4  Week 2 Exercises",
    "section": "",
    "text": "4.1 In-class exercises\nTry out the above script on ./data/CALU1_combined_final.sam.\nMake sure you are in the top folder (bash_for_bio) when you do it.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 2 Exercises</span>"
    ]
  },
  {
    "objectID": "02_assignment.html#in-class-exercises",
    "href": "02_assignment.html#in-class-exercises",
    "title": "4  Week 2 Exercises",
    "section": "",
    "text": "We can run the script on a file (assuming we are in bash_for_bio/):\n\n./scripts/week2/sam_count.sh ./data/my_file.bam\n\n\n\nRun run_bwa.sh using:\n\n./scripts/week2/run_bwa.sh ./data/CALU1_combined_final.fasta\n\nRun process_data.R:\n\nmodule load fhR\nRscript process_data.R input_file=\"LUSC_clinical.csv\"\nmodule purge\n\nRun process_data.py:\n\nmodule load fhPython\npython3 process_file.py lusc_file.csv\nmodule purge",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 2 Exercises</span>"
    ]
  },
  {
    "objectID": "02_assignment.html#homework-exercises",
    "href": "02_assignment.html#homework-exercises",
    "title": "4  Week 2 Exercises",
    "section": "4.2 Homework Exercises",
    "text": "4.2 Homework Exercises\nAll exercises are required for the badge.\n\nCopy the below script into a file called samtools_count.sh. What does the script do?\n\n#!/bin/bash\nmodule load SAMtools\nsamtools view -c $1 \nHow would we modify the above script to redirect the output to ${1}.counts.txt?\n\n\nMake sure you can get scripts/week2/run_bwa.sh to run on rhino.\nModify scripts/week2/run_bwa.sh to\n\nTake an additional argument, a folder path\nSave the SAM file to this folder path\n\n\nHint 1: I recommend that you copy run_bwa.sh into a new script and work from there. Put your code into the codeblock below:\n\nWrite an example to run your new version of the script below and run it:\n\nFor question 4, pick one language to answer.\n4R. (R) Modify the below R script and save it to a file called scripts/week2/r_csv_script.R. It should also take an argument called $FILEPATH for read.csv():\nlibrary(tidyverse)\ncsv_file &lt;- read_csv(\"myfile.csv\")\nsummary(csv_file)\nHow would you run this on the command line?\nmodule load fhR\n-----\nmodule purge\nHow would you redirect the output of your script to a file?\n4Py. (py) Modify the below Python script to be runnable and save it to scripts/week2/py_csv_script.py. Your new version should also take the first position argument (a file path) and process the file:\n#| eval: false\nimport pandas as pd\ncsv_file = pd.read_csv(\"my_file.csv\")\ncsv_file.describe()\nHow would you run this on the command line?\nmodule load fhPython\n-----\nmodule purge",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 2 Exercises</span>"
    ]
  },
  {
    "objectID": "hpc-basics.html",
    "href": "hpc-basics.html",
    "title": "5  Reading: HPC Basics",
    "section": "",
    "text": "5.1 Learning Objectives\nWe all need to start somewhere when we work with High Performance Computing (HPC).\nThis chapter is a review of how HPC works and the basic concepts. If you haven’t used HPC before, no worries! This chapter will get you up to speed.\nAfter reading this chapter, you should be able to:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reading: HPC Basics</span>"
    ]
  },
  {
    "objectID": "hpc-basics.html#learning-objectives",
    "href": "hpc-basics.html#learning-objectives",
    "title": "5  Reading: HPC Basics",
    "section": "",
    "text": "Define key players in both local computing and HPC\nArticulate key differences between local computing and HPC\nDescribe the sequence of events in launching jobs in the HPC cluster\nDifferentiate local storage from shared storage and articulate the advantages of shared storage.\nDescribe the differences between a single job and a batch job.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reading: HPC Basics</span>"
    ]
  },
  {
    "objectID": "hpc-basics.html#important-terminology",
    "href": "hpc-basics.html#important-terminology",
    "title": "5  Reading: HPC Basics",
    "section": "5.2 Important Terminology",
    "text": "5.2 Important Terminology\nLet’s establish the terminology we need to talk about HPC computing.\n\nHigh Performance Computing - A type of computing that uses higher spec machines, or multiple machines that are joined together in a cluster. These machines can either be on-premise (also called on-prem), or in the cloud (such as Amazon EC machines, or Azure Batch).\nCluster - a group of machines networked such that users can use one or more machines at once.\nAllocation - a temporary set of one or more computers requested from a cluster.\nToolchain - a piece of software and its dependencies needed to build a tool on a computer. For example, cromwell (a workflow runner), and java.\nSoftware Environment - everything needed to run a piece of software on a brand new computer. For example, this would include installing tidyverse, but also all of its dependencies (R) as well. A toolchain is similar, but might not contain all system dependencies.\nExecutable - software that is available on the HPC.\nShared Filesystem - Part of the HPC that stores our files and other objects. We’ll see that these other objects include applets, databases, and other object types.\nSLURM - The workload manager of the HPC. Commands in SLURM such as srun, (see Section 5.4) kick off the processes on executing jobs on the worker nodes.\nInteractive Analysis - Any analysis that requires interactive input from the user. Using RStudio and JupyterLab are two examples of interactive analysis. As opposed to non-interactive analysis, which is done via scripts.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reading: HPC Basics</span>"
    ]
  },
  {
    "objectID": "hpc-basics.html#understanding-the-key-players",
    "href": "hpc-basics.html#understanding-the-key-players",
    "title": "5  Reading: HPC Basics",
    "section": "5.3 Understanding the key players",
    "text": "5.3 Understanding the key players\nIn order to understand what’s going on with HPC, we will have to change our mental model of computing.\nLet’s contrast the key players in local computing with the key players in HPC.\n\n5.3.1 Key Players in Local Computing\n\n\n\n\n\n\nFigure 5.1: Local Computing\n\n\n\n\nOur Machine\n\nWhen we run an analysis or process files on our computer, we are in control of all aspects of our computer. We are able to install a software environment, such as R or Python, and then execute scripts/notebooks that reside on our computer on data that’s on our computer.\nOur main point of access to either the HPC cluster is going to be our computer.\n\n\n5.3.2 Key Players in HPC\nLet’s contrast our view of local computing with the key players in the HPC cluster (Figure 5.2).\n\n\n\n\n\n\nFigure 5.2: Key Players in HPC\n\n\n\n\nOur Machine - We interact with the HPC via the terminal installed on our machine. When we utilize HPC resources, we request them from our own computer using commands from the dx toolkit.\nHead Node - The “boss” of the cluster. It keeps tract of which worker nodes is doing what, and which nodes are available for allocations. Our request gets sent to the HPC cluster, and given availability, it will grant access to temporary worker.\nWorker Node - A temporary machine that comes from a pool of available machines in the cluster. We’ll see that it starts out as a blank slate, and we need to establish a software environment to run things on a worker.\nShared Filesystem A distributed filesystem that can be seen by all of the nodes in the cluster. Our scripts and data live here.\n\n\n\n5.3.3 Further Reading\n\nWorking on a remote HPC system is also a good overview of the different parts of HPC.\n\n\n\n\n\n\n\nNoteFor Fred Hutch Users\n\n\n\nThe gizmo cluster at Fred Hutch actually has 3 head nodes called rhino (rhino01, rhino02, rhino03) that are high spec machines (70+ cores, lots of memory). You can run jobs on these nodes, but be aware that others may be running jobs here as well.\nThe worker nodes on the gizmo cluster all have names like gizmoj6, depending on their architecture. You can request certain kinds of nodes in an allocation in several ways:\n\nWhen you use grabnode (Section 10.1.1)\nIn your request when you run srun or sbatch\nIn your WDL or Nextflow Workflow.\n\n\n\n\n\n\n\n\n\nNoneFH Users: Launching Jobs on gizmo\n\n\n\nIn a SLURM cluster like gizmo, SLURM (Simple Linux Utility for Resource Management) is software that runs on the head node that manages jobs on individual nodes.\nThe two main mechanisms for running SLURM jobs are srun (used for single jobs) and sbatch (used for multiple related jobs, such as aligning a set of FASTA files).\nWhen scaling to a larger number of files, we do recommend that you use a workflow manager such as Cromwell, or PROOF, or Nextflow for batching files.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reading: HPC Basics</span>"
    ]
  },
  {
    "objectID": "hpc-basics.html#sec-srun",
    "href": "hpc-basics.html#sec-srun",
    "title": "5  Reading: HPC Basics",
    "section": "5.4 Sequence of Events of Running a Job",
    "text": "5.4 Sequence of Events of Running a Job\nLet’s run through the order of operations of running a job on the HPC cluster. Let’s focus on running an aligner (BWA-MEM) on a FASTQ file. Our output will be a .BAM (aligned reads) file.\nLet’s go over the order of operations needed to execute our job on the HPC cluster (Figure 5.3).\n\n\n\n\n\n\nFigure 5.3: Order of Operations\n\n\n\nA. Start a job using srun to send a request to the cluster. In order to start a job, we will need two things: software (samtools), and a file to process from the shared filesystem (not shown). When we use srun, a request is sent to the cluster.\nB. Head node requests for a worker from available workers; worker made available on cluster. In this step, the head node looks for a set of workers that can meet our needs. Then the computations run on the worker; output files are generated.** Once our app is ready and our file is transferred, we can run the computation on the worker.\nC. Output files transferred back to project storage. Any files that we generate during our computation (53525342.bam) must be transferred back into the shared filesystem.\nWhen you are working with an HPC cluster, especially with batch jobs, keep in mind this order of execution. Being familiar with how the key players interact on the cluster is key to running efficient jobs.\n\n5.4.1 Key Differences with local computing\nAs you might have surmised, running a job on the HPC cluster is very different from computing on your local computer. Here are a few key differences:\n\nWe don’t own the worker machine, we only have temporary access to it. A lot of the complications of running HPC computations comes from this.\nWe have to be explicit about what kind of machine we want. We’ll talk much more about this in terms of machine types and classifieds.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reading: HPC Basics</span>"
    ]
  },
  {
    "objectID": "hpc-basics.html#the-shared-filesystem",
    "href": "hpc-basics.html#the-shared-filesystem",
    "title": "5  Reading: HPC Basics",
    "section": "5.5 The Shared Filesystem",
    "text": "5.5 The Shared Filesystem\nClusters often have a shared filesystem to make things easier. These filesystems can be accessed by all the nodes in a cluster and are designed for fast file transfers and reading. One example of a filesystem is Lustre.\nThink about that: it’s like having an SSD attached to all of the nodes. But how does the shared storage work?\nThe filesystem is distributed such that each set of nodes has a relatively fast access to the files on the system. The data itself is sharded, or broken up, and distributed among the storage servers that provide access to the files.\n\n\n\n\n\n\nNoteFor FH Users\n\n\n\nThere are three main filesystems you will probably use:\n\n/home/ - usually where your scripts will live\n/fh/fast/ - Where data lives. You will usually transfer data files over to /hpc/temp/ and when you generate results, transfer them from /hpc/temp/ back to /fh/fast/\n/hpc/temp/ - A temporary filesystem. Don’t store files here long term - mostly use this as a faster system to do computations on.\n\n\n\n\n5.5.1 Further Reading\n\nTransferring Files is a nice overview of the ways to transfer files to and from a remote system.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reading: HPC Basics</span>"
    ]
  },
  {
    "objectID": "hpc-basics.html#requesting-machines",
    "href": "hpc-basics.html#requesting-machines",
    "title": "5  Reading: HPC Basics",
    "section": "5.6 Requesting Machines",
    "text": "5.6 Requesting Machines\nHow do you request a set of machines on the HPC? There are multiple ways to do so:\n\nOpen an Interactive shell on a Node\nAs part of a job using srun or sbatch\n\nIn general, we recommend trying to use SLURM as much as possible with srun or sbatch. The larger recommendation overall is that we encourage you to use a workflow runner such as PROOF or Nextflow.\n\n5.6.1 Scattering: Distribute the Work\n\n\n\n\n\n\nFigure 5.4: The scattering process\n\n\n\nSo far, everything we’ve seen so far can be run on a single computer. In the cluster, we have access to higher spec’ed machines, but using the cluster in this way doesn’t take advantage of the efficiency of distributed computing, or dividing the work up among multiple worker nodes.\nWe can see an example of this in Figure 5.4. In distributed computing, we break our job up into smaller parts. One of the easiest way to do this is to split up a list of files (file1.bam, file2.bam, file3.bam) that we need to process, process each file separately on a different node, and then bring the results back together. Each node is only doing part of the work, but because we have multiple nodes, it is getting done 3 times faster.\nYou can orchestrate this process yourself with tools such as sbatch, but it is usually much easier to utilize workflow runners such as Cromwell/PROOF (for .wdl files) or Nextflow (for .nf files), because they automatically handle saving the results of intermediate steps.\nTrust me, it is a little more of a learning curve to learn Cromwell or Nextflow, but once you know more about it, the automatic file management and node management makes it much easier in the long run.\n\n\n\n\n\n\nNoneNodes versus CPUs\n\n\n\nOne thing that confused me was understanding the difference between requesting a system with multiple cores versus requesting multiple nodes.\nCores roughly correspond to processors, so a 24-core allocation is a single node that has 24 CPUs.\nNodes correspond to machines - so a 24 node allocation is 24 machines.\nThe reason why this is important is that you use them differently - we use scatter to utilize a 24 node allocation, whereas we can use multicore packages such as {parallel} and mcapply() to utilize a multi-core system.\nIn general, scatter over multiple nodes is handled by sbatch or your workflow runner.\n\n\n\n\n\n\n\n\nNoteFor FH Users: Running Workflows\n\n\n\nAt Fred Hutch, we have two main ways to run workflows on gizmo: Cromwell/MiniWDL and NextFlow. Cromwell users have a nifty GUI to run their workflows called PROOF.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reading: HPC Basics</span>"
    ]
  },
  {
    "objectID": "hpc-basics.html#whats-next",
    "href": "hpc-basics.html#whats-next",
    "title": "5  Reading: HPC Basics",
    "section": "5.7 What’s Next?",
    "text": "5.7 What’s Next?\nWe’ll leverage this knowledge to actually start batching on the gizmo cluster.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reading: HPC Basics</span>"
    ]
  },
  {
    "objectID": "03_batch.html",
    "href": "03_batch.html",
    "title": "6  Batch Processing and Submitting Jobs",
    "section": "",
    "text": "6.1 Exercises\nOpen up the exercises here.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Batch Processing and Submitting Jobs</span>"
    ]
  },
  {
    "objectID": "03_batch.html#learning-objectives",
    "href": "03_batch.html#learning-objectives",
    "title": "6  Batch Processing and Submitting Jobs",
    "section": "6.2 Learning Objectives",
    "text": "6.2 Learning Objectives\n\nExecute a script to run over a list of files on one system\nUtilize globs to specify multiple files in your script\nBatch Process files to run locally and on the HPC cluster",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Batch Processing and Submitting Jobs</span>"
    ]
  },
  {
    "objectID": "03_batch.html#using-for-loops-to-cycle-through-files",
    "href": "03_batch.html#using-for-loops-to-cycle-through-files",
    "title": "6  Batch Processing and Submitting Jobs",
    "section": "6.3 Using for loops to cycle through files",
    "text": "6.3 Using for loops to cycle through files\nA very common pattern is cycling through multiple files in a folder and applying the same script or command to them.\nThere is a simple method for batch processing a bunch of files: a for loop. In our case, a for loop takes a list of file paths (such as a list of FASTA files we want to process), and performs the same task for each element of the list.\n1for file in 01_assignment.qmd 02_scripting.qmd\n2do\n3  wc $file\n4done\n\n1\n\nCycle through the list of 01_assignment.qmd and 02_scripting.qmd.\n\n2\n\nStart of instructions\n\n3\n\nCount the words in each .qmd file using wc\n\n4\n\nEnd of instructions\n\n\nThe do and done sandwich the instructions we want to do to each file in our list. wc will produce a word count of these two files.\nIf we run this in the bash_for_bio/ folder, we’ll get the following:\n      26      96     656 01_assignment.qmd\n     493    2609   16206 02_scripting.qmd",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Batch Processing and Submitting Jobs</span>"
    ]
  },
  {
    "objectID": "03_batch.html#globs-selecting-multiple-files",
    "href": "03_batch.html#globs-selecting-multiple-files",
    "title": "6  Batch Processing and Submitting Jobs",
    "section": "6.4 globs: selecting multiple files",
    "text": "6.4 globs: selecting multiple files\nHowever, typing each element of our list is going to be difficult - can we select files in a different way?\nWe can use globs, or wildcard characters to select multiple files with a criteria.\nFor example, *.qmd will list all of the .qmd files in the bash_for_bio directory.\n#| filename: ./scripts/week3/batch_on_rhino.sh\n#!/bin/bash\n1for file in ./data/*.fastq\n2do\n3  wc $file\n4done\n\n1\n\nStart for loop and cycle through all .qmd files\n\n2\n\nStart of instructions\n\n3\n\nCount the words in each .qmd file using wc\n\n4\n\nEnd of instructions\n\n\nIf we run this in our repository, we get something similar to this:\n3220   3220 142485 ./data/CALU1_combined_final.fastq\n2484   2484 109917 ./data/HCC4006_final.fastq\n1836   1836 81243 ./data/MOLM13_combined_final.fastq\nThe *.fasta (the wildcard operator, also known as a a pattern-matching mechanism used primarily in shells (like Bash) and various programming languages to match filenames and paths. It is also known as “filename expansion” or “wildcard matching.”glob) can be used in various ways. For example, if our files are in a folder called data/, we could specify:\nfor file in ./data/*.fastq\ndo\n  wc $file \ndone\n\n6.4.1 Try it out\n./scripts/week3/batch_on_rhino.sh\n\n\n\n\n\n\nNoneA common pattern: a folder with only one type of file in them\n\n\n\nOne thing that makes it easier to process a bunch of files is to have the data be in the same folder, with nothing else in them.\nFor example, I might create a fastq/ folder where I store my data, so I can pass the glob fastq/* to process the files in that.\n#!/bin/bash\nmodule load BWA\nmodule load SAMtools\nFASTA_LOCATION=\"\"\nOUTPUT_FOLDER=\"/home/tladera2/project_x/bam_files/\"\nfor file in fastq/*\ndo\n  bwa mem ${FASTA_LOCATION} file &gt; ${OUTPUT_FOLDER}${file}.bam \ndone\nmodule purge\n\n\n\n\n6.4.2 For more info on globs\nSee page 12 in Bite Size Bash.\n\n\n\n\n\n\nNoneSelecting files with complicated patterns: Regular Expressions\n\n\n\nAt some point, globs are going to be inadequate depending on how you store files. At that point, you will probably need to learn regular expressions, which is a much more powerful way of describing search patterns.\nI will be honest and say this is one thing LLMs are very good at. But you should have the vocabulary to prompt them, including:\n\nLiteral Characters\nMetacharacters (including how to escape special characters)\nCharacter classes (specifying groups of characters to match)\nCapture Groups, including named capture groups\nQuantifiers\nLogical expressions\nAnchors (matching positions)\n\nKnowing these general concepts will help you with your LLM prompts. And always test your regular expressions to make sure that they are capturing the file patterns you expect.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Batch Processing and Submitting Jobs</span>"
    ]
  },
  {
    "objectID": "03_batch.html#batching-on-hpc",
    "href": "03_batch.html#batching-on-hpc",
    "title": "6  Batch Processing and Submitting Jobs",
    "section": "6.5 Batching on HPC",
    "text": "6.5 Batching on HPC\nNow we can start to do more advanced things on the HPC cluster: use one machine to process each file. We will use a slightly different mechanism to cycle through files: the A numbered list that usually goes from 1 to the length of the list. Used in SLURM batch processing.job array.\nLet’s start out with SLURM scripts.\n\n6.5.1 SLURM Scripts\nSLURM scripts are a special kind of shell script that contain additional information for the SLURM manager. This includes:\n\nNumber of nodes (machines) to request\nMemory and CPU requirements for each machine\n\nWe specify these using a special kind of comment: SLURM directives. Directives begin a line with #SBATCH:\n#SBATCH --nodes=1 \nIn this example, we are specifying the number of nodes.\nNote that because directives begin with a # - they are treated like comments by bash, but are usable by SLURM.n\n\n\n6.5.2 SLURM Directives\nWe are able to set some configuration on running our jobs.\n#| eval: false\n#| filename: scripts/week3/sbatch_test.sh\n#!/bin/bash\n1#SBATCH --nodes=3\n2#SBATCH --array=1-3\necho \"${SLURM_ARRAY_TASK_ID} job\"\n\n1\n\nrequest 1 node\n\n2\n\nstart an array\n\n\n\n\n\n\n\n\nNoneMore about directives\n\n\n\nMuch more information about the kinds of directives that you can specify in a SLURM script is available here: https://www.osc.edu/supercomputing/batch-processing-at-osc/slurm_directives_summary\nThe most important directive you should be aware of is how many nodes you need to request, and how much memory your jobs will need. gizmo does not use the memory parameter. Instead, use the number of cores as a proxy for memory. Much more information is here https://sciwiki.fredhutch.org/scicomputing/compute_jobs/#memory\n\n\n\n\n\n\n\n\nNoteAll About Jobs\n\n\n\nThere is a ton of information that I’m not including about the gizmo cluster. For much more info on submitting jobs, please refer to https://sciwiki.fredhutch.org/scicomputing/compute_jobs/.\n\n\n\n\n6.5.3 Job Arrays\nThis line:\n#SBATCH --array=1-3 \nWill create a job array. This will create a variable called $SLURM_ARRAY_TASK_ID that will cycle through the numbers 1-3. Each Task ID corresponds to a different subjob. Let’s try a simpler script to show what’s going on:\n#| eval: false\n#| filename: sbatch_test.sh\n#!/bin/bash\n#SBATCH --array=1-3\n#SBATCH --nodes=1\necho \"${SLURM_ARRAY_TASK_ID} job\"\nThis is a minimal script that will execute 3 subjobs. It will cycle through the job array and print the array number for each job.\n#| eval: false\nsbatch sbatch_test.sh\nOn submitting, we will get a message like this (your job number will be different):\nSubmitted batch job 26328834\nThis will run very quickly on the three nodes. And if we look for the output files:\nls -l slurm-26328834*\nWe will get the following output:\n-rw-rw---- 1 tladera2 g_tladera2 8 Jul 15 13:50 slurm-26328834_1.out\n-rw-rw---- 1 tladera2 g_tladera2 8 Jul 15 13:50 slurm-26328834_2.out\n-rw-rw---- 1 tladera2 g_tladera2 8 Jul 15 13:50 slurm-26328834_3.out\nTaking a look at one of these files using cat:\ncat slurm-26328834_3.out\nWe’ll see this:\n3 job\n\n\n\n\n\ngraph TD\n  A[\"sbatch sbatch_test.sh\"] --\"1\"--&gt; B\n  B[\"echo 1 job\"]\n  A --\"2\"--&gt; C[\"echo 2 job\"]\n  A --\"3\"--&gt; D[\"echo 3 job\"]\n\n\n\n\n\n\nWhat happened here? sbatch submitted our job array as 3 different subjobs to 3 different nodes under a single job id. Each node then outputs a file with the subjob id that contains the job number.\n\n\n6.5.4 Try it out\nTry running\nsbatch ./scripts/week3/sbatch_test.sh\nAnd look at the resulting .out files.\n\n\n6.5.5 Processing lists of files using Job Arrays\nSo now we know that ${SLURM_ARRAY_TASK_ID} will let us specify a subjob within our script, how do we use it in our script?\nSay, we have a list of 3 files in our directory data/, and we can list them by using ../../data/*.fastq. We can use the ${SLURM_ARRAY_TASK_ID} as an index to specify a different file.\nThe one caveat is that we need to know the number of files beforehand.\nThis script will run our run_bwa.sh on 3 separate files on 3 separate nodes:\n#| eval: false\n#| filename: scripts/week3/run_sbatch.sh\n#!/bin/bash\n1#SBATCH --nodes=3\n2#SBATCH --array=1-3\n3#SBATCH --mem-per-cpu=1gb\n4#SBATCH --time=00:10:00\n5file_array=(../../data/*.fastq)\nind=$((SLURM_ARRAY_TASK_ID-1))\ncurrent_file=${file_array[$ind]}\n./run_bwa.sh $current_file\n\n1\n\nInitialize job array to range from 1 to 10\n\n2\n\nlist all files in ../..data/ with extension .fastq. Assign it to the array $file_array.\n\n3\n\nFor current task id, calculate the appropriate index (we have to subtract 1 because bash arrays begin at 0)\n\n4\n\nPull the current file path given the task id, assign it to $current_file.\n\n5\n\nRun run_bwa.sh on $current_file.\n\n\nWe run this on rhino using this command (we are in ~/bash_for_bio/scripts/week3/)\nsbatch sbatch_bwa.sh\nWe’ll get the response:\nSubmitted batch job 35300989\nIf we take a look at the job queue using squeue, we’ll get something like this:\nsqueue -u tladera2\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n        35300989_1 campus-ne sbatch_b tladera2  R       0:10      1 gizmok34\n        35300989_2 campus-ne sbatch_b tladera2  R       0:10      1 gizmok40\n        35300989_3 campus-ne sbatch_b tladera2  R       0:10      1 gizmok79\nWe can see our three subjobs, which are indicated by the Job IDs 35300989_1, 35300989_2, and 35300989_3.\nladera2@rhino02:~/bash_for_bio/scripts/week3$ ls -l\ntotal 312\n-rw-rw---- 1 tladera2 g_tladera2 211895 Sep 26 10:29 CALU1_combined_final.sam\n-rw-rw---- 1 tladera2 g_tladera2 160489 Sep 26 10:29 HCC4006_final.sam\n-rw-rw---- 1 tladera2 g_tladera2 121509 Sep 26 10:29 MOLM13_combined_final.sam\n-rwxrwxrwx 1 tladera2 g_tladera2    590 Sep 26 09:45 run_bwa.sh\n-rwxrwxrwx 1 tladera2 g_tladera2    256 Sep 26 10:28 run_sbatch.sh\n-rw-rw---- 1 tladera2 g_tladera2    614 Sep 26 10:29 slurm-35300992_1.out\n-rw-rw---- 1 tladera2 g_tladera2    579 Sep 26 10:29 slurm-35300992_2.out\n-rw-rw---- 1 tladera2 g_tladera2    619 Sep 26 10:29 slurm-35300992_3.out\nAnd you’ll see we generated our SAM files for each sample! Neat. There are also the .out files from each subjob as well, which will contain the  for each subjob.\nYou can see that we outputted our files to the scripts/week3/ directory. It’s part of your job in the exercises to adapt run_sbatch.sh and run_bwa.sh to output to a directory of your choosing.\n\n\n\n\n\n\nNoteWhy isn’t my job launching?\n\n\n\nThe gizmo cluster is used by a lot of people at the Hutch, so it gets busy from time to time.\nDon’t worry, your jobs will be eventually processed if your requests are reasonable.\nf you want to look at the jobs that a particular user is running, you can use the -u flag. Try squeue -u on one of the users in the queue, for example tladera2:\nsqueue -u tladera2\nYou will usually use squeue -u on your own username, so you can see the status of your jobs. This is an example from my previous career at OHSU (I didn’t have time to generate one on gizmo)\n            JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n           3970834 very_long KIRC.bla    wooma  R 11-04:21:09      1 exanode-7-15\n           3970835 very_long OV.blast    wooma  R 11-04:20:32      1 exanode-7-5\nLet’s take a look at the output. You can see some pretty useful info: the JOBID, what PARTITION the job is running under, and the STatus.\nSTatus is really important, because it can tell you whether your job is:\nR (Running), PD (Pending), ST (Stopped), S (Suspended), CD (Completed).\nR or PD status is what you want to see, because that means it’s in the queue and will be executed.\nFor much more info, please check out https://sciwiki.fredhutch.org/scicomputing/compute_jobs/#job-priority and https://sciwiki.fredhutch.org/scicomputing/compute_jobs/#why-isnt-my-job-running.\n\n\n\n\n6.5.6 scanceling a jobarray\nAs we noted, one of the strengths in using a job array to process multiple files is that they are spawned as sub or child jobs of a parent job id.\nWhat if we made a mistake? We can use the scancel command to cancel the entire set of jobs by giving it our parent job id:\nscancel 26328834\nThis will cancel all sub jobs related to the parent job. No fuss, no muss.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Batch Processing and Submitting Jobs</span>"
    ]
  },
  {
    "objectID": "03_batch.html#whats-next",
    "href": "03_batch.html#whats-next",
    "title": "6  Batch Processing and Submitting Jobs",
    "section": "6.6 What’s Next?",
    "text": "6.6 What’s Next?\nNext week we will discuss using MiniWDL (a workflow manager) to process files through multi-step workflows in WDL.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Batch Processing and Submitting Jobs</span>"
    ]
  },
  {
    "objectID": "03_assignment.html",
    "href": "03_assignment.html",
    "title": "7  Week 3 Exercises",
    "section": "",
    "text": "7.1 In-class\nand examine the following output files:",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Week 3 Exercises</span>"
    ]
  },
  {
    "objectID": "03_assignment.html#in-class",
    "href": "03_assignment.html#in-class",
    "title": "7  Week 3 Exercises",
    "section": "",
    "text": "Try it out:\n\n./scripts/week3/batch_on_rhino.sh\n\nTry it out:\n\n#| eval: false\nsbatch sbatch_test.sh\n\n\nTry it out:",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Week 3 Exercises</span>"
    ]
  },
  {
    "objectID": "03_assignment.html#homework-exercises",
    "href": "03_assignment.html#homework-exercises",
    "title": "7  Week 3 Exercises",
    "section": "7.2 Homework exercises",
    "text": "7.2 Homework exercises\nExercises 1, 2, 3 are required for the badge.\n\nIn the data/ folder, there is a list of .csv files. Modify the code below to process each file.\n\n#!/bin/bash\nfor file in ------\ndo\n  head ------\ndone\n\nModify the code in 1) to write a for loop in a script to count the number of lines for each .csv file. The output should be:\n\necho \"${file}\\t(wc -l $file)\\n\"\nRun the script and redirect your output to files_out.txt. Put your bash command you used below:\n\n\nModify the below script to process the .sam files in ./data/. Save it as sbatch_sam.sh and run it. How many files were generated?\n\n#!/bin/bash\n#SBATCH --array=1-________  \nmodule load SAMtools\nfile_array=(./data/*.sam) \nind=$((SLURM_ARRAY_TASK_ID-1)) \ncurrent_file=${file_array[$ind]} \nsamtools view -c $current_file &gt; ${current_file}.counts.out",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Week 3 Exercises</span>"
    ]
  },
  {
    "objectID": "container-basics.html",
    "href": "container-basics.html",
    "title": "8  Reading: JSON/Container Basics",
    "section": "",
    "text": "8.1 What is JSON?\nOne requirement for running workflows is basic knowledge of JSON.\nJSON is short for JavaScript Object Notation. It is a format used for storing information on the web and for interacting with Application Program Interfaces (APIs).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Reading: JSON/Container Basics</span>"
    ]
  },
  {
    "objectID": "container-basics.html#sec-json",
    "href": "container-basics.html#sec-json",
    "title": "8  Reading: JSON/Container Basics",
    "section": "",
    "text": "8.1.1 How is JSON used?\nJSON is used in multiple ways:\n\nSubmitting Jobs with complex parameters/inputs\n\nSo having basic knowledge of JSON can be really helpful. JSON is the common language of the internet.\n\n\n8.1.2 Elements of a JSON file\nHere are the main elements of a JSON file:\n\nKey:Value Pair. Example: \"name\": \"Ted Laderas\". In this example, our key is “name” and our value is “Ted Laderas”\nList [] - a collection of values. All values have to be the same data type. Example: [\"mom\", \"dad\"]\nObject {} - A collection of key/value pairs, enclosed with curly brackets ({}).\n\n\n\n\n\n\n\nNoteCheck Yourself\n\n\n\nWhat does the names value contain in the following JSON? Is it a list, object or key:value pair?\n{\n  \"names\": [\"Ted\", \"Lisa\", \"George\"]\n}\n\n\n\n\n\n\n\n\nNoteAnswer\n\n\n\n\n\nIt is a list. We know this because the value contains a [].\n{\n  \"names\": [\"Ted\", \"Lisa\", \"George\"]\n}\n\n\n\n\n\n8.1.3 JSON Input Files\nWhen you are working with WDL, it is easiest to manage files using JSON files. Here’s the example we’re going to use from the ww-fastq-to-cram workflow.\n#| eval: false\n#| filename: \"json_data/example.json\"\n{\n  \"PairedFastqsToUnmappedCram.batch_info\": [\n    {\n      \"dataset_id\": \"TESTFASTQ1\",\n      \"sample_name\": \"HG02635\",\n      \"library_name\": \"SRR581005\",\n      \"sequencing_center\": \"1000-Genomes\",\n      \"filepaths\": [{\n        \"flowcell_name\": \"20121211\",\n        \"fastq_r1_locations\": [\"tests/data/SRR581005_1.ds.fastq.gz\"],\n        \"fastq_r2_locations\": [\"tests/data/SRR581005_2.ds.fastq.gz\"]\n      }]\n    },\n    {\n      \"dataset_id\": \"TESTFASTQ2\",\n      \"sample_name\": \"HG02642\",\n      \"library_name\": \"SRR580946\",\n      \"sequencing_center\": \"1000-Genomes\",\n      \"filepaths\": [{\n        \"flowcell_name\": \"20121211\",\n        \"fastq_r1_locations\": [\"tests/data/SRR580946_1.ds.fastq.gz\"],\n        \"fastq_r2_locations\": [\"tests/data/SRR580946_2.ds.fastq.gz\"]\n      }]\n    }\n  ]\n}\nThis might seem overwhelming, but let’s look at the top-level structures first:\n1{\n2  \"PairedFastqsToUnmappedCram.batch_info\": [\n   ...   \n  ]\n}\n\n1\n\nThe top level of the file is a JSON object\n\n2\n\nThe next level down (“PairedFastqsToUnmappedCram.batch_info”) is a list.\n\n\nThis workflow specifies the file inputs using the PairedFastqsToUnmappedCram.batch_info object, which is a list.\nEach sample in the PairedFastqsToUnmappedCram.batch_info list is its own object:\n  \"PairedFastqsToUnmappedCram.batch_info\": [\n    {\n      \"dataset_id\": \"TESTFASTQ1\",\n      \"sample_name\": \"HG02635\",\n      \"library_name\": \"SRR581005\",\n      \"sequencing_center\": \"1000-Genomes\",\n      \"filepaths\": [{\n        \"flowcell_name\": \"20121211\",\n        \"fastq_r1_locations\": [\"tests/data/SRR581005_1.ds.fastq.gz\"],\n        \"fastq_r2_locations\": [\"tests/data/SRR581005_2.ds.fastq.gz\"]\n      }]\n    },\n    ....\nBecause we are aligned paired-end data, notice there are two keys, fastq_r1_locations and fastq_r2_locations.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Reading: JSON/Container Basics</span>"
    ]
  },
  {
    "objectID": "container-basics.html#sec-containers",
    "href": "container-basics.html#sec-containers",
    "title": "8  Reading: JSON/Container Basics",
    "section": "8.2 Containers",
    "text": "8.2 Containers\nWe already learned about software modules (Section 3.5) on the gizmo cluster. There is an alternative way to use software: pulling and running a software The executable software environment actually installed and running on a machine. Runnable. Generate from docker pull from a repository.container.\n\n8.2.1 What is a Container?\nA container is a self-contained unit of software. It contains everything needed to run the software on a variety of machines. If you have the container software installed on your machine, it doesn’t matter whether it is MacOS, Linux, or Windows - the container will behave consistently across different operating systems and architectures.\nThe container has the following contents:\n\nSoftware - The software we want to run in a container. For bioinformatics work, this is usually something like an aligner like bwa, or utilities such as samtools\nSoftware Dependencies - various software packages needed to run the software. For example, if we wanted to run tidyverse in a container, we need to have R installed in the container as well.\nFilesystem - containers have their own isolated filesystem that can be connected to the “outside world” - everything outside of the container. We’ll learn more about customizing these with bind paths (Section 10.2.3).\n\nIn short, the container has everything needed to run the software. It is not a full operating system, but a smaller mini-version that cuts out a lot of cruft.\nContainers are When you are finished with Docker containers, everything that you created in them will disappear when you finish running scripts in the container.ephemeral. They leverage the the file system of their host to manage files. These are called both Volumes (the Docker term) and Bind Paths (the apptainer term).\n\n\n8.2.2 Docker vs. Apptainer\nThere are two basic ways to run Docker containers:\n\nUsing the Docker software\nUsing the Apptainer software (for HPC systems)\n\nIn general, Docker is used on systems where you have a high level of access to the system. This is because docker uses a special user group called docker that has essentially root level privileges. This is not something to be taken lightly.\nThis is not the case for HPC systems, which are shared and granting this level of access to many people is not practical. This is when we use A container software that is often used on HPC systems. Can run Docker containers.Apptainer (which used to be called Singularity), which requires a much lower level of user privileges to execute tasks. For more info, see Section 10.2 .\n\n\n\n\n\n\nWarningBe Secure\n\n\n\nBefore we get started, security is always a concern when running containers. The docker group has elevated status on a system, so we need to be careful that when we’re running them, these containers aren’t introducing any system vulnerabilities. Note that on HPC systems, the main mechanism for running containers is apptainer, which is designed to be more secure.\nThese are mostly important when running containers that are web-servers or part of a web stack, but it is also important to think about when running jobs on HPC.\nHere are some guidelines to think about when you are working with a container.\n\nUse vendor-specific Docker Images when possible.\nUse container scanners to spot potential vulnerabilities. DockerHub has a vulnerability scanner that scans your Docker images for potential vulnerabilities. For example, the WILDS Docker library employs a vulnerability scanner and the containers are regularly patched to prevent vulnerabilities.\nAvoid kitchen-sink images. One issue is when an image is built on top of many other images. It makes it really difficult to plug vulnerabilities. When in doubt, use images from trusted people and organizations. At the very least, look at the Dockerfile to see that suspicious software isn’t being installed.\n\n\n\n\n\n8.2.3 The WILDS Docker Library\nThe Data Science Lab has a set of Docker containers for common Bioinformatics tasks available in the WILDS Docker Library. These include:\n\nbwa mem\nsamtools\ngatk\nbcftools\nmanta\ncnvkit\ndeseq2\n\nAmong many others. Be sure to check it out before you start building your own containers.\n\n\n8.2.4 Pulling a Docker Container\nLet’s pull a docker container from the Docker registry. Note we have to specify docker:// when we pull the container, because Apptainer has its own internal format called SIF.\nmodule load Apptainer/1.1.6\napptainer pull docker://ghcr.io/getwilds/scanpy:latest\napptainer run --bind /path/to/data:/data,/path/to/script:/script docker://getwilds/scanpy:latest python /script/example.py\n\n\n8.2.5 Bind Paths\nOne thing to keep in mind is that containers have their own filesystem. They can only read and write to folders in the external filesystem that you give them access to with bind paths. The one exception is the current working directory.\nFor more info about bind paths see Section 10.2.3.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Reading: JSON/Container Basics</span>"
    ]
  },
  {
    "objectID": "container-basics.html#glossary",
    "href": "container-basics.html#glossary",
    "title": "8  Reading: JSON/Container Basics",
    "section": "8.3 Glossary",
    "text": "8.3 Glossary\n\n Term  Definition",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Reading: JSON/Container Basics</span>"
    ]
  },
  {
    "objectID": "04_containers_workflows.html",
    "href": "04_containers_workflows.html",
    "title": "9  Workflows",
    "section": "",
    "text": "9.1 Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Workflows</span>"
    ]
  },
  {
    "objectID": "04_containers_workflows.html#learning-objectives",
    "href": "04_containers_workflows.html#learning-objectives",
    "title": "9  Workflows",
    "section": "",
    "text": "Execute a workflow on the Fred Hutch Cluster\nModify a workflow and test it on a compute node\nUtilize a container and test inside it on a compute node",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Workflows</span>"
    ]
  },
  {
    "objectID": "04_containers_workflows.html#using-a-container",
    "href": "04_containers_workflows.html#using-a-container",
    "title": "9  Workflows",
    "section": "9.2 Using a Container",
    "text": "9.2 Using a Container\nIn Section 8.2, we learned a little bit about using Apptainer to run a Docker container. Let’s try to pull a common container, the Genome Analysis Toolkit (GATK) and run things inside the container.\nThe first thing we need to do is load Apptainer:\nmodule load Apptainer/1.1.6\nThen we can pull the docker container:\napptainer pull docker://biocontainers/samtools:v1.9-4-deb_cv1\nWe can check if we have pulled the docker image by using\napptainer cache list\nOkay, now we have confirmed that we downloaded the apptainer image. Now we can try to execute things with it.\napptainer run \\\n1    --bind --bind /path/to/data:/data \\\n2    docker://biocontainers/samtools:v1.9-4-deb_cv1 \\\n3    samtools view -c /mydata/$1 &gt; /mydata/$1.counts.txt\n\n1\n\nBind path\n\n2\n\nDocker image we have downloaded\n\n3\n\nsamtools command to run.\n\n\nIt’s worth trying this once to make sure you understand how all of the pieces are connected. In general, I do recommend using a workflow runner (Section 9.3) instead, because it helps manage all of these details",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Workflows</span>"
    ]
  },
  {
    "objectID": "04_containers_workflows.html#sec-workflows",
    "href": "04_containers_workflows.html#sec-workflows",
    "title": "9  Workflows",
    "section": "9.3 Running Workflows",
    "text": "9.3 Running Workflows\nSo far, we’ve been batch processing using job arrays in SLURM. However, when you graduate to processing in multiple steps, you should consider using a workflow manager for processing files.\nA workflow manager will take a workflow, which can consist of multiple steps and allow you to batch process files.\nA good workflow manager will allow you to:\n\nRestart failed subjobs in the workflow\nAllow you to customize where intermediate and final outputs go\nSwap and customize modules in your workflow\nAdapt to different computing architectures (HPC/cloud/etc)\n\nMany bioinformaticists have used workflow managers to process and manage hundreds or thousands of files at a time. They are well worth the extra effort it takes to learn.\nHere is an overview of some of the common bioinformatics workflow managers. We will be using miniWDL, which runs WDL files.\n\n\n\nManager Software\nWorkflow Formats\nNotes\n\n\n\n\nCromwell\nWDL/CWL\nMade for HPC Jobs\n\n\nSprocket\nWDL\nMade for HPC Jobs\n\n\nMiniWDL\nWDL\nUsed for local testing of workflows\n\n\nDNANexus\nWDL/CWL\nUsed for systems such as AllOfUs\n\n\nNextflow\n.nf files\nOwned by seqera\n\n\nSnakemake\nmake files\n\n\n\n\n\n9.3.1 Grabbing a WDL workflow from GETWILDS\ngit clone https://github.com/getwilds/ww-star-deseq2/\n\n\n9.3.2 Executing a WDL workflow\nSay someone has given us a WDL file - how do we set it up to run on our own data?\nIn order to test this out on a batch of files, we’ll first request a node using grabnode (Section 10.1.1).\ngrabnode\nIn our node, we’ll use miniWDL to run our WDL workflow. It is accessible via the cirro module.\nmodule load cirro\nwhich miniwdl\nWe will get the repsonse:\n/app/software/cirro/1.7.0-foss-2024a/bin/miniwdl\nWe’ll investigate using miniwdl run as an initial way to interact with miniwdl.\nminiwdl run \njava -jar $EBROOTCROMWELL/cromwell.jar run ww-star2-deseq2.wdl \\\n   --inputs input_file.json\n\n\n9.3.3 Input Files as JSON\nIf you want to work with the JSON format (Section 8.1), there is a trick to generating a template .json file for a workflow. There is an additional utility called womtools that will generate a template file from a workflow file.\nminiwdl input-template ww-star2-deseq2.wdl &gt; ww-star2-deseq2-inputs.json\nThis will generate a file called ww-star2-deseq2-inputs.json that contains all of the inputs:\n{\n  \"star_deseq2.samples\": \"Array[WomCompositeType {\\n name -&gt; String\\nr1 -&gt; File\\nr2 -&gt; File\\ncondition -&gt; String \\n}]\",\n  \"star_deseq2.rnaseqc_cov.cpu_cores\": \"Int (optional, default = 2)\",\n  ...\n}\nThis can be a good head start to making your .json files.\n\n\n9.3.4 Working with file manifests in WDL\nAnother strategy you can use to cycle through a list of files is to use a file manifest to process a list of files.\nThere is an example workflow that shows how to work with a file manifest. This can be helpful for those who aren’t necessarily good at working with JSON.\nThis workflow has a single input, which is the location of the file manifest. It will then cycle through the manifest, line by line.\nThis is what the file manifest contains. Notice there are three named columns for this file: sampleName, bamLocation, and bedLocation.\n\n\n\n\n\n\n\n\nsampleName\nbamLocation\nbedLocation\n\n\n\n\nsmallTestData\n/fh/fast/paguirigan_a/pub/ReferenceDataSets/workflow_testing_data/WDL/unpaired-panel-consensus-variants-human/smallTestData.unmapped.bam\n/fh/fast/paguirigan_a/pub/ReferenceDataSets/reagent_specific_data/sequencing_panel_bed/TruSight-Myeloid-Illumina/trusight-myeloid-amplicon-v4-track_interval-liftOver-hg38.bed\n\n\nsmallTestData-reference\n/fh/fast/paguirigan_a/pub/ReferenceDataSets/workflow_testing_data/WDL/paired-panel-consensus-variants-human/smallTestData-reference.unmapped.bam\n/fh/fast/paguirigan_a/pub/ReferenceDataSets/reagent_specific_data/sequencing_panel_bed/TruSight-Myeloid-Illumina/trusight-myeloid-amplicon-v4-track_interval-liftOver-hg38.bed\n\n\n\nHere’s the workflow part of the file:\nversion 1.0\n#### WORKFLOW DEFINITION\n\nworkflow ParseBatchFile {\n  input {\n    File batch_file\n  }\n\n1  Array[Object] batch_info = read_objects(batch_file)\n\n2  scatter (job in batch_info){\n3    String sample_name = job.sampleName\n4    File bam_file = job.bamLocation\n5    File bed_file = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN YOUR BATCH FILE HERE!!!!\n6    call Test {\n      input: in1=sample_name, in2=bam_file, in3=bed_file\n    }\n  }  # End Scatter over the batch file\n\n  # Outputs that will be retained when execution is complete\n  output {\n    Array[File] output_array = Test.item_out\n  }\n\n  parameter_meta {\n    batch_file: \"input tsv containing details about each sample in the batch\"\n    output_array: \"array containing details about each samples in the batch\"\n  }\n} # End workflow\n\n1\n\nRead in file manifest line by line, and store in the array called batch_info.\n\n2\n\nCycle through the manifest line by line, scattering to multiple nodes\n\n3\n\nGet sample_name input from job.sample_name\n\n4\n\nGet bam_file input from job.bam_file\n\n5\n\nGet bed_file input from job.bed_file\n\n6\n\nDo something with the inputs.\n\n\n\n\n9.3.5 Let’s start with WDL Tasks\nWe will start with the lower level of abstraction in WDL: The task.\n\n\n9.3.6 Anatomy of a Task\ntask build_star_index {\n  meta {\n      ...\n  }\n\n  parameter_meta {\n    reference_fasta: \"Reference genome FASTA file\"\n    reference_gtf: \"Reference genome GTF annotation file\"\n    sjdb_overhang: \"Length of the genomic sequence around the annotated junction to be used in constructing the splice junctions database\"\n    genome_sa_index_nbases: \"Length (bases) of the SA pre-indexing string, typically between 10-15 (scales with genome size)\"\n    memory_gb: \"Memory allocated for the task in GB\"\n    cpu_cores: \"Number of CPU cores allocated for the task\"\n  }\n\n1  input {\n    File reference_fasta\n    File reference_gtf\n    Int sjdb_overhang = 100\n    Int genome_sa_index_nbases = 14\n    Int memory_gb = 64\n    Int cpu_cores = 8\n  }\n\n2  command &lt;&lt;&lt;\n    set -eo pipefail\n    \n    mkdir star_index\n\n    echo \"Building STAR index...\"\n    STAR \\\n      --runMode genomeGenerate \\\n      --runThreadN ~{cpu_cores} \\\n      --genomeDir star_index \\\n      --genomeFastaFiles \"~{reference_fasta}\" \\\n      --sjdbGTFfile \"~{reference_gtf}\" \\\n      --sjdbOverhang ~{sjdb_overhang} \\\n      --genomeSAindexNbases ~{genome_sa_index_nbases}\n\n    tar -czf star_index.tar.gz star_index/\n  &gt;&gt;&gt;\n\n3  output {\n    File star_index_tar = \"star_index.tar.gz\"\n  }\n\n4  runtime {\n    docker: \"getwilds/star:2.7.6a\"\n    memory: \"~{memory_gb} GB\"\n    cpu: cpu_cores\n  }\n}\n\n1\n\nInput for our task.\n\n2\n\nBash commands to execute in task\n\n3\n\nDescription of output\n\n4\n\nRuntime requirements for execution. This is a lot like the #SBATCH directives.\n\n\nEverything between the &lt;&lt;&lt; and &gt;&gt;&gt; is essentially a bash script. WDL has its own variables\n\n\n9.3.7 Architecture of a WDL file\nThe best way to read WDL files is to read them top down. We’ll focus on the basic sections of a WDL file before we see how they work together.\nThe code below is from the WILDs WDL Repo.\nworkflow SRA_STAR2Pass {\n   meta{\n   ...\n   }\n\n   parameter_meta{\n   ...\n   }\n\n1  input {\n    ...\n  }\n\n2  if (!defined(reference_genome)) {\n    call download_reference {}\n  }\n\n  RefGenome ref_genome_final = select_first([reference_genome, download_reference.genome])\n\n3  call build_star_index {\n    ...\n  }\n\n  # Outputs that will be retained when execution is complete\n4  output {\n    ...\n  }\n\n} \n\n1\n\nInputs for workflow\n\n2\n\nIf/then statement of workflow\n\n3\n\ncall a task\n\n4\n\nOutputs of workflow\n\n\nLet’s go through each of these in detail (we’ll get back to the meta and parameters-meta sections)\nThe structure of the workflow\nworkflow SRA_STAR2Pass {\n  input {\n    Array[SampleInfo] samples\n    RefGenome? reference_genome\n    String reference_level = \"\"\n    String contrast = \"\"\n  }\n\n  if (!defined(reference_genome)) {\n    call download_reference {}\n  }\n\n  RefGenome ref_genome_final = select_first([reference_genome, download_reference.genome])\n\n  call build_star_index { input:\n      reference_fasta = ref_genome_final.fasta,\n      reference_gtf = ref_genome_final.gtf\n  }\n\n  # Outputs that will be retained when execution is complete\n  output {\n    ...\n  }\n\n}",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Workflows</span>"
    ]
  },
  {
    "objectID": "04_containers_workflows.html#where-next",
    "href": "04_containers_workflows.html#where-next",
    "title": "9  Workflows",
    "section": "9.4 Where Next?",
    "text": "9.4 Where Next?\nNow that you understand the basics of working with Bash and WDL, you are now ready to start working with WDL workflows.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Workflows</span>"
    ]
  },
  {
    "objectID": "testing.html",
    "href": "testing.html",
    "title": "10  Testing Scripts",
    "section": "",
    "text": "10.1 Developing and Testing scripts\nOne of the hard things to understand is what can be run on a compute node versus the head node, and what file systems are accessible via a compute node.\nA lot of the issues you might have is because you need to understand the mental model of how cluster computing works. And the best way to understand that is to test your code on a compute node.\nLet’s explore how we can do that. You should also review the material about using screen (Section 12.7).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Testing Scripts</span>"
    ]
  },
  {
    "objectID": "testing.html#developing-and-testing-scripts",
    "href": "testing.html#developing-and-testing-scripts",
    "title": "10  Testing Scripts",
    "section": "",
    "text": "10.1.1 Testing code on a compute node\nFred Hutch users have the advantage of grabnode, which is a custom command that lets you request an interactive instance of a compute node. (Non-FH folks can usually request this with the -it flag for srun)\nWhy would you want to do this? A good part of this is about testing software and making sure that your paths are correct.\n\n\n\n\n\n\nNoneDon’t rely on grabnode/interactive mode for your batch work\n\n\n\nWe often see users that will request a multicore node with higher memory, and do their processing on that node.\nThis doesn’t take advantage of all of the machines that are available on the cluster, and thus is a suboptimal way to utilize the cluster.\nWhen you are doing interactive analysis, such as working in JupyterLab or RStudio, that is a valid way to work. But when you have tasks you can scatter amongst many nodes, requesting a high-spec node isn’t a great way to optimally achieve things.\nThe other disadvantage is that you may be waiting a very long time to get that multicore node, whereas if you batch across a bunch of nodes, you will get your work done much faster.\n\n\n\n\n10.1.2 Grabbing an interactive shell on a worker\nWhen you’re testing code that’s going to run on a worker node, you need to be aware of what the worker node sees.\nIt’s also important in estimating how long our tasks are going to run since we can test how long a task runs for a representative dataset.\n\n\n\n\n\n\nNoteFor FH Users: grabnode\n\n\n\nOn the FH system, we can use a command called grabnode, which will let us request a node. It will ask us for our requirements (numbers of cores, memory, etc.) for our node.\ntladera2@rhino01:~$ grabnode\ngrabnode will then ask us for what kind of instance we want, in terms of CPUs, Memory, and GPUs. Here, I’m grabbing a node with 8 cores, 8 Gb of memory, using it for 1 day, and no GPU.\nHow many CPUs/cores would you like to grab on the node? [1-36] 8\nHow much memory (GB) would you like to grab? [160] 8\nPlease enter the max number of days you would like to grab this node: [1-7] 1\nDo you need a GPU ? [y/N]n\n\nYou have requested 8 CPUs on this node/server for 1 days or until you type exit.\n\nWarning: If you exit this shell before your jobs are finished, your jobs\non this node/server will be terminated. Please use sbatch for larger jobs.\n\nShared PI folders can be found in: /fh/fast, /fh/scratch and /fh/secure.\n\nRequesting Queue: campus-new cores: 8 memory: 8 gpu: NONE\nsrun: job 40898906 queued and waiting for resources\nAfter a little bit, you’ll arrive at a new prompt:\n(base) tladera2@gizmok164:~$\nNow you can test your batch scripts, in order to make sure your file paths are correct. It is also helpful in profiling your job.\nIf you’re doing interactive analysis that is going to span over a few days, I recommend that you use screen or tmux.\n\n\n\n\n\n\n\n\nNoneFor Other HPC systems\n\n\n\nOn a SLURM system, the way to open interactive shells on a node has changed. Check your version first:\nsrun --version\nIf you’re on a version before 20.11, you can use srun -i --pty bash to open an interactive terminal on a worker:\nsrun -i --pty bash\nIf the version is past 20.11, we can open an interactive shell on a worker with salloc.\nsalloc bash\n\n\n\n\n\n\n\n\nNoneRemember hostname\n\n\n\nWhen you are doing interactive analysis, it is easy to forget in which node you’re working in. Just as a quick check, I use hostname (Section 12.1) to remind myself whether I’m in rhino, gizmo, or within an apptainer container.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Testing Scripts</span>"
    ]
  },
  {
    "objectID": "testing.html#sec-open-container",
    "href": "testing.html#sec-open-container",
    "title": "10  Testing Scripts",
    "section": "10.2 Testing code in a container",
    "text": "10.2 Testing code in a container\nIn this section, we talk about testing scripts in a container using apptainer. We use apptainer (formerly Singularity) in order to run Docker containers on a shared HPC system. This is because Docker itself requires root-level privileges, which is not secure on shared systems.\nIn order to do our testing, we’ll first pull the Docker container, map our bind point (so our container can access files outside of its file system), and then run scripts in the container.\nEven if you aren’t going to frequently use Apptainer in your work, I recommend trying an interactive shell in a container at least once or twice to learn about the container filesystem and conceptually understand how you connect it to the external filesystem.\nI think the hardest thing about working with containers is wrapping your head around the indirectness of them. You are running software with its own internal filesystem and the challenges are getting the container to read files in folders/paths outside of its own filesytem, as well as outputting files into those outside folders.\n\n10.2.1 Pulling a Docker Container\nLet’s pull a docker container from the Docker registry. Note we have to specify docker:// when we pull the container, because Apptainer has its own internal format called SIF.\nmodule load Apptainer/1.1.6\napptainer pull docker://biocontainers/samtools:v1.9-4-deb_cv1\n\n\n10.2.2 Opening a Shell in a Container with apptainer shell\nWhen you’re getting started, opening a shell using Apptainer can help you test out things like filepaths and how they’re accessed in the container. It’s hard to get an intuition for how file I/O works with containers until you can see the limited view from the container.\nBy default, apptainers can see your current directory and navigate to the files in it.\nYou can open an Apptainer shell in a container using apptainer shell. Remember to use docker:// before the container name. For example:\nmodule load Apptainer/1.1.6\napptainer shell docker://biocontainers/samtools:v1.9-4-deb_cv1\nThis will load the apptainer module, and then open a Bash shell in the container using apptainer shell. Once you’re in the container, you can test code, especially seeing whether your files can be seen by the container (see Section 10.2.3). 90% of the issues with using Docker containers has to do with bind paths, so we’ll talk about that next.\nOnce you’re in the shell, you can take a look at where samtools is installed:\nwhich samtools\nNote that the container filesystem is isolated, and we need to explicitly build connections to it (called bind paths) to get files in and out. We’ll talk more about this in the next section.\nOnce we’re done testing scripts in our containers, we can exit the shell and get back into the node.\nexit\n\n\n10.2.3 Using bind paths in containers\nOne thing to keep in mind is that every container has its own filesystem. One of the hardest things to wrap your head around for containers is how their filesystems work, and how to access files that are outside of the container filesystem. We’ll call any filesystems outside of the container external filesystems to make the discussion a little easier.\nBy default, the containers have access to your current working directory. We could make this where our scripts live (such as /home/tladera2/), but because our data is elsewhere, we’ll need to specify that location (/fh/fast/mylab/) as well.\nThe main mechanism we have in Apptainer to access the external filesystem are bind paths. Much like mounting a drive, we can bind directories from the external filesystem using these bind points.\n\n\n\n\n\nflowchart LR\n   B[\"External Directory\\n/fh/fast/mydata/\"] \n   B --read--&gt; C\n   C --write--&gt; B\n   A[\"Container Filesystem\\n/mydata/\"]--write--&gt;C(\"--bind /fh/fast/mydata/:/mydata/\")\n   C --read--&gt; A\n\n\n\n\n\n\nI think of bind paths as “tunnels” that give access to particular folders in the external filesystem. Once the tunnel is open, we can access data files, process them, and save them using the bind path.\nSay my data lives in /fh/fast/mydata/. Then I can specify a bind point in my apptainer shell and apptainer run commands.\nWe can do this with the --bind option:\napptainer shell --bind /fh/fast/mydata:/mydata docker://biocontainers/samtools:v1.9-4-deb_cv1\nNote that the bind syntax doesn’t have the trailing slash (/). That is, note that it is:\n--bind /fh/fast/mydata: ....\nRather than\n--bind /fh/fast/mydata/: ....\nNow our /fh/fast/mydata/ folder will be available as /mydata/ in my container. We can read and write files to this bind point. For example, I’d refer to the .bam file /fh/fast/mydata/my_bam_file.bam as:\nsamtools view -c /mydata/my_bam_file.bam\n\n\n\n\n\n\nNoteOpening a Shell in a Docker Container with Docker\n\n\n\nFor the most part, due to security reasons, we don’t use docker on HPC systems. In short, the docker group essentially has root-level access to the machine, and it’s not a good for security on a shared resource like an HPC.\nHowever, if you have admin level access (for example, on your own laptop), you can open up an interactive shell with docker run -it:\ndocker run -it biocontainers/samtools:v1.9-4-deb_cv1 /bin/bash\nThis will open a bash shell much like apptainer shell. Note that volumes (the docker equivalent of bind paths) are specified differently in Docker compared to Apptainer.\n\n\n\n\n\n\n\n\nNoteWDL makes this way easier\n\n\n\nA major point of failure with Apptainer scripting is when our scripts aren’t using the right bind paths. It becomes even more complicated when you are running multiple steps.\nThis is one reason we recommend writing WDL Workflows and a A system that works with the cluster manager to orchestrate processing data through a workflowworkflow manager (such as A workflow runner. Currently works with WDL files.Cromwell or Sprocket) to run your workflows. You don’t have to worry that your bind points are setup correctly, because they are handled by the workflow manager.\n\n\n\n\n10.2.4 Testing in the Apptainer Shell\nOk, now we have a bind point, so now we can test our script in the shell. For example, we can see if we are invoking samtools in the correct way and that our bind points work.\nsamtools view -c /mydata/my_bam_file.bam &gt; /mydata/bam_counts.txt\nAgain, trying out scripts in the container is the best way to understand what the container can and can’t see.\n\n\n10.2.5 Exiting the container when you’re done\nYou can exit, like any shell you open. You should be out of the container. Confirm by using hostname to make sure you’re out of the container.\n\n\n10.2.6 Testing outside of the container\nLet’s take everything that we learned and put it in a script that we can run on the HPC:\n# Script to samtools view -c an input file:\n# Usage: ./run_sam.sh &lt;my_bam_file.bam&gt;\n# Outputs a count file: my_bam_file.bam.counts.txt\n#!/bin/bash\nmodule load Apptainer/1.1.6\napptainer run --bind /fh/fast/mydata:/mydata \\ \n    docker://biocontainers/samtools:v1.9-4-deb_cv1 \\ \n    samtools view -c /mydata/$1 &gt; /mydata/$1.counts.txt\n#apptainer cache clean\nmodule purge\nWe can use this script by the following command:\n./run_sam.sh chr1.bam \nAnd it will output a file called chr1.bam.counts.txt.\n\n\n\n\n\n\nNoneApptainer Cache\n\n\n\nThe apptainer cache is where your docker images live. They are translated to the native apptainer .sif format.\nYou can see what’s in your cache by using\napptainer cache list\nBy default the cache lives at ~/.apptainer/cache.\nIf you need to clear out the cache, you can run\napptainer cache clean\nto clear out the cache.\nThere are a number of environment variables (Section 11.5) that can be set, including login tokens for pulling from a private registry. More information is here.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Testing Scripts</span>"
    ]
  },
  {
    "objectID": "configuring.html",
    "href": "configuring.html",
    "title": "11  Appendix: Configuring your Shell",
    "section": "",
    "text": "11.1 .bashrc: Where do I put my configuration?\nThere is a file in your home directory called .bashrc. This is where you can customize the way the Bash shell behaves.\nThere are 2 things you should know how to set:",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Appendix: Configuring your Shell</span>"
    ]
  },
  {
    "objectID": "configuring.html#sec-bashrc",
    "href": "configuring.html#sec-bashrc",
    "title": "11  Appendix: Configuring your Shell",
    "section": "",
    "text": "Aliases\nEnvironment Variables, especially $PATH",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Appendix: Configuring your Shell</span>"
    ]
  },
  {
    "objectID": "configuring.html#when-does-the-bashrc-file-get-loaded",
    "href": "configuring.html#when-does-the-bashrc-file-get-loaded",
    "title": "11  Appendix: Configuring your Shell",
    "section": "11.2 When does the bashrc file get loaded?",
    "text": "11.2 When does the bashrc file get loaded?\nYou can force reloading of the",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Appendix: Configuring your Shell</span>"
    ]
  },
  {
    "objectID": "configuring.html#an-example-.bashrc-file",
    "href": "configuring.html#an-example-.bashrc-file",
    "title": "11  Appendix: Configuring your Shell",
    "section": "11.3 An example .bashrc file",
    "text": "11.3 An example .bashrc file",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Appendix: Configuring your Shell</span>"
    ]
  },
  {
    "objectID": "configuring.html#aliases",
    "href": "configuring.html#aliases",
    "title": "11  Appendix: Configuring your Shell",
    "section": "11.4 Aliases",
    "text": "11.4 Aliases\nAliases are shortcuts for commands. You can specify them using alias as a line in your .bashrc file:\nalias ll='ls -la'\nWe are defining an alias called ll that runs ls -la (long listing for directory for all files) here. Once\nSome people even add aliases for things they mistype frequently.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Appendix: Configuring your Shell</span>"
    ]
  },
  {
    "objectID": "configuring.html#sec-environment",
    "href": "configuring.html#sec-environment",
    "title": "11  Appendix: Configuring your Shell",
    "section": "11.5 Environment Variables",
    "text": "11.5 Environment Variables\nEnvironment variables are variables which can be seen globally in the Linux (or Windows) system across executables.\nYou can get a list of all set environment variables by using the env command. Here’s an example from my own system:\nenv\nSHELL=/bin/bash\nNVM_INC=/home/tladera2/.nvm/versions/node/v21.7.1/include/node\nWSL_DISTRO_NAME=Ubuntu\nNAME=2QM6TV3\nPWD=/home/tladera2\nLOGNAME=tladera2\n[....]\nOne common environment variable you may have seen is $JAVA_HOME, which is used to find the Java Software Development Kit (SDK). (I usually encounter it when a software application yells at me when I haven’t set it.)\nYou can see whether an environment variable is set using echo, such as\necho $PATH\n/home/tladera2/.local/bin:/home/tladera2/gems/bin:/home/tladera2/.nvm/versions/node/v21.7.1/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/ [....]\n\n11.5.1 Setting Environment Variables\nIn Bash, we use the export command to declare an environment variable. For example, if we wanted to declare the environment variable $SAMTOOLS_PATH we’d do the following:\n# works: note no spaces\nexport SAMTOOLS_PATH=\"/home/tladera2/miniconda/bin/\"\nOne thing to note is that spacing matters when you declare environment variables. For example, this won’t declare the $SAMTOOLS_PATH variable:\n# won't work because of spaces\nexport SAMTOOLS_PATH = \"/home/tladera2/miniconda/bin/\"\nAnother thing to note is that we declare environment variables differently than we use them. If we wanted to use SAMTOOLS_PATH in a script, we use a dollar sign ($) in front of it:\n${SAMTOOLS_PATH}/samtools view -c $input_file\nIn this case, the value of $SAMTOOLS_PATH will be expanded (substituted) to give the overall path:\n/home/tladera2/miniconda/bin/samtools view -c $input_file\n\n\n11.5.2 A Very Special Environment Variable: $PATH\nThe most important environment variable is the $PATH variable. This variable is important because it determines where to search for software executables (also called binaries). If you have softwware installed by a package manager (such as miniconda), you may need to add the location of your executables to your $PATH.\nWe can add more directories to the $PATH by appending to it. You might have seen the following bit of code in your .bashrc:\nexport PATH=$PATH:/home/tladera2/samtools/\nIn this line, we are adding the path /home/tladera2/samtools/ to our $PATH environment variable. Note that how we refer to the PATH variable is different depending on which side the variable is on of the equals sign.\n\n\n\n\n\n\nNoteOrder matters in your $PATH\n\n\n\n\n\n\nTLDR: We declare the variable using export PATH (no dollar sign) and we append to the variable using $PATH (with dollar sign). This is something that trips me up all the time.\n\n\n\n\n\n\nNoteFor FH Users\n\n\n\nIn general, when you use environment modules on gizmo, you do not need to modify your $PATH variable. You mostly need to modify it when you are compiling executables so that the system can find them. Be sure to use which to see where the environment module is actually located:\nwhich samtools\n\n\n\n\n11.5.3 Making your own environment variables\nOne of the difficulties with working on a cluster is that your scripts may be in one filesystem (/home/), and your data might be in another filesystem (/fh/fast/). And it might be recommended that you transfer over files to a faster-access filesystem (/fh/temp/) to process them.\nYou can set your own environment variables for use in your own scripts. For example, we might define a $TCR_FILE_HOME variable:\nexport TCR_FILE_HOME=/fh/fast/my_tcr_project/\nto save us some typing across our scripts. We can use this new environment variable like any other existing environment variable:\n#!/bin/Bash\nexport my_file_location=$TCR_FILE_HOME/fasta_files/\n\n\n\n\n\n\nNote.bashrc versus .bash_profile\n\n\n\nOk, what’s the difference between .bashrc and .bash_profile?\nThe main difference is when these two files are sourced. bash_profile is used when you do an interactive login, and .bashrc is used for non-interactive shells.\n.bashrc should contain the environment variables that you use all the time, such as $PATH and $JAVA_HOME for example.\nYou can get the best of both worlds by including the following line in your .bash_profile:\nsource ~/.bashrc\nThat way, everything in the .bashrc file is loaded when you log in interactively.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Appendix: Configuring your Shell</span>"
    ]
  },
  {
    "objectID": "configuring.html#sec-project",
    "href": "configuring.html#sec-project",
    "title": "11  Appendix: Configuring your Shell",
    "section": "11.6 Project/folder based workflows",
    "text": "11.6 Project/folder based workflows\nOn a particular machine, using absolute paths is safe. However, you do this at the cost of portability - code that you write on one machine may not run on another.\nIf you ever anticipate doing the analysis on a separate machine, using project structures with relative paths is the safest. For example, you may want to move from the on-premise FH system to working with the data in AWS.\nHere’s one example of putting everything into a single folder:\nmy_project\n├── data\n│   ├── chr1.fa.gz\n│   ├── chr2.fa.gz\n│   └── chr3.fa.gz\n├── results\n├── run_workflow.sh\n└── scripts\n    └── run_bowtie.sh\nIn the above example, our project is named my_project, and there are three folders inside it: data/, results/, and scripts/. Our main script for running is my_project/run_workflow.sh. Because this script is in the root folder, we can refer to the data/ folder to process files:\n./scripts/run_bowtie.sh data/*.fa.gz results/\nWhen we run run_workflow.sh, it will execute run_bowtie.sh on all of the files in data/, and save them in results/, resulting in the following updated structure.\nmy_project\n├── data\n│   ├── chr1.fa.gz\n│   ├── chr2.fa.gz\n│   └── chr3.fa.gz\n├── results\n│   ├── chr1.bam\n│   ├── chr2.bam\n│   └── chr3.bam\n├── run_workflow.sh\n└── scripts\n    └── run_bowtie.sh\nYou may have seen relative paths such as ../another_directory/ - the .. means to go up a directory in the file hierarchy, and then look in that directory for the another_directory/ directory. I try to avoid using relative paths like these.\nIn general for portability and reproducibility, you will want to use relative paths within a directory, and avoid using relative paths like ../../my_folder, where you are navigating up. In general, use relative paths to navigate down.\n\n\n\n\n\n\nNoneWhy This is Important\n\n\n\nWhen you start executing scripts, it’s important to know where the results go. When you execute SAMtools on a file in /fh/temp/, for example, where does the output go?\nWorkflow Runners such as Cromwell and Nextflow will output into certain file structures by default. This can be changed, but knowing the default behavior is super helpful when you don’t specify an output directory.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Appendix: Configuring your Shell</span>"
    ]
  },
  {
    "objectID": "miscellaneous.html",
    "href": "miscellaneous.html",
    "title": "12  Miscellaneous",
    "section": "",
    "text": "12.1 hostname What Machine am I on?\nOne of the most confusing things about working on HPC is that sometimes you have a shell open on the head node, but oftentimes, you are on a worker node.\nYour totem for telling which node you’re in is hostname, which will give you the host name of the machine you’re on.\nFor example, if I used grabnode to grab a gizmo node for interactive work, I can check which node I’m in by using:\nIf you’re confused about which node you’re in, remember hostname. It will save you from making mistakes, especially when using utilities like screen.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Miscellaneous</span>"
    ]
  },
  {
    "objectID": "miscellaneous.html#sec-hostname",
    "href": "miscellaneous.html#sec-hostname",
    "title": "12  Miscellaneous",
    "section": "",
    "text": "hostname\ngizmok164\n\n\n12.1.1 Try it out\nAfter logging into rhino, try running hostname. What host are you on?",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Miscellaneous</span>"
    ]
  },
  {
    "objectID": "miscellaneous.html#workflows",
    "href": "miscellaneous.html#workflows",
    "title": "12  Miscellaneous",
    "section": "12.2 Workflows",
    "text": "12.2 Workflows\n\n12.2.1 One Workflow: /fh/fast/ and /hpc/temp/\nOne approach is to have your scripts also live in your project folder in fast. Then, you can sync the project in /fh/fast/ over to /hpc/temp/, run the scripts in /hpc/temp/, and then sync the two folders again. You can do the file sync’ing in both directions with Motuz, which has its own advantages.\nIf you want to go this route, you should think about using a Folder Based Workflow (Section 12.10), where everything lives in a folder.\nAnother thing to consider is to have a backup of the scripts that is either on your own machine or in GitHub. You can do this by using your .gitignore to exclude the data and results.\n\n\n\n\n\ngraph LR\n    A[\"Fast\\n/fh/fast/my_lab/project/\\nRaw Data & Scripts\"] --\"a. Sync Data & scripts\"--&gt;B\n    B[\"Temp\\n/hpc/temp/my_lab/project\\nb. Run Scripts here\"] --\"c. Sync results\"--&gt;A\n\n\n\n\n\n\n\n\n12.2.2 Another Approach\nBelow is a a diagram with another way to work with these multiple filesystems.\n\nWe transfer the raw files to be processed from /fh/fast/ to our directory /fh/temp/. For example, a set of .bam files.\nWe run our scripts from /home/, on the raw files in /fh/temp/ and produce results in /fh/temp/.\nWe transfer our results from /fh/temp/ to /fh/fast/.\n\n\n\n\n\n\ngraph TD\n    A[\"Home Directory\\n/home/tladera2/\\nScripts\"] --\"b. run scripts\"--&gt; C\n    B[\"Fast\\n/fh/fast/tladera2\\nResearch Data\"] --\"a. transfer raw files\"--&gt; C\n    C[\"Temp\\n/hpc/temp/tladera2\"] --\"c. transfer results\"--&gt; B",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Miscellaneous</span>"
    ]
  },
  {
    "objectID": "miscellaneous.html#quoting-and-escaping-filenames-in-bash",
    "href": "miscellaneous.html#quoting-and-escaping-filenames-in-bash",
    "title": "12  Miscellaneous",
    "section": "12.3 Quoting and Escaping Filenames in Bash",
    "text": "12.3 Quoting and Escaping Filenames in Bash\nOne point of confusion is when do you quote things in Bash? When do you use single quotes (') versus double-quotes (\")? When do you use \\ to escape characters?\nLet’s talk about some quoting rules in Bash. I’ve tried to make things as simplified and generalized as possible, rather than stating all of the rules for each quote.\n\nIf you have spaces in a filename, use double quotes (\"chr 1.bam\")\nIf you have a single quote in the filename, use double quotes to wrap it (\"ted's file.bam\")\nOnly escape characters when necessary - if you can solve a problem with quotes, use them\nIf you need to preserve an escaped character, use single quotes\n\nLet’s go over each of these with an example.\n\n12.3.1 If you have spaces in a filename, use double quotes (Most common)\nFor example, if your filename is chr 1 file.bam, then use double quotes in your argument\nsamtools view -c \"chr 1 file.bam\"\n\n\n12.3.2 If you have a single quote in the name, use double quotes to wrap it (less common)\nSay you have a file called ted's new file.bam. This can be a problem when you are calling it, especially because of the single quote.\nIn this case, you can do this:\nsamtools view -c \"ted's new file.bam\"\n\n\n12.3.3 Only escape characters when necessary (less common)\nThere are a number of special characters (such as Tab, and Newline) that can be specified as escape characters. In double quotes, characters such as $ are signals to Bash to expand or evaluate code.\nSay that someone had a $ in their file name such as Thi$file is money.bam\nHow do we refer to it? We can escape the character with a backslash \\:\nsamtools view -c \"Thi\\$file is money.bam\"\nThe backslash is a clue to Bash that we don’t want variable expansion in this case. Without it, bash would look for a variable called $file.\n\n\n12.3.4 If you need to preserve an escaped character, use single quotes (least common)\nThis is rarely used, but if you need to keep an escaped character in your filename, you can use single quotes. Say we have a filename called Thi\\$file.bam and you need that backslash in the file name (btw, please don’t do this), you can use single quotes to preserve that backslash:\nsamtools view -c 'Thi\\$file.bam'\nAgain, hopefully you won’t need this.\n\n\n12.3.5 For More Info\nhttps://www.grymoire.com/Unix/Quote.html#uh-3\n\n\n\n\n\n\nNoteWhat about backticks?\n\n\n\nBackticks (`) are an old way to do command evaluation in Bash. For example, if we run the following on the command-line:\necho \"there are `ls -l | wc -l` files in this directory\"\nWill produce:\nthere are       36 files in this directory\nTheir use is deprecated, so you should be using $() in your command evaluations instead:\necho \"there are $(ls -l | wc -l) files in this directory\"\n\n\n\n\n\n\n\n\nNoteWhat about X use case?\n\n\n\nThere are a lot of rules for Bash variable expansion and quoting that I don’t cover here. I try to show you a way to do things that work in multiple situations on the cluster.\nThat’s why I focus on double quotes for filenames and ${} for variable expansion in general. They will work whether your Bash script is on the command line or in an App, or in WDL.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Miscellaneous</span>"
    ]
  },
  {
    "objectID": "miscellaneous.html#using-pipes-stdin-stdout-stderr",
    "href": "miscellaneous.html#using-pipes-stdin-stdout-stderr",
    "title": "12  Miscellaneous",
    "section": "12.4 Using pipes: STDIN, STDOUT, STDERR",
    "text": "12.4 Using pipes: STDIN, STDOUT, STDERR\nWe will need to use pipes to chain our commands together. Specifically, we need to take a command that generates a list of files on the cluster shared filesystem, and then spawns individual jobs to process each file. For this reason, understanding a little bit more about how pipes (|) work in Bash is helpful.\nIf we want to understand how to chain our scripts together into a pipeline, it is helpful to know about the different streams that are available to the utilities.\n\n\n\n\n\n\n\n\ngraph LR\n  A(STDIN) --&gt; E[run_samtools.sh]\n  E --&gt; B(STDOUT)\n  E --&gt; C(STDERR)\n\n\n\n\n\n\n\n\nFigure 12.1: Inputs/outputs to a script\n\n\n\nEvery script has three streams available to it: Standard In (STDIN), Standard Out (STDOUT), and Standard Error (STDERR) (Figure 12.1).\nSTDIN contains information that is directed to the input of a script (usually text output via STDOUT from another script).\nWhy do these matter? To work in a Unix pipeline, a script must be able to utilize STDIN, and generate STDOUT, and STDERR.\nSpecifically, in pipelines, STDOUT of a script (here it’s run_samtools) is directed into STDIN of another command (here wc, or word count)\n\n\n\n\n\n\n\n\ngraph LR\n  E[run_samtools.sh] --&gt; B(STDOUT)\n  B --&gt; F{\"|\"}\n  E --&gt; C(STDERR)\n  F --&gt; D(\"STDIN (wc)\")\n  D --&gt; G[wc]\n\n\n\n\n\n\n\n\nFigure 12.2: Piping a script run_samtools.sh into another command (wc)\n\n\n\nWe will mostly use STDOUT in our bash scripts, but STDERR can be really helpful in debugging what’s going wrong.\n\n\n\n\n\n\nNoteWhy this is important on the Cluster\n\n\n\nWe’ll use pipes and pipelines not only in starting a bunch of jobs using batch scripting on our home computer, but also when we are processing files within a job.\n\n\n\n12.4.1 For more info about pipes and pipelines\nhttps://swcarpentry.github.io/shell-novice/04-pipefilter/index.html https://datascienceatthecommandline.com/2e/chapter-2-getting-started.html?q=stdin#combining-command-line-tools",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Miscellaneous</span>"
    ]
  },
  {
    "objectID": "miscellaneous.html#basename-can-be-very-handy-when-on-workers",
    "href": "miscellaneous.html#basename-can-be-very-handy-when-on-workers",
    "title": "12  Miscellaneous",
    "section": "12.5 basename can be very handy when on workers",
    "text": "12.5 basename can be very handy when on workers\nIf we are processing a bunch of files on a worker, we need a way to get the bare filename from a path. We will take advantage of this when we run process multiple files on the worker.\nFor example:\nbasename /mnt/project/worker_scripts/srun-script.sh\nThis will return:\nsrun-script.sh\nWhich can be really handy when we name our outputs. This command is so handy it is used in WDL.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Miscellaneous</span>"
    ]
  },
  {
    "objectID": "miscellaneous.html#sec-xargs",
    "href": "miscellaneous.html#sec-xargs",
    "title": "12  Miscellaneous",
    "section": "12.6 Advanced Batch Processing: Iterating using xargs",
    "text": "12.6 Advanced Batch Processing: Iterating using xargs\nA really common pattern is taking a delimited list of files and doing something with them. We can do some useful things such as seeing the first few lines of a set of files, or doing some sort of processing with the set of jobs.\n\n\n\n\n\n\nWarningDon’t xargs for HPC jobs\n\n\n\nYou might be tempted to use xargs with srun to work on a bunch of files. It’s worth trying once so you can see the mechanics of how jobs are processed.\nIn general, I don’t recommend it in practice because if you spawn 1000 jobs using xargs, there’s no real mechanism to terminate that 1000 jobs, except one by one. With sbatch, all your jobs in batch mode run as subjobs, which means you can terminate the parent job to terminate all of the subjobs.\nAgain, this is a good reason to use a workflow runner in your day to day work. You don’t have to worry about jobs and subjobs. It takes a little setup, but it will make your life easier in general.\n\n\nLet’s start out with a list of files:\nsource ~/.bashrc #| hide_line\nls data/*.sh\ndata/batch-on-worker.sh\nNow we have a list of files, let’s look at the first few lines of each of them, and print a separator --- for each.\n#| filename: scripting-basics/xargs_example.sh\nsource ~/.bashrc #| hide_line\nls data/*.sh | xargs -I% sh -c 'head %; echo \"\\n---\\n\"'\n#!/bash/bin\n\ncmd_to_run=\"ls *.vcf.gz | xargs -I% sh -c \"bcftools stats % &gt; %.stats.txt\"\n\ndx run swiss-army-knife \\\n  -iin=\"data/chr1.vcf.gz\" \\\n  -iin=\"data/chr2.vcf.gz\" \\\n  -iin=\"data/chr3.vcf.gz\" \\\n  -icmd=${cmd_to_run}\n---\ndx find data --name \"*.bam\" --brief\n---\nLet’s take this apart piece by piece.\nxargs takes an -I argument that specifies a placeholder. In our case, we are using % as our placeholder in this statement.\nWe’re passing on each filename from ls into the following code:\nsh -c 'head %; echo \"---\\n\"'\nThe sh -c opens a subshell so that we can execute our command for each of the files in our list. We’re using sh -c to run:\n'head %; echo \"---\\n\"'\nSo for our first file, 01-scripting-basics.qmd, we are substituting that for % in our command:\n'head hpc-basics.qmd; echo \"---\\n\"'\nFor our second file, hpc-basics.qmd, we would substitute that for the %:\n'head hpc-basics.qmd; echo \"---\\n\"'\nUntil we cycle through all of the files in our list.\n\n12.6.1 The Basic xargs pattern\n\n\n\n\n\n\n\n\ngraph LR\n  A[\"ls *.bam\"] --&gt; B{\"|\"} \n  B --&gt; C[\"xargs -I% sh -c\"] \n  C --&gt; D[\"command_to_run %\"]\n\n\n\n\n\n\n\n\nFigure 12.3: Basics of using xargs to iterate on a list of files\n\n\n\nAs you cycle through lists of files, keep in mind this basic pattern (Figure 12.3):\nls &lt;wildcard&gt; | xargs -I% sh -c \"&lt;command to run&gt; %\"\n\n\n\n\n\n\nNoteTest Yourself\n\n\n\nHow would we modify the below code to do the following?\n\nList only .json files in our data/ folder using ls\nUse tail instead of head\n\nls *.txt | xargs -I% sh -c \"head %; echo '---\\n'\"\n\n\n\n\n\n\n\n\nNoteAnswer\n\n\n\n\n\nls data/*.json | xargs -I% sh -c \"tail %; echo '---\\n'\"\n\n\n\n\n\n\n\n\n\nNoteWhy this is important on HPC\n\n\n\nWe can use xargs to execute small batch jobs on a small number of files. This especially becomes powerful on the cluster when we use ls to list files in our HPC project.\nNote that as we graduate to workflow tools like WDL/Nextflow, there are other mechanisms for running jobs on multiple files (such as WDL/Cromwell) that we should move to.\nTrust me; you don’t want to have to handle iterating through a huge directory and handling when routines give an error, or your jobs get interrupted. Rerunning and resuming failed jobs are what workflow runner tools excel at.\n\n\n\n\n12.6.2 For more information\nhttps://www.baeldung.com/linux/xargs-multiple-arguments",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Miscellaneous</span>"
    ]
  },
  {
    "objectID": "miscellaneous.html#sec-screen",
    "href": "miscellaneous.html#sec-screen",
    "title": "12  Miscellaneous",
    "section": "12.7 screen or tmux: keep your session open",
    "text": "12.7 screen or tmux: keep your session open\n\nOftentimes, when you are running something interactive on a system, you’ll have to leave your shell open. Otherwise, your running job will terminate.\nYou can use screen or tmux, which are known as window managers, to keep your sessions open on a remote machine. We’ll talk about screen.\n\nscreen works by starting a new bash shell. You can tell this because your bash prompt will change.\nThe key of working remotely with screen is that you can then request an hpc node.\nFor FH users, you can request a gizmo node using grabnode. We can then check we’re on the gizmo node by using hostname.\nIf we have something running on this node, we can keep it running by detaching the screen session. Once we are detached, we should check that we’re back in rhino by using hostname. Now we can log out and our job will keep running.\n\nIf we need to get back into that screen session, we can use:\nscreen -ls\nTo list the number of sessions:\nThere is a screen on:\n        37096.pts-321.rhino01   (05/10/2024 10:21:54 AM)        (Detached)\n1 Socket in /run/screen/S-tladera2.\nOnce we’ve found the id for our screen session (in this case it’s 37096), we can reattach to the screen session using:\nscreen -r 37096\nAnd we’ll be back in our screen session! Handy, right?\n\n\n\n\n\n\nNoteFor FH Users\n\n\n\nNote that if you logout from rhino, you’ll need to log back into the same rhino node to access your screen session.\nFor example, if my screen session was on rhino01, I’d need to ssh back into rhino01, not rhino02 or rhino03. This means you will need to ssh into rhino01 specifically to get back into your screen session.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Miscellaneous</span>"
    ]
  },
  {
    "objectID": "miscellaneous.html#using-file-manifests",
    "href": "miscellaneous.html#using-file-manifests",
    "title": "12  Miscellaneous",
    "section": "12.8 Using file manifests",
    "text": "12.8 Using file manifests\nOne approach that I use a lot is using file manifests to process multiple sets of files. Each line of the file manifest will contain all of the related files I need to process.\nFor example, if I am aligning paired-end reads, then I can have a tab-separated column for the first strand, and a column for the second strand.\nsample1_1.fq   sample1_2.fq\nsample2_1.fq   sample2_2.fq\nThe one trick with using file manifests in bash is that we need to change the what’s called the internal field separator (IFS), which specifies how to split up a string with a for loop. By default, bash uses an IFS of ” ” (a space), which means that the for loop will cycle through words (strings separated by spaces), instead of lines.\nWe can change this behavior by setting the IFS at the beginning of our script:\nOUTPUT_FOLDER=\".\"\nFASTA_INDEX_LOCATION=\"/shared/biodata/reference/iGenomes/Homo_sapiens/UCSC/hg19/Sequence/BWAIndex/genome.fa\"\n1for file in (cat manifest.txt)\n2IFS=\"\"\ndo\n3  read1=$(echo $file | awk '{print $1}')\n4  read2=$(echo $file | awk '{print $2}')\n  bwa mem ${FASTA_INDEX_LOCATION} ${read1} ${read2} &gt; ${OUTPUT_FOLDER}/${read1}.sam \ndone\n\n5unset IFS\n\n1\n\nCycle through each line in manifest.txt\n\n2\n\nChange IFS to be \"\" (no space), to process a file line by line.\n\n3\n\nGrab the word in the 1st position and assign it to $read1\n\n4\n\nGrab the word in the 2nd position and assign it to $read2\n\n5\n\nReset IFS to original behavior.\n\n\nThe line:\nread1=$(echo $file | awk '{print $1}') \nTakes some unpacking. The first thing to notice is that we’re assigning something into the $read1 variable.\nThis part:\n$(echo $file | awk '{print $1}')\nputs $file through a command called awk that lets us extract words of a string by position. In our case, our different parts of the string are separated by tabs.\nThe $(   ) part lets us run a subshell and then return the value of the output.\nThis part:\n'{print $1}'\nAre instructions to awk to output the first word of our string. awk is an extremely powerful command, but we don’t have enough time to cover it in full. Julia Evans has a great page on it in Bite Size Command Line.\nSo, when we put it together, the command assigns $read1 the value of the first position (sample1_1.fq), letting us do things with this file path.\nWe’ll see that you can also use file manifests with workflow runners such as Cromwell.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Miscellaneous</span>"
    ]
  },
  {
    "objectID": "miscellaneous.html#removing-file-extensions-with-variable-expansion",
    "href": "miscellaneous.html#removing-file-extensions-with-variable-expansion",
    "title": "12  Miscellaneous",
    "section": "12.9 Removing file extensions with variable expansion",
    "text": "12.9 Removing file extensions with variable expansion\nOftentimes, when you are processing a file, you want to generate a new file name with the same basename, but not the same extension. For example, you might want to generate a mysamfile.txt from a file callled mysamefile.sam.\nYou can use \"${filename%.sam}\" to remove the .sam from the file name.\n#/bin/bash\noutput_file=\"${1%.sam}.txt\"\necho $output_file\nIf you ran this script with\n./run_output.sh mybamfile.sam\nYou’ll get the output:\nmybamfile.txt\nThis is a very helpful trick. Note that it only works if the string to remove (the file extension) is a suffix at the end of the filepath. There is a similar trick using # if you want to remove the prefix of a file name.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Miscellaneous</span>"
    ]
  },
  {
    "objectID": "miscellaneous.html#sec-project",
    "href": "miscellaneous.html#sec-project",
    "title": "12  Miscellaneous",
    "section": "12.10 Project/folder based workflows",
    "text": "12.10 Project/folder based workflows\nOn a particular machine, using absolute paths is safe. However, you do this at the cost of portability - code that you write on one machine may not run on another.\nIf you ever anticipate doing the analysis on a separate machine, using project structures with relative paths is the safest. For example, you may want to move from the on-premise FH system to working with the data in AWS.\nHere’s one example of putting everything into a single folder:\nmy_project\n├── data\n│   ├── chr1.fa.gz\n│   ├── chr2.fa.gz\n│   └── chr3.fa.gz\n├── results\n├── run_workflow.sh\n└── scripts\n    └── run_bowtie.sh\nIn the above example, our project is named my_project, and there are three folders inside it: data/, results/, and scripts/. Our main script for running is my_project/run_workflow.sh. Because this script is in the root folder, we can refer to the data/ folder to process files:\n./scripts/run_bowtie.sh data/*.fa.gz results/\nWhen we run run_workflow.sh, it will execute run_bowtie.sh on all of the files in data/, and save them in results/, resulting in the following updated structure.\nmy_project\n├── data\n│   ├── chr1.fa.gz\n│   ├── chr2.fa.gz\n│   └── chr3.fa.gz\n├── results\n│   ├── chr1.bam\n│   ├── chr2.bam\n│   └── chr3.bam\n├── run_workflow.sh\n└── scripts\n    └── run_bowtie.sh\nYou may have seen relative paths such as ../another_directory/ - the .. means to go up a directory in the file hierarchy, and then look in that directory for the another_directory/ directory. I try to avoid using relative paths like these.\nIn general for portability and reproducibility, you will want to use relative paths within a directory, and avoid using relative paths like ../../my_folder, where you are navigating up. In general, use relative paths to navigate down.\n\n\n\n\n\n\nNoneWhy This is Important\n\n\n\nWhen you start executing scripts, it’s important to know where the results go. When you execute SAMtools on a file in /fh/temp/, for example, where does the output go?\nWorkflow Runners such as Cromwell and Nextflow will output into certain file structures by default. This can be changed, but knowing the default behavior is super helpful when you don’t specify an output directory.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Miscellaneous</span>"
    ]
  }
]