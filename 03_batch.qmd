---
title: "Batch Processing and Submitting Jobs"
---

## Learning Objectives

- **Execute** a script to run over a list of files on one system
- **Utilize** globs to specify multiple files in your script
- **Batch Process** files to run locally and on the HPC cluster

## Using `for` loops to cycle through files

A very common pattern is cycling through multiple files in a folder and applying the same script or command to them. 

There is a simple method for batch processing a bunch of files: a `for` loop. In our case, a for loop takes a list of file paths (such as a list of FASTA files we want to process), and performs the same task for each element of the list.

```bash
for file in 01_assignment.qmd 02_scripting.qmd  #<1>
do                                              #<2>
  wc $file                                      #<3>
done                                            #<4>
```
1. Cycle through the list of `01_assignment.qmd` and `02_scripting.qmd`.
2. Start of instructions
3. Count the words in each `.qmd` file using `wc`
4. End of instructions

The `do` and `done` sandwich the instructions we want to do to each file in our list. `wc` will produce a word count of these two files.

If we run this in the `bash_for_bio/` folder, we'll get the following:

```
      26      96     656 01_assignment.qmd
     493    2609   16206 02_scripting.qmd
```

## globs: selecting multiple files

However, typing each element of our list is going to be difficult - can we select files in a different way?

We can use `globs`, or wildcard characters to select multiple files with a criteria.

For example, `*.qmd` will list all of the `.qmd` files in the `bash_for_bio` directory. 

```bash
#!/bin/bash
for file in *.qmd #<1>
do                #<2>
  wc $file        #<3>
done              #<4>
```
1. Start `for` loop and cycle through all `.qmd` files
2. Start of instructions
3. Count the words in each `.qmd` file using `wc`
4. End of instructions

If we run this in our repository, we get something similar to this:

```
      22      92     634 01_assignment.qmd
     462    2417   15396 01_basics.qmd
       5       8      49 02_assignment.qmd
     303    1577    9509 02_scripting.qmd
     303    1667   10233 03_batch.qmd
     198    1368    9027 04_containers_workflows.qmd
     205    1151    7858 configuring.qmd
      53     363    2516 index.qmd
       9      25     157 intro.qmd
     214    1314    8001 miscellaneous.qmd
```

The `*.qmd` (the wildcard operator, also known as a {{<glossary glob>}}) can be used in various ways. For example, if our files are in a folder called `raw_data/`, we could specify:

```
for file in ./data/*.fastq
do
  wc $file 
done
```

:::{.callout}
## A common pattern: a folder with only one type of file in them

One thing that makes it easier to process a bunch of files is to have the data be in the same folder, with nothing else in them.

For example, I might create a `fastq/` folder where I store my data, so I can pass the glob `fastq/*` to process the files in that.

```bash
#!/bin/bash
module load BWA
module load SAMtools
FASTA_LOCATION=""
OUTPUT_FOLDER="/hpc/temp/my_lab/project_x/bam_files/"
for file in fastq/*
do
  bwa mem ${FASTA_LOCATION} file > ${OUTPUT_FOLDER}/${file}.bam 
  samtools sort ${OUTPUT_FOLDER}/${file}.bam |> ${OUTPUT_FOLDER}/${file}.sorted.bam
done
module purge
```

:::

### For more info on globs

See page 12 in Bite Size Bash.

:::{.callout}
## Selecting files with complicated patterns: Regular Expressions

At some point, globs are going to be inadequate depending on how you store files. At that point, you will probably need to learn regular expressions, which is a much more powerful way of describing search patterns. 

I will be honest and say this is one thing LLMs are very good at. But you should have the vocabulary to prompt them, including:

1. Literal Characters
2. Metacharacters (including how to escape special characters)
3. Character classes (specifying groups of characters to match)
4. Capture Groups, including named capture groups
5. Quantifiers
6. Logical expressions
7. Anchors (matching positions)

Knowing these general concepts will help you with your LLM prompts. And always test your regular expressions to make sure that they are capturing the file patterns you expect.
:::

## Batching on HPC

Now we can start to do more advanced things on the HPC cluster: use one machine to process each file. We will use a slightly different mechanism to cycle through files: the {{<glossary "job array">}}. 

Let's start out with {{<glossary SLURM>}} scripts. 

### SLURM Scripts

SLURM scripts are a special kind of shell script that contain additional information for the SLURM manager. This includes:

1. Number of nodes (machines) to request
2. Memory and CPU requirements for each machine

We specify these using a special kind of comment: SLURM directives. Directives begin a line with `#SBATCH `:

```
#SBATCH --nodes=1 
```
In this example, we are specifying the number of nodes. 

Note that because directives begin with a `#` - they are treated like comments by `bash`, but are usable by `SLURM`.n 

### SLURM Directives

We are able to set some configuration on running our jobs. 

```bash
#| eval: false
#| filename: scripts/week3/sbatch_test.sh
#!/bin/bash
#SBATCH --nodes=3 #<1>
#SBATCH --array=1-3 #<2>
echo "${SLURM_ARRAY_TASK_ID} job" #<3>
```
1. request 1 node
2. start an array

:::{.callout}
## More about directives

Much more information about the kinds of directives that you can specify in a SLURM script is available here: <https://www.osc.edu/supercomputing/batch-processing-at-osc/slurm_directives_summary>

The most important directive you should be aware of is how many nodes you need to request, and how much memory your jobs will need.
:::

### Job Arrays

This line:

```bash
#SBATCH --array=1-3 
```

Will create a job array. This will create a variable called `$SLURM_ARRAY_TASK_ID` that will cycle through the numbers 1-3. Each Task ID corresponds to a different subjob. Let's try a simpler script to show what's going on:

```bash
#| eval: false
#| filename: sbatch_test.sh
#!/bin/bash
#SBATCH --array=1-3
#SBATCH --nodes=1
echo "${SLURM_ARRAY_TASK_ID} job"
```

This is a minimal script that will execute 3 subjobs. It will cycle through the job array and print the array number for each job.

```bash
#| eval: false
sbatch sbatch_test.sh
```

On submitting, we will get a message like this (your job number will be different):

```
Submitted batch job 26328834
```

This will run very quickly on the three nodes. And if we look for the output files:

```bash
ls -l slurm-26328834*
```

We will get the following output:

```
-rw-rw---- 1 tladera2 g_tladera2 8 Jul 15 13:50 slurm-26328834_1.out
-rw-rw---- 1 tladera2 g_tladera2 8 Jul 15 13:50 slurm-26328834_2.out
-rw-rw---- 1 tladera2 g_tladera2 8 Jul 15 13:50 slurm-26328834_3.out
```

Taking a look at one of these files using `cat`:

```bash
cat slurm-26328834_3.out
```

We'll see this:

```
3 job
```

```{mermaid}
graph TD
  A["sbatch sbatch_test.sh"] --"1"--> B
  B["echo 1 job"]
  A --"2"--> C["echo 2 job"]
  A --"3"--> D["echo 3 job"]
```

What happened here? `sbatch` submitted our job array as 3 different subjobs to 3 different nodes under a single job id. Each node then outputs a file with the subjob id that contains the job number. 

### Processing lists of files using Job Arrays

So now we know that `${SLURM_ARRAY_TASK_ID}` will let us specify a subjob within our script, how do we use it in our script?

Say, we have a list of 3 files in our directory `data/`, and we can list them by using `../../data/*.fastq`. We can use the `${SLURM_ARRAY_TASK_ID}` as an index to specify a different file.

The one caveat is that we need to know the number of files beforehand. 

This script will run our `run_bwa.sh` on 3 separate files on 3 separate nodes:

```bash
#| eval: false
#| filename: scripts/week3/run_sbatch.sh
#!/bin/bash
#SBATCH --nodes=3 # <1>
#SBATCH --array=1-3 # <2>
#SBATCH --mem-per-cpu=1gb # <3>
#SBATCH --time=00:10:00 # <4>
file_array=(../../data/*.fastq) #<5>
ind=$((SLURM_ARRAY_TASK_ID-1)) #<6>
current_file=${file_array[$ind]} #<7>
./run_bwa.sh $current_file
```
1. Initialize job array to range from 1 to 10
2. list all files in `../..data/` with extension `.fastq`. Assign it to the array `$file_array`.
3. For current task id, calculate the appropriate index (we have to subtract 1 because bash arrays begin at `0`)
4. Pull the current file path given the task id, assign it to `$current_file`.
5. Run `run_bwa.sh` on `$current_file`.

We run this on `rhino` using this command (we are in `~/bash_for_bio/scripts/week3/`)

```bash
sbatch sbatch_bwa.sh
```

We'll get the response:

```
Submitted batch job 35300989
```

If we take a look at the job queue using `squeue`, we'll get something like this:

```bash
squeue -u tladera2
```

```
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
        35300989_1 campus-ne sbatch_b tladera2  R       0:10      1 gizmok34
        35300989_2 campus-ne sbatch_b tladera2  R       0:10      1 gizmok40
        35300989_3 campus-ne sbatch_b tladera2  R       0:10      1 gizmok79
```

We can see our three subjobs, which are indicated by the Job IDs `35300989_1`, `35300989_2`, and `35300989_3`.

```bash
ladera2@rhino02:~/bash_for_bio/scripts/week3$ ls -l
```

```
total 312
-rw-rw---- 1 tladera2 g_tladera2 211895 Sep 26 10:29 CALU1_combined_final.sam
-rw-rw---- 1 tladera2 g_tladera2 160489 Sep 26 10:29 HCC4006_final.sam
-rw-rw---- 1 tladera2 g_tladera2 121509 Sep 26 10:29 MOLM13_combined_final.sam
-rwxrwxrwx 1 tladera2 g_tladera2    590 Sep 26 09:45 run_bwa.sh
-rwxrwxrwx 1 tladera2 g_tladera2    256 Sep 26 10:28 run_sbatch.sh
-rw-rw---- 1 tladera2 g_tladera2    614 Sep 26 10:29 slurm-35300992_1.out
-rw-rw---- 1 tladera2 g_tladera2    579 Sep 26 10:29 slurm-35300992_2.out
-rw-rw---- 1 tladera2 g_tladera2    619 Sep 26 10:29 slurm-35300992_3.out
```

And you'll see we generated our SAM files for each sample! Neat. There are also the `.out` files from each subjob as well, which will contain the <STDOUT> for each subjob. 

You can see that we outputted our files to the `scripts/week3/` directory. It's part of your job in the exercises to adapt `run_sbatch.sh` and `run_bwa.sh` to output to a directory of your choosing.

### `scancel`ing a jobarray

As we noted, one of the strengths in using a job array to process multiple files is that they are spawned as *sub* or *child* jobs of a parent job id. 

What if we made a mistake? We can use the `scancel` command to cancel the entire set of jobs by giving it our parent job id:

```bash
scancel 26328834
```

This will cancel all sub jobs related to the parent job. No fuss, no muss.

## What's Next?

Next week we will discuss using `MiniWDL` (a workflow manager) to process files through multi-step workflows in {{<glossary WDL>}}.

