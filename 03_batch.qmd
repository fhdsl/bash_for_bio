---
title: "Batch Processing and Submitting Jobs"
---

## Learning Objectives

- **Execute** a script to run over a list of files on one system
- **Utilize** globs to specify multiple files in your script
- **Batch Process** files to run locally and on the HPC cluster

## Using `for` loops to cycle through files

A very common pattern is cycling through multiple files in a folder and applying the same script or command to them. 

There is a simple method for batch processing a bunch of files: a `for` loop. In our case, a for loop takes a list of file paths (such as a list of FASTA files we want to process), and performs the same task for each element of the list.

```bash
for file in 01_assignment.qmd 02_scripting.qmd  #<1>
do                                              #<2>
  wc $file                                      #<3>
done                                            #<4>
```
1. Cycle through the list of `01_assignment.qmd` and `02_scripting.qmd`.
2. Start of instructions
3. Count the words in each `.qmd` file using `wc`
4. End of instructions

The `do` and `done` sandwich the instructions we want to do to each file in our list. `wc` will produce a word count of these two files.

If we run this in the `bash_for_bio/` folder, we'll get the following:

```
      26      96     656 01_assignment.qmd
     493    2609   16206 02_scripting.qmd
```

## globs: selecting multiple files

However, typing each element of our list is going to be difficult - can we select files in a different way?

We can use `globs`, or wildcard characters to select multiple files with a criteria.

For example, `*.qmd` will list all of the `.qmd` files in the `bash_for_bio` directory. 

```bash
#!/bin/bash
for file in *.qmd #<1>
do                #<2>
  wc $file        #<3>
done              #<4>
```
1. Start `for` loop and cycle through all `.qmd` files
2. Start of instructions
3. Count the words in each `.qmd` file using `wc`
4. End of instructions

If we run this in our repository, we get something similar to this:

```
      22      92     634 01_assignment.qmd
     462    2417   15396 01_basics.qmd
       5       8      49 02_assignment.qmd
     303    1577    9509 02_scripting.qmd
     303    1667   10233 03_batch.qmd
     198    1368    9027 04_containers_workflows.qmd
     205    1151    7858 configuring.qmd
      53     363    2516 index.qmd
       9      25     157 intro.qmd
     214    1314    8001 miscellaneous.qmd
```

The `*.qmd` (the wildcard operator, also known as a {{<glossary glob>}}) can be used in various ways. For example, if our files are in a folder called `raw_data/`, we could specify:

```
for file in raw_data/*.fq
do
  bwa mem 
done
```

:::{.callout}
## A common pattern: a folder with only one type of file in them

One thing that makes it easier to process a bunch of files is to have the data be in the same folder, with nothing else in them.

For example, I might create a `fastq/` folder where I store my data, so I can pass the glob `fastq/*` to process the files in that.

```bash
#!/bin/bash
module load BWA
module load SAMtools
FASTA_LOCATION=""
OUTPUT_FOLDER="/hpc/temp/my_lab/project_x/bam_files/"
for file in fastq/*
do
  bwa mem ${FASTA_LOCATION} file > ${OUTPUT_FOLDER}/${file}.bam 
  samtools sort ${OUTPUT_FOLDER}/${file}.bam |> ${OUTPUT_FOLDER}/${file}.sorted.bam
done
module purge
```

:::

### For more info on globs

See page 12 in Bite Size Bash.

:::{.callout}
## Selecting files with complicated patterns: Regular Expressions

At some point, globs are going to be inadequate depending on how you store files. At that point, you will probably need to learn regular expressions, which is a much more complex way of describing search patterns. 
:::

## Batching on HPC

Now we can start to do more advanced things on the HPC cluster: use one machine to process each file. We will use a slightly different mechanism to cycle through files: the {{<glossary "job array">}}. 

Let's start out with {{<glossary SLURM>}} scripts. 

### SLURM Scripts

SLURM scripts are a special kind of shell script that contain additional information for the SLURM manager. This includes:

1. Number of nodes (machines) to request
2. Memory and CPU requirements for each machine

We specify these using a special kind of comment: SLURM directives. Directives begin a line with `#SBATCH `:

```
#SBATCH --nodes=1 
```
In this example, we are specifying the number of nodes. 

### SLURM Directives

We are able to set some configuration on running our jobs. 

```bash
#!/bin/bash
#SBATCH --nodes=1 # <1>
#SBATCH --array=1-3 # <2>
#SBATCH --mem-per-cpu=1gb # <3>
#SBATCH --time=00:05:00 # <4>
./samtools_opt sort SRR1576820_000${SLURM_ARRAY_TASK_ID}.bam -o SRR1576820_000${SLURM_ARRAY_TASK_ID}.sorted.bam # <5>
```
1. request 1 node
2. start an array
3. request 1 gigabyte per cpu
4. ask for 5 minutes on the node
6. Run `samtools sort` on a bam file, and output it (will do for the whole job array)

:::{.callout}
## More about directives

Much more information about the kinds of directives that you can specify in a SLURM script is available here: <https://www.osc.edu/supercomputing/batch-processing-at-osc/slurm_directives_summary>

The most important directive you should be aware of is how 
:::

### Job Arrays

This line:

```bash
#SBATCH --array=1-3 
```

Will create a job array. This will create a variable called `$SLURM_ARRAY_TASK_ID` that will cycle through the numbers 1-3. Each Task ID corresponds to a different subjob. Let's try a simpler script to show what's going on:

```bash
#| eval: false
#| filename: sbatch_test.sh
#!/bin/bash
#SBATCH --array=1-3
#SBATCH --nodes=1
echo "${SLURM_ARRAY_TASK_ID} job"
```

This is a minimal script that will execute 3 subjobs. It will cycle through the job array and print the array number for each job.

```bash
#| eval: false
sbatch sbatch_test.sh
```

On submitting, we will get a message like this (your job number will be different):

```
Submitted batch job 26328834
```

And if we look for the output files:

```bash
ls -l slurm-26328834*
```

We will get the following output:

```
-rw-rw---- 1 tladera2 g_tladera2 8 Jul 15 13:50 slurm-26328834_1.out
-rw-rw---- 1 tladera2 g_tladera2 8 Jul 15 13:50 slurm-26328834_2.out
-rw-rw---- 1 tladera2 g_tladera2 8 Jul 15 13:50 slurm-26328834_3.out
```

Taking a look at one of these files using `cat`:

```bash
cat slurm-26328834_3.out
```

We'll see this:

```
3 job
```

```{mermaid}
graph TD
  A["sbatch sbatch_test.sh"] --"1"--> B
  B["echo 1 job"]
  A --"2"--> C["echo 2 job"]
  A --"3"--> D["echo 3 job"]
```

What happened here? `sbatch` submitted our job array as 3 different subjobs to 3 different nodes under a single job id. Each node then outputs a file with the subjob id that contains the job number. 

### Processing lists of files using Job Arrays

So now we know that `${SLURM_ARRAY_TASK_ID}` will let us specify a subjob within our script, how do we use it in our script?

Say, we have a list of 10 files in a directory `fastq/`, and we can list them by using `fastq/*.fq`. We can use the `${SLURM_ARRAY_TASK_ID}` as an index to specify a different file.

The one caveat is that we need to know the number of files beforehand:

```bash
#| eval: false
#| filename: sbatch_loop.sh
#!/bin/bash
#SBATCH --array=1-10
#SBATCH --nodes=1
module load SAMtools
file_array=($(ls fastq/*.fq))            #<1>
ind=$((SLURM_ARRAY_TASK_ID-1))     #<2>
current_file = $file_array[$ind]   #<3>
samtools view -c $current_file > "${current_file%sam}.txt" #<4>
```
1. list all files in `fastq/` with extension `.fq`. Assign it to the array `$file_array`
2. For current task id, calculate the appropriate index (we have to subtract 1 because bash arrays begin at `0`)
3. Pull the current file path given the task id, assign it to `$current_file`.
4. Run `samtools view -c` on `$current_file`

### `scancel`ing a job array

As we noted, one of the strengths in using a job array to process multiple files is that they are spawed as *sub* or *child* jobs of a parent job id. 

What if we made a mistake? We can use the `scancel` command to cancel the entire set of jobs by giving it our parent job id:

```bash
scancel 26328834
```

This will cancel all sub jobs related to the parent job. No fuss, no muss.


## What's Next

Next week we will discuss using `cromwell` (a workflow manager) to process files through multi-step workflows in {{<glossary WDL>}}

