---
title: "Batch Processing and Submitting Jobs"
---

## Batch Processing Basics: Iterating using `xargs` {#sec-xargs}

A really common pattern is taking a delimited list of files and doing something with them. We can do some useful things such as seeing the first few lines of a set of files, or doing some sort of processing with the set of jobs.

:::{.callout-warning}
## Don't `xargs` for HPC jobs
  
You might be tempted to use `xargs` with `srun` to work on a bunch of files. It's worth trying once so you can see the mechanics of how jobs are processed.

In general, I don't recommend it in practice because if you spawn 1000 jobs using `xargs`, there's no real mechanism to terminate that 1000 jobs, except one by one. With `sbatch`, all your jobs in batch mode run as *subjobs*, which means you can terminate the parent job to terminate all of the subjobs.

Again, this is a good reason to use a workflow runner in your day to day work. You don't have to worry about jobs and subjobs. It takes a little setup, but it will make your life easier in general.
:::


Let's start out with a list of files:

```bash
source ~/.bashrc #| hide_line
ls data/*.sh
```

```
data/batch-on-worker.sh
```

Now we have a list of files, let's look at the first few lines of each of them, and print a separator `---` for each.

```bash
#| filename: scripting-basics/xargs_example.sh
source ~/.bashrc #| hide_line
ls data/*.sh | xargs -I% sh -c 'head %; echo "\n---\n"'
```

```
#!/bash/bin

cmd_to_run="ls *.vcf.gz | xargs -I% sh -c "bcftools stats % > %.stats.txt"

dx run swiss-army-knife \
  -iin="data/chr1.vcf.gz" \
  -iin="data/chr2.vcf.gz" \
  -iin="data/chr3.vcf.gz" \
  -icmd=${cmd_to_run}
---
dx find data --name "*.bam" --brief
---
```


Let's take this apart piece by piece.

`xargs` takes an `-I` argument that specifies a placeholder. In our case, we are using `%` as our placeholder in this statement. 

We're passing on each filename from `ls` into the following code:

```bash
sh -c 'head %; echo "---\n"'
```

The `sh -c` opens a subshell so that we can execute our command for each of the files in our list. We're using `sh -c` to run:

```bash
'head %; echo "---\n"'
```

So for our first file, `01-scripting-basics.qmd`, we are substituting that for `%` in our command:

```bash
'head hpc-basics.qmd; echo "---\n"'
```

For our second file, `hpc-basics.qmd`, we would substitute that for the `%`:

```bash
'head hpc-basics.qmd; echo "---\n"'
```

Until we cycle through all of the files in our list.

### The Basic `xargs` pattern

:::{#fig-xargs}
```{mermaid}
graph LR
  A["ls *.bam"] --> B{"|"} 
  B --> C["xargs -I% sh -c"] 
  C --> D["command_to_run %"]
```
Basics of using `xargs` to iterate on a list of files
:::

As you cycle through lists of files, keep in mind this basic pattern (@fig-xargs):

```bash
ls <wildcard> | xargs -I% sh -c "<command to run> %"
```

:::{.callout-note}
## Test Yourself

How would we modify the below code to do the following?

1. List only `.json` files in our `data/` folder using `ls`
1. Use `tail` instead of `head`

```bash
ls *.txt | xargs -I% sh -c "head %; echo '---\n'"
```
:::

:::{.callout-note collapse="true"}
## Answer

```bash
ls data/*.json | xargs -I% sh -c "tail %; echo '---\n'"
```
:::

:::{.callout-note}
## Why this is important on HPC

We can use `xargs` to execute small batch jobs on a small number of files. This especially becomes powerful on the cluster when we use `ls` to list files in our HPC project.

Note that as we *graduate* to workflow tools like WDL/Nextflow, there are other mechanisms for running jobs on multiple files (such as WDL/Cromwell) that we should move to. 

Trust me; you don't want to have to handle iterating through a huge directory and handling when routines give an error, or your jobs get interrupted. Rerunning and resuming failed jobs are what workflow runner tools excel at. 
:::

### For more information

<https://www.baeldung.com/linux/xargs-multiple-arguments>


## Batching on HPC

Now we can 

### SLURM Scripts

SLURM scripts are a special kind of shell script that contain additional information for the SLURM manager. This includes:

1. Number of nodes (machines) to request
2. Memory and CPU requirements for each machine

We specify these using a special kind of comment: SLURM directives. Directives begin a line with `#SBATCH`

### SLURM Directives

```bash
#!/bin/bash
#SBATCH --nodes=1 # <1>
#SBATCH --array=1-6 # <2>
#SBATCH --tasks-per-node=3 # <3>
#SBATCH --cpus-per-task=1 # <4>
#SBATCH --mem-per-cpu=1gb # <5>
#SBATCH --time=00:05:00 # <6>
./samtools_opt sort SRR1576820_000$SLURM_ARRAY_TASK_ID.bam -o SRR1576820_000$SLURM_ARRAY_TASK_ID.sorted.bam # <7>
```
1. request 1 node
2. start an array
3. We want our node to do 3 tasks at the same time
4. Ask for 1 CPUs per task (3 * 1 = 3 total requested CPUs)
5. request 1 gigabyte per cpu
6. ask for 5 minutes on the node
7. Run `samtools sort` on a bam file,

### Job Arrays

This line:

```bash
#SBATCH --array=1-6 
```

Will start a job array. This will create a variable called `$SLURM_ARRAY_TASK_ID` that will cycle through the numbers 1-6. Let's try a simpler script to show what's going on:

```bash
#| eval: false
#| filename: sbatch_test.sh
#!/bin/bash
#SBATCH --array=1-5
#SBATCH --nodes=1
echo "${SLURM_ARRAY_TASK_ID} job"
```

This is a minimal script that will execute 5 subjobs. It will cycle through the job array and print the array number for each job.

```bash
#| eval: false
sbatch sbatch_test.sh
```

On submitting, we will get this message:

```
Submitted batch job 26328834
```

And if we look for the output files:

```bash
ls -l slurm-26328834_*
```

We will get the following output:

```
-rw-rw---- 1 tladera2 g_tladera2 8 Jul 15 13:50 slurm-26328834_1.out
-rw-rw---- 1 tladera2 g_tladera2 8 Jul 15 13:50 slurm-26328834_2.out
-rw-rw---- 1 tladera2 g_tladera2 8 Jul 15 13:50 slurm-26328834_3.out
-rw-rw---- 1 tladera2 g_tladera2 8 Jul 15 13:50 slurm-26328834_4.out
-rw-rw---- 1 tladera2 g_tladera2 8 Jul 15 13:50 slurm-26328834_5.out
```

Taking a look at one of these files using `cat`:

```bash
cat slurm-26328834_3.out
```

We'll see this:

```
3 job
```

```{mermaid}
graph TD
  A["sbatch sbatch_test.sh"] --"1"--> B
  B["echo 1 job"]
  A --"2"--> C["echo 2 job"]
  A --"3"--> D["echo 3 job"]
```

What happened here? `sbatch` submitted our job array as 5 different subjobs to 5 different nodes under a single job id. Each node then outputs a file with the subjob id that contains the job number. 

### Processing files using Job Arrays

So now we know that `${SLURM_ARRAY_TASK_ID}` will let us specify a subjob within our 

## Containers

We already learned about software modules (@sec-modules). There is an alternative way to use software: containers.

### What is a Container?

A container is a self-contained unit of software. It contains everything needed to run the software on a variety of machines. If you have the container software installed on your machine, it doesn't matter whether it is MacOS, Linux, or Windows - the container will behave consistently across different operating systems and architectures.

The container has the following contents:

- **Software** - The software we want to run in a container. For bioinformatics work, this is usually something like an aligner like `bwa`, or utilities such as `samtools`
- **Software Dependencies** - various software packages needed to run the software. For example, if we wanted to run `tidyverse` in a container, we need to have `R` installed in the container as well
- **Filesystem** - containers have their own isolated filesystem that can be connected to the "outside world" - everything outside of the container. We'll learn more about customizing these with bind paths (@sec-bindpaths).

In short, the container has everything needed to run the software. It is not a full operating system, but a smaller mini-version that cuts out a lot of cruft. 

Containers are ephemeral. They leverage the the file system of their host to manage files. These are called both *Volumes* (the Docker term) and Bind Paths (the apptainer term).

### Docker vs. Apptainer

There are two basic ways to run Docker containers: 

1. Using the Docker software
2. Using the Apptainer software (for HPC systems)

In general, Docker is used on systems where you have a high level of access to the system. This is because docker uses a special user group called `docker` that has essentially root level privileges. 

This is not the case for HPC systems, which are shared. This is when we use Apptainer (which used to be called Singularity), which requires a much lower level of user privileges to execute tasks. For more info, see @sec-open-container . 

:::{.callout-warning}
## Be Secure

Before we get started, security is always a concern when running containers. The `docker` group has elevated status on a system, so we need to be careful that when we're running them, they aren't introducing any system vulnerabilities. Note that on HPC systems, the main mechanism for running containers is `apptainer`, which is designed to be more secure.

These are mostly important when running containers that are web-servers or part of a web stack, but it is also important to think about when running jobs on HPC.

Here are some guidelines to think about when you are working with a container.

- **Use vendor-specific Docker Images when possible**. 
- **Use container scanners to spot potential vulnerabilities**. DockerHub has a vulnerability scanner that scans your Docker images for potential vulnerabilities. 
- **Avoid kitchen-sink images**. One issue is when an image is built on top of many other images. It makes it really difficult to plug vulnerabilities. When in doubt, use images from trusted people and organizations. At the very least, look at the Dockerfile to see that suspicious software isn't being installed.
:::

### Common Containers for Bioinformatics

- GATK (the genome analysis toolkit) is one common container that we can use for analysis.
- 

:::{.callout}
## The WILDS Docker Library

The Data Science Lab 
:::